Whilst starting to learn lisp, I\'ve come across the term tail-recursive. What does it mean exactly?Tail recursion is well-described in previous answers, but I think an example in action would help to illustrate the concept. Consider a simple function that adds the first N integers. (e.g. sum(5) = 1 + 2 + 3 + 4 + 5 = 15).Here is a simple Python implementation that uses recursion:If you called recsum(5), this is what the Python interpreter would evaluate.Note how every recursive call has to complete before the Python interpreter begins to actually do the work of calculating the sum.Here\'s a tail-recursive version of the same function:Here\'s the sequence of events that would occur if you called tailrecsum(5), (which would effectively be tailrecsum(5, 0), because of the default second argument).In the tail-recursive case, with each evaluation of the recursive call, the running_total is updated.Note: As mentioned in the comments, Python doesn\'t have built-in support for optimizing away tail calls, so there\'s no advantage to doing this in Python. However, you can use a decorator to achieve the optimization.In traditional recursion, the typical model is that you perform your recursive calls first, and then you take the return value of the recursive call and calculate the result. In this manner, you don\'t get the result of your calculation until you have returned from every recursive call.In tail recursion, you perform your calculations first, and then you execute the recursive call, passing the results of your current step to the next recursive step. This results in the last statement being in the form of "(return (recursive-function params))" (I think that\'s the syntax for Lisp). Basically, the return value of any given recursive step is the same as the return value of the next recursive call.The consequence of this is that once you are ready to perform your next recursive step, you don\'t need the current stack frame any more. This allows for some optimization. In fact, with an appropriately written compiler, you should never have a stack overflow snicker with a tail recursive call. Simply reuse the current stack frame for the next recursive step. I\'m pretty sure Lisp does this.An important point is that tail recursion is essentially equivalent to looping. It\'s not just a matter of compiler optimization, but a fundamental fact about expressiveness. This goes both ways: you can take any loop of the formwhere E and Q are expressions and S is a sequence of statements, and turn it into a tail recursive functionOf course, E, S, and Q have to be defined to compute some interesting value over some variables. For example, the looping functionis equivalent to the tail-recursive function(s)(This "wrapping" of the tail-recursive function with a function with fewer parameters is a common functional idiom.)This excerpt from the book Programming in Lua shows how to make a proper tail recursion (in Lua, but should apply to Lisp too) and why it\'s better.A tail call [tail recursion] is a kind of goto dressed\n  as a call. A tail call happens when a\n  function calls another as its last\n  action, so it has nothing else to do.\n  For instance, in the following code,\n  the call to g is a tail call:After f calls g, it has nothing else\n  to do. In such situations, the program\n  does not need to return to the calling\n  function when the called function\n  ends. Therefore, after the tail call,\n  the program does not need to keep any\n  information about the calling function\n  in the stack. ...Because a proper tail call uses no\n  stack space, there is no limit on the\n  number of "nested" tail calls that a\n  program can make. For instance, we can\n  call the following function with any\n  number as argument; it will never\n  overflow the stack:... As I said earlier, a tail call is a\n  kind of goto. As such, a quite useful\n  application of proper tail calls in\n  Lua is for programming state machines.\n  Such applications can represent each\n  state by a function; to change state\n  is to go to (or to call) a specific\n  function. As an example, let us\n  consider a simple maze game. The maze\n  has several rooms, each with up to\n  four doors: north, south, east, and\n  west. At each step, the user enters a\n  movement direction. If there is a door\n  in that direction, the user goes to\n  the corresponding room; otherwise, the\n  program prints a warning. The goal is\n  to go from an initial room to a final\n  room.This game is a typical state machine,\n  where the current room is the state.\n  We can implement such maze with one\n  function for each room. We use tail\n  calls to move from one room to\n  another. A small maze with four rooms\n  could look like this:So you see, when you make a recursive call like:This is not tail recursive because you still have things to do (add 1) in that function after the recursive call is made. If you input a very high number it will probably cause a stack overflow.Instead of explaining it with words, here\'s an example. This is a Scheme version of the factorial function:\nHere is a version of factorial that is tail-recursive:You will notice in the first version that the recursive call to fact is fed into the multiplication expression, and therefore the state has to be saved on the stack when making the recursive call. In the tail-recursive version there is no other S-expression waiting for the value of the recursive call, and since there is no further work to do, the state doesn\'t have to be saved on the stack. As a rule, Scheme tail-recursive functions use constant stack space.The jargon file has this to say about the definition of tail recursion:tail recursion /n./If you aren\'t sick of it already, see tail recursion. Using regular recursion, each recursive call pushes another entry onto the call stack. When the recursion is completed, the app then has to pop each entry off all the way back down.With tail recursion, the compiler is able to collapse the stack down to one entry, so you save stack space...A large recursive query can actually cause a stack overflow.Basically Tail recursions are able to be optimized into iteration.It means that rather than needing to push the instruction pointer on the stack, you can simply jump to the top of a recursive function and continue execution. This allows for functions to recurse indefinitely without overflowing the stack. I wrote a blog post on the subject, which has graphical examples of what the stack frames look like.Tail recursion refers to the recursive call being last in the last logic instruction in the recursive algorithm.Typically in recursion you have a base-case which is what stops the recursive calls and begins popping the call stack.  To use a classic example, though more C-ish than Lisp, the factorial function illustrates tail recursion.  The recursive call occurs after checking the base-case condition.Note, the initial call to factorial must be factorial(n, 1) where n is the number for which the factorial is to be calculated.Here is a quick code snippet comparing two functions. The first is traditional recursion for finding the factorial of a given number. The second uses tail recursion. Very simple and intuitive to understand.Easy way to tell if a recursive function is tail recursive, is if it returns a concrete value in the base case. Meaning that it doesn\'t return 1 or true or anything like that. It will more then likely return some variant of one of the method paramters.Another way is to tell is if the recursive call is free of any addition, arithmetic, modification, etc... Meaning its nothing but a pure recursive call. In Java, here\'s a possible tail recursive implementation of the Fibonacci function:Contrast this with the standard recursive implementation:Here is a Common Lisp example that does factorials using tail-recursion.  Due to the stack-less nature, one could perform insanely large factorial computations ... And then for fun you could try (format nil "~R" (! 25))here is a Perl 5 version of the tailrecsum function mentioned earlier.I\'m not a Lisp programmer, but I think this will help.Basically it\'s a style of programming such that the recursive call is the last thing you do.To understand some of the core differences between tail-call recursion and non-tail-call recursion we can explore the .NET implementations of these techniques. Here is an article with some examples in C#, F#, and C++\\CLI: Adventures in Tail Recursion in C#, F#, and C++\\CLI.C# does not optimize for tail-call recursion whereas F# does.The differences of principle involve loops vs. Lambda calculus. C# is designed with loops in mind whereas F# is built from the principles of Lambda calculus. For a very good (and free) book on the principles of Lambda calculus, see: Structure and Interpretation of Computer Programs, by Abelson, Sussman, and Sussman. Regarding tail calls in F#, for a very good introductory article , see: Detailed Introduction to Tail Calls in F#. Finally, here is an article that covers the difference between non-tail recursion and tail-call recursion (in F#): Tail-recursion vs. non-tail recursion in F sharp.If you want to read about some of the design differences of tail-call recursion between C# and F#, see: Generating Tail-Call Opcode in C# and F#.If you care enough to want to know what conditions prevent the C# compiler from performing tail-call optimizations, see this article: JIT CLR tail-call conditions.In short, a tail recursion has the recursive call as the last statement in the function so that it doesn\'t have to wait for the recursive call.So this is a tail recursion i.e. N(x - 1, p * x) is the last statement in the function where the compiler is clever to figure out that it can be optimised to a for-loop (factorial). The second parameter p carries the intermediate product value. This is the non-tail-recursive way of writing the above factorial function (although some C++ compilers may be able to optimise it anyway).but this is not:I did write a long post titled "Understanding Tail Recursion \xe2\x80\x93 Visual Studio C++ \xe2\x80\x93 Assembly View"This is an excerpt from Structure and Interpretation of Computer Programs about tail recursion.In contrasting iteration and recursion, we must be careful not to\n  confuse the notion of a recursive process with the notion of a\n  recursive procedure. When we describe a procedure as recursive, we are\n  referring to the syntactic fact that the procedure definition refers\n  (either directly or indirectly) to the procedure itself. But when we\n  describe a process as following a pattern that is, say, linearly\n  recursive, we are speaking about how the process evolves, not about\n  the syntax of how a procedure is written. It may seem disturbing that\n  we refer to a recursive procedure such as fact-iter as generating an\n  iterative process. However, the process really is iterative: Its state\n  is captured completely by its three state variables, and an\n  interpreter need keep track of only three variables in order to\n  execute the process.One reason that the distinction between process and procedure may be\n  confusing is that most implementations of common languages (including Ada, Pascal, and\n  C) are designed in such a way that the interpretation of any recursive\n  procedure consumes an amount of memory that grows with the number of\n  procedure calls, even when the process described is, in principle,\n  iterative. As a consequence, these languages can describe iterative\n  processes only by resorting to special-purpose \xe2\x80\x9clooping constructs\xe2\x80\x9d\n  such as do, repeat, until, for, and while. The implementation of\n  Scheme does not share this defect. It\n  will execute an iterative process in constant space, even if the\n  iterative process is described by a recursive procedure. An\n  implementation with this property is called tail-recursive. With a\n  tail-recursive implementation, iteration can be expressed using the\n  ordinary procedure call mechanism, so that special iteration\n  constructs are useful only as syntactic sugar.Recursion means a function calling itself. For example:Tail-Recursion means the recursion that conclude the function:See, the last thing un-ended function (procedure, in Scheme jargon) does is to call itself. Another (more useful) example is:In the helper procedure, the LAST thing it does if left is not nil is to call itself (AFTER cons something and cdr something). This is basically how you map a list.The tail-recursion has a great advantage that the interperter (or compiler, dependent on the language and vendor) can optimize it, and transform it into something equivalent to a while loop. As matter of fact, in Scheme tradition, most "for" and "while" loop is done in tail-recursion manner (there is no for and while, as far as I know).The best way for me to understand tail call recursion is: a special case of recursion where the last call(or the tail call) is the function itself.Comparing the examples provided in Python:^RECURSION^TAIL RECURSIONAs you can see in the general recursive version, the final call in the code block is x + recsum(x - 1). So after calling the recsum method there is another operation which is x + ...However in the tail recursive version the final call(or the tail call) in the code block is tailrecsum(x - 1, running_total + x) which means the last call is made to the method itself and no operation after that.This point is important because tail recursion as seen here is not making the memory grow because when the underlying VM sees a function calling itself in a tail position (the last expression to be evaluated in a function), it eliminates the current stack frame, which is known as Tail Call Optimization(TCO).Tail recursion is the life you are living right now. You constantly recycle the same stack frame, over and over, because there\'s no reason or means to return to a "previous" frame. The past is over and done with so it can be discarded. You get one frame, forever moving into the future, until your process inevitably dies.The analogy breaks down when you consider some processes might utilize additional frames but are still considered tail-recursive if the stack does not grow infinitely.This question has a lot of great answers... but I cannot help but chime in with an alternative take on how to define "tail recursion", or at least "proper tail recursion." Namely: should one look at it as a property of a particular expression in a program? Or should one look at it as a property of an implementation of a programming language?For more on the latter view, there is a classic paper by Will Clinger, "Proper Tail Recursion and Space Efficiency" (PLDI 1998), that defined "proper tail recursion" as a property of a programming language implementation. The definition is constructed to allow one to ignore implementation details (such as whether the call stack is actually represented via the runtime stack or via a heap-allocated linked list of frames).To accomplish this, it uses asymptotic analysis: not of program execution time as one usually sees, but rather of program space usage. This way, the space usage of a heap-allocated linked list vs a runtime call stack ends up being asymptotically equivalent; so one gets to ignore that programming language implementation detail (a detail which certainly matters quite a bit in practice, but can muddy the waters quite a bit when one attempts to determine whether a given implementation is satisfying the requirement to be "property tail recursive")The paper is worth careful study for a number of reasons:It gives an inductive definition of the tail expressions and tail calls of a program. (Such a definition, and why such calls are important, seems to be the subject of most of the other answers given here.)Here are those definitions, just to provide a flavor of the text:Definition 1 The tail expressions of a program written in Core Scheme are defined inductively as follows.Definition 2 A tail call is a tail expression that is a procedure call.(a tail recursive call, or as the paper says, "self-tail call" is a special case of a tail call where the procedure is invoked itself.)It provides formal definitions for six different "machines" for evaluating Core Scheme, where each machine has the same observable behavior except for the asymptotic space complexity class that each is in.For example, after giving definitions for machines with respectively, 1. stack-based memory management, 2. garbage collection but no tail calls, 3. garbage collection and tail calls, the paper continues onward with even more advanced storage management strategies, such as 4. "evlis tail recursion", where the environment does not need to be preserved across the evaluation of the last sub-expression argument in a tail call, 5. reducing the environment of a closure to just the free variables of that closure, and 6. so-called "safe-for-space" semantics as defined by Appel and Shao.In order to prove that the machines actually belong to six distinct space complexity classes, the paper, for each pair of machines under comparison, provides concrete examples of programs that will expose asymptotic space blowup on one machine but not the other.(Reading over my answer now, I\'m not sure if I\'m managed to actually capture the crucial points of the Clinger paper. But, alas, I cannot devote more time to developing this answer right now.)