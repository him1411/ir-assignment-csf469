How can I do this fast?Sure I can do this:But I\'m looking for either a BCL function or some highly optimized proven way to do this.works nicely, but it doesn\'t look like that would work for x64.Note my super-fast answer here.You can use Enumerable.SequenceEqual method.If you can\'t use .NET 3.5 for some reason, your method is OK.\nCompiler\\run-time environment will optimize your loop so you don\'t need to worry about performance. P/Invoke powers activate!There\'s a new built-in solution for this in .NET 4 - IStructuralEquatableUser gil suggested unsafe code which spawned this solution:which does 64-bit based comparison for as much of the array as possible. This kind of counts on the fact that the arrays start qword aligned. It\'ll work if not qword aligned, just not as fast as if it were.It performs about seven timers faster than the simple for loop. Using the J# library performed equivalently to the original for loop. Using .SequenceEqual runs around seven times slower; I think just because it is using IEnumerator.MoveNext. I imagine LINQ-based solutions being at least that slow or worse..NET 3.5 and newer have a new public type, System.Data.Linq.Binary that encapsulates byte[]. It implements IEquatable<Binary> that (in effect) compares two byte arrays. Note that System.Data.Linq.Binary also has implicit conversion operator from byte[].MSDN documentation:System.Data.Linq.BinaryReflector decompile of the Equals method:Interesting twist is that they only proceed to byte-by-byte comparison loop if hashes of the two Binary objects are the same. This, however, comes at the cost of computing the hash in constructor of Binary objects (by traversing the array with for loop :-) ).The above implementation means that in the worst case you may have to traverse the arrays three times: first to compute hash of array1, then to compute hash of array2 and finally (because this is the worst case scenario, lengths and hashes equal) to compare bytes in array1 with bytes in array 2.Overall, even though System.Data.Linq.Binary is built into BCL, I don\'t think it is the fastest way to compare two byte arrays :-|.If you are not opposed to doing it, you can import the J# assembly "vjslib.dll" and use its Arrays.equals(byte[], byte[]) method...Don\'t blame me if someone laughs at you though...EDIT: For what little it is worth, I used Reflector to disassemble the code for that, and here is what it looks like:I posted a similar question about checking if byte[] is full of zeroes. (SIMD code was beaten so I removed it from this answer.) Here is fastest code from my comparisons:Measured on two 256MB byte arrays:I would use unsafe code and run the for loop comparing Int32 pointers.Maybe you should also consider checking the arrays to be non-null.If you look at how .NET does string.Equals, you see that it uses a private method called EqualsHelper which has an "unsafe" pointer implementation. .NET Reflector is your friend to see how things are done internally.This can be used as a template for byte array comparison which I did an implementation on in blog post Fast byte array comparison in C#. I also did some rudimentary benchmarks to see when a safe implementation is faster than the unsafe.That said, unless you really need killer performance, I\'d go for a simple fr loop comparison.I developed a method that slightly beats memcmp() (plinth\'s answer) and very slighly beats EqualBytesLongUnrolled() (Arek Bulski\'s answer). Basically, it unrolls the loop by 4 instead of 8.This runs about 25% faster than memcmp() and about 5% faster than EqualBytesLongUnrolled() on my machine.Let\'s add one more!Recently Microsoft released a special NuGet package, System.Runtime.CompilerServices.Unsafe. It\'s special because it\'s written in IL, and provides low-level functionality not directly available in C#.One of its methods, Unsafe.As<T>(object) allows casting any reference type to another reference type, skipping any safety checks. This is usually a very bad idea, but if both types have the same structure, it can work. So we can use this to cast a byte[] to a long[]:Note that long1.Length would still return the original array\'s length, since it\'s stored in a field in the array\'s memory structure.This method is not quite as fast as other methods demonstrated here, but it is a lot faster than the naive method, doesn\'t use unsafe code or P/Invoke or pinning, and the implementation is quite straightforward (IMO). Here are some BenchmarkDotNet results from my machine:I\'ve also created a gist with all the tests.For comparing short byte arrays the following is an interesting hack:Then I would probably fall out to the solution listed in the question.It\'d be interesting to do a performance analysis of this code.Couldn\'t find a solution I\'m completely happy with (reasonable performance, but no unsafe code/pinvoke) so I came up with this, nothing really original, but works:Performance compared with some of the other solutions on this page:Simple Loop: 19837 ticks, 1.00*BitConverter: 4886 ticks, 4.06UnsafeCompare: 1636 ticks, 12.12EqualBytesLongUnrolled: 637 ticks, 31.09P/Invoke memcmp: 369 ticks, 53.67Tested in linqpad, 1000000 bytes identical arrays (worst case scenario), 500 iterations each.It seems that EqualBytesLongUnrolled is the best from the above suggested.Skipped methods (Enumerable.SequenceEqual,StructuralComparisons.StructuralEqualityComparer.Equals),  were not-patient-for-slow. On 265MB arrays I have measured this:I thought about block-transfer acceleration methods built into many graphics cards. But then you would have to copy over all the data byte-wise, so this doesn\'t help you much if you don\'t want to implement a whole portion of your logic in unmanaged and hardware-dependent code...Another way of optimization similar to the approach shown above would be to store as much of your data as possible in a long[] rather than a byte[] right from the start, for example if you are reading it sequentially from a binary file, or if you use a memory mapped file, read in data as long[] or single long values. Then, your comparison loop will only need 1/8th of the number of iterations it would have to do for a byte[] containing the same amount of data.\nIt is a matter of when and how often you need to compare vs. when and how often you need to access the data in a byte-by-byte manner, e.g. to use it in an API call as a parameter in a method that expects a byte[]. In the end, you only can tell if you really know the use case...Sorry, if you\'re looking for a managed way you\'re already doing it correctly and to my knowledge there\'s no built in method in the BCL for doing this.You should add some initial null checks and then just reuse it as if it where in BCL.Use SequenceEquals for this to comparison.The short answer is this:In such a way you can use the optimized .NET string compare to make a byte array compare without the need to write unsafe code. This is how it is done in the background:This is almost certainly much slower than any other version given here, but it was fun to write.If you are looking for a very fast byte array equality comparer, I suggest you take a look at this STSdb Labs article: Byte array equality comparer. It features some of the fastest implementations for byte[] array equality comparing, which are presented, performance tested and summarized.You can also focus on these implementations:BigEndianByteArrayComparer - fast byte[] array comparer from left to right (BigEndian)\nBigEndianByteArrayEqualityComparer - - fast byte[] equality comparer from left to right (BigEndian)\nLittleEndianByteArrayComparer - fast byte[] array comparer from right to left (LittleEndian)\nLittleEndianByteArrayEqualityComparer - fast byte[] equality comparer from right to left (LittleEndian)In case you have a huge byte array, you can compare them by converting them to string.You can use something likeI have used this and I have seen a huge performance impact.