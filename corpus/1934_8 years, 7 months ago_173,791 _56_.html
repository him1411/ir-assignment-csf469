I am looking for opinions of how to handle large binary files on which my source code (web application) is dependent. We are currently discussing several alternatives:What are your experiences/thoughts regarding this? Also: Does anybody have experience with multiple git repositories and managing them in one project? Update: The files are images for a program which generates PDFs with those files in it. The files will not change very often(as in years) but are very relevant to a program. The program will not work without the files.If the program won\'t work without the files it seems like splitting them into a separate repo is a bad idea.  We have large test suites that we break into a separate repo but those are truly "auxiliary" files.However, you may be able to manage the files in a separate repo and then use git-submodule to pull them into your project in a sane way.  So, you\'d still have the full history of all your source but, as I understand it, you\'d only have the one relevant revision of your images submodule.  The git-submodule facility should help you keep the correct version of the code in line with the correct version of the images.Here\'s a good introduction to submodules from Git Book.I discoverd git-annex recently which I find awesome. It was designed for managing large files efficiently. I use it for my photo/music (etc.) collections. The development of git-annex is very active. The content of the files can be removed from the git repo, only the tree hierarchy is tracked by git (through symlinks). However, to get the content of the file, a second step is necessary after pulling/pushing, e.g.:There are many commands available, and there is a great documentation on the website. A package is available on debian.Another solution, since April 2015 is Git Large File Storage (LFS) (by GitHub).It uses git-lfs (see git-lfs.github.com) and tested with a server supporting it: lfs-test-server:\nYou can store metadata only in the git repo, and the large file elsewhere.Have a look at git bup which is a git extension to smartly store large binaries in a git repo.You\'d want to have it as a submodule but you won\'t have to worry about the repo getting hard to handle. One of their sample use cases is storing VM images in git.I haven\'t actually seen better compression rates but my repos don\'t have really large binaries in them.YMMVYou can also use git-fat. What I like about it is that it only depends on stock python and rsync. It also supports the usual git workflow, with the following self explanatory commands:In addition, you need to check in a .gitfat file into your repo and modify your .gitattributes to specify the file extensions you want git fat to manage.You add a binary using the normal git add, which in turn invokes git fat based on your gitattributes rules.Finally, it has the advantage that the location where your binaries are actually stored can be shared across repositories and users and supports anything rsync does.UPDATE: Do not use git-fat if you\'re using a git-svn bridge. It will end up removing the binary files from your subversion repository. However, if you\'re using a pure git repository, it works beautifully.I would use submodules (as Pat Notz) or two distinct repositories. If you modify your binary files too often, then I would try to minimize the impact of the huge repository cleaning the history:I had a very similar problem several months ago: ~21Gb of mp3\'s, unclassified (bad names, bad id3\'s, don\'t know if I like that mp3 or not...), and replicated in three computers. I used an external harddisk with the main git repo and I cloned it into each computer. Then, I started to classify them in the habitual way (pushing, pulling, merging... deleting and renaming many times).At the end, I had only ~6Gb of mp3\'s and ~83Gb in the .git dir. I used git-write-tree and git-commit-tree to create a new commit, without commit ancestors, and started a new branch pointing to that commit. The "git log" for that branch only showed one commit.Then, I deleted the old branch, kept only the new branch, deleted the ref-logs, and run "git prune": after that, my .git folders weighted only ~6Gb...You could "purge" the huge repository from time to time in the same way: Your "git clone"\'s will be faster.In my opinion, if you\'re likely to often modify those large files, or if you intend to make a lot of git clone or git checkout, then you should seriously consider using another git repository (or maybe another way to access thoses files).But if you work like we do, and if your binary file are not often modified, then the first clone/checkout will be long, but after that it should be as fast as you want (considering your users keep using the first cloned repo they had).The solution I\'d like to propose is based on orphan branches and a slight abuse of the tag mechanism, henceforth referred to as *Orphan Tags Binary Storage (OTABS)TL;DR 12-01-2017 If you can use github\'s LFS or some other 3rd party, by all means you should. If you can\'t, then read on. Be warned, this solution is a hack and should be treated as such.Desirable properties of OTABSUndesirable properties of OTABSAdding the Binary FilesBefore you start make sure that you\'ve committed all your changes, your working tree is up to date and your index doesn\'t contain any uncommitted changes. It might be a good idea to push all your local branches to your remote (github etc.) in case any disaster should happen.Checking out the Binary FileCompletely Deleting the Binary FileIf you decide to completely purge VeryBigBinary.exe from your local repository, your remote repository and your colleague\'s repositories you can just:AfterwordOTABS doesn\'t touch your master or any other source code/development branches. The commit hashes, all of the history, and small size of these branches is unaffected. If you\'ve already bloated your source code history with binary files you\'ll have to clean it up as a separate piece of work. This script might be useful.Confirmed to work on Windows with git-bash.It is a good idea to apply a set of standard trics to make storage of binary files more efficient. Frequent running of git gc (without any additional arguments) makes git optimise underlying storage of your files by using binary deltas. However, if your files are unlikely to stay similar from commit to commit you can switch off binary deltas altogether. Additionally, because it makes no sense to compress already compressed or encrypted files, like .zip, .jpg or .crypt, git allows you to switch off compression of the underlying storage. Unfortunately it\'s an all-or-nothing setting affecting your source code as well.You might want to script up parts of OTABS to allow for quicker usage. In particular, scripting steps 2-3 from Completely Deleting Binary Files into an update git hook could give a compelling but perhaps dangerous semantics to git fetch ("fetch and delete everything that is out of date").You might want to skip the step 4 of Completely Deleting Binary Files to keep a full history of all binary changes on the remote at the cost of the central repository bloat. Local repositories will stay lean over time.In Java world it is possible to combine this solution with maven --offline to create a reproducible offline build stored entirely in your version control (it\'s easier with maven than with gradle). In Golang world it is feasible to build on this solution to manage your GOPATH instead of go get. In python world it is possible to combine this with virtualenv to produce a self-contained development environment without relying on PyPi servers for every build from scratch.If your binary files change very often, like build artifacts, it might be a good idea to script a solution which stores 5 most recent versions of the artifacts in the orphan tags monday_bin, tuesday_bin, ..., friday_bin, and also an orphan tag for each release 1.7.8bin 2.0.0bin, etc. You can rotate the weekday_bin and delete old binaries daily. This way you get the best of two worlds: you keep the entire history of your source code but only the relevant history of your binary dependencies. It is also very easy to get the binary files for a given tag without getting entire source code with all its history: git init && git remote add <name> <url> && git fetch <name> <tag> should do it for you.SVN seems to handle binary deltas more efficiently than gitHad to decide on a versioning system for documentation (jpgs, pdfs, odts). Just tested adding a jpeg and rotating it 90 degrees 4 times (to check effectiveness of binary deltas). git\'s repository grew 400%. SVN\'s repository grew by only 11%So it looks like SVN is much more efficient with binary filesSo my choice is git for source code and SVN for binary files like documentation.I am looking for opinions of how to handle large binary files on which my source code (web application) is dependent. What are your experiences/thoughts regarding this?I personally have run into sync failures with git with some of my cloud hosts once my web apps binary data notched above the 3GB mark. Considered BFT Repo Cleaner at the time but it felt like a hack. Since then I\'ve begun to just keep files outside of git purview, instead leveraging purpose-built tools such as S3 for managing files, versioning and back-up.Does anybody have experience with multiple git repositories and managing them in one project?Yes. Hugo themes are primarily managed this way. It\'s a little kudgy but gets the job done.My suggestion is to choose the right tool for the job. If it\'s for a company and you\'re managing your codeline on GitHub pay the money and use Git-LFS. Otherwise you could explore more creative options such as decentralized, encrypted file storage using blockchain. Additional options include Minio and s3cmd.Have you looked at camlistore It is not really git based, but I find it more appropriate for what you have to do. 