I\'m trying to parse JSON returned from a curl request, like so:The above splits the JSON into fields, for example:How do I print a specific field (denoted by the -v k=text)?There are a number of tools specifically designed for the purpose of manipulating JSON from the command line, and will be a lot easier and more reliable than doing it with Awk, such as jq:You can also do this with tools that are likely already installed on your system, like Python using the json module, and so avoid any extra dependencies, while still having the benefit of a proper JSON parser. The following assume you want to use UTF-8, which the original JSON should be encoded in and is what most modern terminals use as well:Python 2:Python 3:This answer originally recommended jsawk, which should still work, but is a little more cumbersome to use than jq, and depends on a standalone JavaScript interpreter being installed which is less common than a Python interpreter, so the above answers are probably preferable:This answer also originally used the Twitter API from the question, but that API no longer works, making it hard to copy the examples to test out, and the new Twitter API requires API keys, so I\'ve switched to using the GitHub API which can be used easily without API keys.  The first answer for the original question would be:To quickly extract the values for a particular key, I personally like to use "grep -o", which only returns the regex\'s match.  For example, to get the "text" field from tweets, something like:This regex is more robust than you might think; for example, it deals fine with strings having embedded commas and escaped quotes inside them. I think with a little more work you could make one that is actually guaranteed to extract the value, if it\'s atomic.  (If it has nesting, then a regex can\'t do it of course.)And to further clean (albeit keeping the string\'s original escaping) you can use something like: | perl -pe \'s/"text"://; s/^"//; s/",$//\'.  (I did this for this analysis.)To all the haters who insist you should use a real JSON parser -- yes, that is essential for correctness, butTo write maintainable code, I always use a real parsing library.  I haven\'t tried jsawk, but if it works well, that would address point #1.One last, wackier, solution: I wrote a script that uses Python json and extracts the keys you want, into tab-separated columns; then I pipe through a wrapper around awk that allows named access to columns.  In here: the json2tsv and tsvawk scripts.  So for this example it would be:This approach doesn\'t address #2, is more inefficient than a single Python script, and it\'s a little brittle: it forces normalization of newlines and tabs in string values, to play nice with awk\'s field/record-delimited view of the world.  But it does let you stay on the command line, with more correctness than grep -o.On the basis that some of the recommendations here (esp in the comments) suggested the use of Python, I was disappointed not to find an example.So, here\'s a one liner to get a single value from some JSON data. It assumes that you are piping the data in (from somewhere) and so should be useful in a scripting context. Following MartinR and Boecko\'s lead: That will give you an extremely grep friendly output. Very convenient:You could just download jq binary for your platform and run (chmod +x jq):It extracts "name" attribute from the json object.jq homepage says it is like sed for JSON data.Use Python\'s JSON support instead of using awk!Something like this:If the system has node installed, it\'s possible to use the -p print and -e evaulate script flags with JSON.parse to pull out any value that is needed.A simple example using the JSON string { "foo": "bar" } and pulling out the value of "foo":Because we have access to cat and other utilities, we can use this for files:Or any other format such as an URL that contains JSON:You\'ve asked how to shoot yourself in the foot and I\'m here to provide the ammo:You could use tr -d \'{}\' instead of sed. But leaving them out completely seems to have the desired effect as well.If you want to strip off the outer quotes, pipe the result of the above through sed \'s/\\(^"\\|"$\\)//g\'I think others have sounded sufficient alarm. I\'ll be standing by with a cell phone to call an ambulance. Fire when ready.Create a bash function in your .bash_rc fileThenHere is the same function, but with error checking.Where $# -ne 1 makes sure at least 1 input, and -t 0 make sure you are redirecting from a pipe.The nice thing about this implementation is that you can access nested json values and get json in return! =)Example:If you want to be really fancy, you could pretty print the data:jqArguably off topic but since precedence reigns this question remains incomplete without a mention of our trusty and faithful PHP, am I right?  Using the same example JSON but lets assign it to a variable to reduce obscurity.Now for PHP goodness, using file_get_contents and the php://stdin stream wrapper. or as pointed out using fgets and the already opened stream at CLI constant STDIN.nJoy!please don\'t do it!do not use line-oriented tools to parse hierarchical data serialized into text.  it works only for special cases and will haunt you and other people.  if you really can\'t use a ready-made json parser, write a simple one using recursive descent.  it\'s easy and will endure changes the emitting side justly considers cosmetic (added or removed whitespace including newlines).TickTick is a JSON parser written in bash (<250 lines of code)Here\'s the author\'s snippit from his article, Imagine a world where Bash supports JSON:Native Bash version:\nAlso works well with backslashes (\\) and quotes (")Do not reinvent the wheel and select from the oficial JSON parsing software recommended by the JSON creator: \nhttp://www.json.org/\n(see at the bottom)Version which uses Ruby and http://flori.github.com/json/or more concisely:Everyone seems to underestimate awk. True, a one or two line awk script is not going to suffice. But it\'s not difficult to write a true JSON parser in awk. I just added one to my awkenough libraries.A few years late I guess (sorry) but I had created a pure bash script, that supports nesting, and can easily get values.Main part of the script:The script needs JSON.sh in the working directory.The author of the json format reccomends using JSON.sh in Bash (go to www.json.org and scroll down to the second section and the it listed under "Bash").If you want your script just to be in one .sh file you can copy and paste the throw, parse_array, parse_object, parse_value and the parse commands into your script.\nChange your script to this: This way you can use only 1 script file and use pure bash, and support nested json. Enjoy! (although you\'ve probably moved on to other projects now).I created a module specifically designed for command-line JSON manipulation:https://github.com/ddopson/underscore-cliSelecting a field is pretty easy:By default, it will pretty print the output with "smart-whitespace" that is both readable and 100% strict JSON (but you can select other formats with flags):If you have any feature requests, comment on this post or add an issue in github.  I\'d be glad to prioritize features that are needed by members of the community.You can use jshon:here\'s one way you can do it with awk This might be considered offtopic, but it could be helpful.\nHere is my JSON parser for Spotify URIs:The script is quite useful when using Spotify URI as a parameter (i.e. $1).jsawk @ githubYou can try something like this - Someone who also has xml files, might want to look at my Xidel. It is a cli, dependency-free JSONiq processor. (i.e. it also supports XQuery for xml or json processing)The example in the question would be:Or with my own, non standard extension syntax:Parsing JSON is painful in a shell script. With a more appropriate language, create a tool that extracts JSON attributes in a way consistent with shell scripting conventions. You can use your new tool to solve the immediate shell scripting problem and then add it to your kit for future situations.For example, consider a tool jsonlookup such that if I say jsonlookup access token id it will return the attribute id defined within the attribute token defined within the attribute access from stdin, which is presumably JSON data. If the attribute doesn\'t exist, the tool returns nothing (exit status 1). If the parsing fails, exit status 2 and a message to stderr. If the lookup succeeds, the tool prints the attribute\'s value.Having created a unix tool for the precise purpose of extracting JSON values you can easily use it in shell scripts:Any language will do for the implementation of jsonlookup. Here is a fairly concise python version:A two-liner which uses python. It works particularly well if you\'re writing a single .sh file and you don\'t want to depend on another .py file. It also leverages the usage of pipe |. echo "{\\"field\\": \\"value\\"}" can be replaced by anything printing a json to the stdout. This is a good usecase for pythonpy:If you have php:For example:\nwe have resource that provides json with countries iso codes: http://country.io/iso3.json and we can easily see it in a shell with curl:but it looks not very convenient, and not readable, better parse json and see readable structure:This code will print something like:if you have nested arrays this output will looks much better...Hope this will helpful...Unfortunately the top voted answer that uses grep returns the full match that didn\'t work in my scenario, but if you know the JSON format will remain constant you can use lookbehind and lookahead to extract just the desired values.How about using Rhino?  It\'s a command-line JavaScript tool.  Unfortunately, it\'s a bit rough for this type of application.  It doesn\'t read from stdin very well.