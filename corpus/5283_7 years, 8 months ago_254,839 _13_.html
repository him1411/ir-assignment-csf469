I\'ve got a large (by number of lines) plain text file that I\'d like to split into smaller files, also by number of lines.  So if my file has around 2M lines, I\'d like to split it up into 10 files that contain 200k lines, or 100 files that contain 20k lines (plus one file with the remainder; being evenly divisible doesn\'t matter).I could do this fairly easily in Python but I\'m wondering if there\'s any kind of ninja way to do this using bash and unix utils (as opposed to manually looping and counting / partitioning lines).Have you looked at the split command?You could do something like:How about the split command?Yes, there is a split command.  It will split a file by lines or bytes.use splitSplit a file into fixed-size pieces, creates output files containing consecutive sections of INPUT (standard input if none is given or INPUT is `-\')Syntax\n      split [options] [INPUT [PREFIX]]http://ss64.com/bash/split.htmlUse:Here, 1 and 100 are the line numbers which you will capture in output.txt.you can also use awk