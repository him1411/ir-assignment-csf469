Is hashing a password twice before storage any more or less secure than just hashing it once?What I\'m talking about is doing this:instead of just this:If it is less secure, can you provide a good explanation (or a link to one)?Also, does the hash function used make a difference?  Does it make any difference if you mix md5 and sha1 (for example) instead of repeating the same hash function?Note 1:  When I say "double hashing" I\'m talking about hashing a password twice in an attempt to make it more obscured.  I\'m not talking about the technique for resolving collisions.Note 2:  I know I need to add a random salt to really make it secure.  The question is whether hashing twice with the same algorithm helps or hurts the hash.No, multiple hashes are not less secure; they are an essential part of secure password use.Iterating the hash increases the time it takes for an attacker to try each password in their list of candidates. You can easily increase the time it takes to attack a password from hours to years.Merely chaining hash output to input isn\'t sufficient for security. The iteration should take place in the context of an algorithm that preserves the entropy of the password. Luckily, there are several published algorithms that have had enough scrutiny to give confidence in their design.A good key derivation algorithm like PBKDF2 injects the password into each round of hashing, mitigating concerns about collisions in hash output. PBKDF2 can be used for password authentication as-is. Bcrypt follows the key derivation with an encryption step; that way, if a fast way to reverse the key derivation is discovered, an attacker still has to complete a known-plaintext attack.Stored passwords need protection from an offline attack. If passwords aren\'t salted, they can be broken with a pre-computed dictionary attack (for example, using a Rainbow Table). Otherwise, the attacker must spend time to compute a hash for each password and see if it matches the stored hash.All passwords are not equally likely. Attackers might exhaustively search all short passwords, but they know that their chances for brute-force success drop sharply with each additional character. Instead, they use an ordered list of the most likely passwords. They start with "password123" and progress to less frequently used passwords. Let\'s say an attackers list is long, with 10 billion candidates; suppose also that a desktop system can compute 1 million hashes per second. The attacker can test her whole list is less than three hours if only one iteration is used. But if just 2000 iterations are used, that time extends to almost 8 months. To defeat a more sophisticated attacker—one capable of downloading a program that can tap the power of their GPU, for example—you need more iterations.The number of iterations to use is a trade-off between security and user experience. Specialized hardware that can be used by attackers is cheap, but it can still perform hundreds of millions of iterations per second. The performance of the attacker\'s system determines how long it takes to break a password given a number of iterations. But your application is not likely to use this specialized hardware. How many iterations you can perform without aggravating users depends on your system. You can probably let users wait an extra ¾ second or so during authentication. Profile your target platform, and use as many iterations as you can afford. Platforms I\'ve tested (one user on a mobile device, or many users on a server platform) can comfortably support PBKDF2 with between 60,000 and 120,000 iterations, or bcrypt with cost factor of 12 or 13.Read PKCS #5 for authoritative information on the role of salt and iterations in hashing. Even though PBKDF2 was meant for generating encryption keys from passwords, it works well as a one-way-hash for password authentication. Each iteration of bcrypt is more expensive than a SHA-2 hash, so you can use fewer iterations, but the idea is the same. Bcrypt also goes a step beyond most PBKDF2-based solutions by using the derived key to encrypt a well-known plain text. The resulting cipher text is stored as the "hash," along with some meta-data. However, nothing stops you from doing the same thing with PBKDF2.Here are other answers I\'ve written on this topic:To those who say it\'s secure, they are correct in general. "Double" hashing (or the logical expansion of that, iterating a hash function) is absolutely secure if done right, for a specific concern.To those who say it\'s insecure, they are correct in this case. The code that is posted in the question is insecure. Let\'s talk about why:There are two fundamental properties of a hash function that we\'re concerned about:Pre-Image Resistance - Given a hash $h, it should be difficult to find a message $m such that $h === hash($m)Second-Pre-Image Resistance - Given a message $m1, it should be difficult to find a different message $m2 such that hash($m1) === hash($m2)Collision Resistance - It should be difficult to find a pair of messages ($m1, $m2) such that hash($m1) === hash($m2) (note that this is similar to Second-Pre-Image resistance, but different in that here the attacker has control over both messages)...For the storage of passwords, all we really care about is Pre-Image Resistance. The other two would be moot, because $m1 is the user\'s password we\'re trying to keep safe. So if the attacker already has it, the hash has nothing to protect...Everything that follows is based on the premise that all we care about is Pre-Image Resistance. The other two fundamental properties of hash functions may not (and typically don\'t) hold up in the same way. So the conclusions in this post are only applicable when using hash functions for the storage of passwords. They are not applicable in general...For the sake of this discussion, let\'s invent our own hash function:Now it should be pretty obvious what this hash function does. It sums together the ASCII values of each character of input, and then takes the modulo of that result with 256. So let\'s test it out:Now, let\'s see what happens if we run it a few times around a function:That outputs:Hrm, wow. We\'ve generated collisions!!! Let\'s try to look at why:Here\'s the output of hashing a string of each and every possible hash output:Notice the tendency towards higher numbers. That turns out to be our deadfall. Running the hash 4 times ($hash = ourHash($hash)`, for each element) winds up giving us:We\'ve narrowed ourselves down to 8 values... That\'s bad... Our original function mapped S(\xe2\x88\x9e) onto S(256). That is we\'ve created a Surjective Function mapping $input to $output. Since we have a Surjective function, we have no guarantee the mapping for any subset of the input won\'t have collisions (in fact, in practice they will).That\'s what happened here! Our function was bad, but that\'s not why this worked (that\'s why it worked so quickly and so completely). The same thing happens with MD5. It maps S(\xe2\x88\x9e) onto S(2^128). Since there\'s no guarantee that running MD5(S(output)) will be Injective, meaning that it won\'t have collisions.Therefore, since feeding the output back to md5 directly can generate collisions, every iteration will increase the chance of collisions. This is a linear increase however, which means that while the result set of 2^128 is reduced, it\'s not significantly reduced fast enough to be a critical flaw.So,The more times you iterate, the further the reduction goes.Fortunately for us, there\'s a trivial way to fix this: Feed back something into the further iterations:Note that the further iterations aren\'t 2^128 for each individual value for $input. Meaning that we may be able to generate $input values that still collide down the line (and hence will settle or resonate at far less than 2^128 possible outputs). But the general case for $input is still as strong as it was for a single round.Wait, was it? Let\'s test this out with our ourHash() function. Switching to $hash = ourHash($input . $hash);, for 100 iterations:There\'s still a rough pattern there, but note that it\'s no more of a pattern than our underlying function (which was already quite weak). Notice however that 0 and 3 became collisions, even though they weren\'t in the single run. That\'s an application of what I said before (that the collision resistance stays the same for the set of all inputs, but specific collision routes may open up due to flaws in the underlying algorithm).By feeding back the input into each iteration, we effectively break any collisions that may have occurred in the prior iteration.Therefore, md5($input . md5($input)); should be (theoretically at least) as strong as md5($input). Yes. This is one of the reasons that PBKDF2 replaced PBKDF1 in RFC 2898. Consider the inner loops of the two::PBKDF1:Where c is the iteration count, P is the Password and S is the saltPBKDF2:Where PRF is really just a HMAC. But for our purposes here, let\'s just say that PRF(P, S) = Hash(P || S) (that is, the PRF of 2 inputs is the same, roughly speaking, as hash with the two concatenated together). It\'s very much not, but for our purposes it is.So PBKDF2 maintains the collision resistance of the underlying Hash function, where PBKDF1 does not.We know of secure ways of iterating a hash. In fact:Is typically safe. Now, to go into why we would want to hash it, let\'s analyze the entropy movement.A hash takes in the infinite set: S(\xe2\x88\x9e) and produces a smaller, consistently sized set S(n). The next iteration (assuming the input is passed back in) maps S(\xe2\x88\x9e) onto S(n) again:Notice that the final output has exactly the same amount of entropy as the first one. Iterating will not "make it more obscured". The entropy is identical. There\'s no magic source of unpredictability (it\'s a Pseudo-Random-Function, not a Random Function).There is however a gain to iterating. It makes the hashing process artificially slower. And that\'s why iterating can be a good idea. In fact, it\'s the basic principle of most modern password hashing algorithms (the fact that doing something over-and-over makes it slower).Slow is good, because it\'s combating the primary security threat: brute forcing. The slower we make our hashing algorithm, the harder attackers have to work to attack password hashes stolen from us. And that\'s a good thing!!!Yes, re-hashing reduces the search space, but no, it doesn\'t matter - the effective reduction is insignificant.Re-hashing increases the time it takes to brute-force, but doing so only twice is also suboptimal.What you really want is to hash the password with PBKDF2 - a proven method of using a secure hash with salt and iterations. Check out this SO response.EDIT: I almost forgot - DON\'T USE MD5!!!! Use a modern cryptographic hash such as the SHA-2 family (SHA-256, SHA-384, and SHA-512).Yes - it reduces the number of possibly strings that match the string.As you have already mentioned, salted hashes are much better.An article here: http://websecurity.ro/blog/2007/11/02/md5md5-vs-md5/, attempts a proof at why it is equivalent, but I\'m not sure with the logic. Partly they assume that there isn\'t software available to analyse md5(md5(text)), but obviously it\'s fairly trivial to produce the rainbow tables.I\'m still sticking with my answer that there are smaller number of md5(md5(text)) type hashes than md5(text) hashes, increasing the chance of collision (even if still to an unlikely probability) and reducing the search space.I just look at this from a practical standpoint. What is the hacker after? Why, the combination of characters that, when put through the hash function, generates the desired hash.You are only saving the last hash, therefore, the hacker only has to bruteforce one hash. Assuming you have roughly the same odds of stumbling across the desired hash with each bruteforce step, the number of hashes is irrelevant. You could do a million hash iterations, and it would not increase or reduce security one bit, since at the end of the line there\'s still only one hash to break, and the odds of breaking it are the same as any hash.Maybe the previous posters think that the input is relevant; it\'s not. As long as whatever you put into the hash function generates the desired hash, it will get you through, correct input or incorrect input.Now, rainbow tables are another story. Since a rainbow table only carries raw passwords, hashing twice may be a good security measure, since a rainbow table that contains every hash of every hash would be too large.Of course, I\'m only considering the example the OP gave, where it\'s just a plain-text password being hashed. If you include the username or a salt in the hash, it\'s a different story; hashing twice is entirely unnecessary, since the rainbow table would already be too large to be practical and contain the right hash.Anyway, not a security expert here, but that\'s just what I\'ve figured from my experience.Personally I wouldn\'t bother with multiple hashses, but I\'d make sure to also hash the UserName (or another User ID field) as well as the password so two users with the same password won\'t end up with the same hash. Also I\'d probably throw some other constant string into the input string too for good measure.Most answers are by people without a background in cryptography or security. And they are wrong. Use a salt, if possible unique per record. MD5/SHA/etc are too fast, the opposite of what you want. PBKDF2 and bcrypt are slower (wich is good) but can be defeated with ASICs/FPGA/GPUs (very afordable nowadays). So a memory-hard algorithm is needed: enter scrypt.Here\'s a layman explanation on salts and speed (but not about memory-hard algorithms).In general, it provides no additional security to double hash or double encrypt something.  If you can break the hash once, you can break it again.  It usually doesn\'t hurt security to do this, though.In your example of using MD5, as you probably know there are some collision issues.  "Double Hashing" doesn\'t really help protect against this, since the same collisions will still result in the same first hash, which you can then MD5 again to get the second hash.This does protect against dictionary attacks, like those "reverse MD5-databases", but so does salting.On a tangent, Double encrypting something doesn\'t provide any additional security because all it does is result in a different key which is a combination of the two keys actually used.  So the effort to find the "key" is not doubled because two keys do not actually need to be found.  This isn\'t true for hashing, because the result of the hash is not usually the same length as the original input.From what I\'ve read, it may actually be recommended to re-hash the password hundreds or thousands of times.  The idea is that if you can make it take more time to encode the password, it\'s more work for an attacker to run through many guesses to crack the password.  That seems to be the advantage to re-hashing -- not that it\'s more cryptographically secure, but it simply takes longer to generate a dictionary attack.Of course computers get faster all the time, so this advantage diminishes over time (or requires you to increase the iterations).As several responses in this article suggest, there are some cases where it may improves security and others where it definately hurts it. There is a better solution that will definately improve security. Instead of doubling the number of times you calculate the hash, double the size of your salt, or double the number of bits used int the hash, or do both! Instead of SHA-245, jump up to SHA-512.Let us assume you use the hashing algorithm: compute rot13, take the first 10 characters. If you do that twice (or even 2000 times) it is possible to make a function that is faster, but which gives the same result (namely just take the first 10 chars).Likewise it may be possible to make a faster function that gives the same output as a repeated hashing function. So your choice of hashing function is very important: as with the rot13 example it is not given that repeated hashing will improve security. If there is no research saying that the algorithm is designed for recursive use, then it is safer to assume that it will not give you added protection.That said: For all but the simplest hashing functions it will most likely take cryptography experts to compute the faster functions, so if you are guarding against attackers that do not have access to cryptography experts it is probably safer in practice to use a repeated hashing function.Double hashing makes sense to me only if I hash the password on the client, and then save the hash (with different salt) of that hash on the server.That way even if someone hacked his way into the server (thereby ignoring the safety SSL provides), he still can\'t get to the clear passwords.Yes he will have the data required to breach into the system, but he wouldn\'t be able to use that data to compromise outside accounts the user has. And people are known to use the same password for virtually anything.The only way he could get to the clear passwords is installing a keygen on the client - and that\'s not your problem anymore.So in short:The concern about reducing the search space is mathematically correct, although the search space remains large enough that for all practical purposes (assuming you use salts), at 2^128.  However, since we are talking about passwords, the number of possible 16-character strings (alphanumeric, caps matter, a few symbols thrown in) is roughly 2^98, according to my back-of-the-envelope calculations.  So the perceived decrease in the search space is not really relevant.Aside from that, there really is no difference, cryptographically speaking. Although there is a crypto primitive called a "hash chain" -- a technique that allows you to do some cool tricks, like disclosing a signature key after it\'s been used, without sacrificing the integrity of the system -- given minimal time synchronization, this allows you to cleanly sidestep the problem of initial key distribution.  Basically, you precompute a large set of hashes of hashes - h(h(h(h....(h(k))...))) , use the nth value to sign, after a set interval, you send out the key, and sign it using key (n-1).  The recepients can now verify that you sent all the previous messages, and no one can fake your signature since the time period for which it is valid has passed.  Re-hashing hundreds of thousands of times like Bill suggests is just a waste of your cpu.. use a longer key if you are concerned about people breaking 128 bits.Double hashing is ugly because it\'s more than likely an attacker has built a table to come up with most hashes. Better is to salt your hashes, and mix hashes together. There are also new schemas to "sign" hashes (basically salting), but in a more secure manner. Absolutely do not use multiple iterations of a conventional hash function, like md5(md5(md5(password))). At best you will be getting a marginal increase in security (a scheme like this offers hardly any protection against a GPU attack; just pipeline it.) At worst, you\'re reducing your hash space (and thus security) with every iteration you add. In security, it\'s wise to assume the worst.Do use a password has that\'s been designed by a competent cryptographer to be an effective password hash, and resistant to both brute-force and time-space attacks. These include bcrypt, scrypt, and in some situations PBKDF2. The glibc SHA-256-based hash is also acceptable.I\'m going to go out on a limb and say it\'s more secure in certain circumstances... don\'t downvote me yet though!From a mathematical / cryptographical point of view, it\'s less secure, for reasons that I\'m sure someone else will give you a clearer explanation of than I could.However, there exist large databases of MD5 hashes, which are more likely to contain the "password" text than the MD5 of it.  So by double-hashing you\'re reducing the effectiveness of those databases.Of course, if you use a salt then this advantage (disadvantage?) goes away.