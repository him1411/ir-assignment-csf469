I have a web server which will read large binary files (several megabytes) into byte arrays. The server could be reading several files at the same time (different page requests), so I am looking for the most optimized way for doing this without taxing the CPU too much. Is the code below good enough?Simply replace the whole thing with:However, if you are concerned about the memory consumption, you should not read the whole file into memory all at once at all. You should do that in chunks. I might argue that the answer here generally is "don\'t". Unless you absolutely need all the data at once, consider using a Stream-based API (or some variant of reader / iterator). That is especially important when you have multiple parallel operations (as suggested by the question) to minimise system load and maximise throughput.For example, if you are streaming data to a caller:I would think this:Your code can be factored to this (in lieu of File.ReadAllBytes):Note the Integer.MaxValue - file size limitation placed by the Read method.  In other words you can only read a 2GB chunk at once.Also note that the last argument to the FileStream is a buffer size.I would also suggest reading about FileStream and BufferedStream.As always a simple sample program to profile which is fastest will be most beneficial.Also your underlying hardware will have a large effect on performance.  Are you using server based hard disk drives with large caches and a RAID card with onboard memory cache?  Or are you using a standard drive connected to the IDE port?Depending on the frequency of operations, the size of the files, and the number of files you\'re looking at, there are other performance issues to take into consideration.  One thing to remember, is that each of your byte arrays will be released at the mercy of the garbage collector.  If you\'re not caching any of that data, you could end up creating a lot of garbage and be losing most of your performance to % Time in GC. If the chunks are larger than 85K, you\'ll be allocating to the Large Object Heap(LOH) which will require a collection of all generations to free up (this is very expensive, and on a server will stop all execution while it\'s going on).  Additionally, if you have a ton of objects on the LOH, you can end up with LOH fragmentation (the LOH is never compacted) which leads to poor performance and out of memory exceptions.  You can recycle the process once you hit a certain point, but I don\'t know if that\'s a best practice.The point is, you should consider the full life cycle of your app before necessarily just reading all the bytes into memory the fastest way possible or you might be trading short term performance for overall performance.I\'d say BinaryReader is fine, but can be refactored to this, instead of all those lines of code for getting the length of the buffer:Should be better than using .ReadAllBytes(), since I saw in the comments on the top response that includes .ReadAllBytes() that one of the commenters had problems with files > 600 MB, since a BinaryReader is meant for this sort of thing.  Also, putting it in a using statement ensures the FileStream and BinaryReader are closed and disposed.Use the BufferedStream class in C# to improve performance. A buffer is a block of bytes in memory used to cache data, thereby reducing the number of calls to the operating system. Buffers improve read and write performance.See the following for a code example and additional explanation:\nhttp://msdn.microsoft.com/en-us/library/system.io.bufferedstream.aspxI would recommend trying the Response.TransferFile() method then a Response.Flush() and Response.End() for serving your large files.If you\'re dealing with files above 2Â GB, you\'ll find that the above methods fail.It\'s much easier just to hand the stream off to MD5 and allow that to chunk your file for you: