In our application, we receive text files (.txt, .csv, etc.) from diverse sources. When reading, these files sometimes contain garbage, because the files where created in a different/unknown codepage.Is there a way to (automatically) detect the codepage of a text file? The detectEncodingFromByteOrderMarks, on the StreamReader constructor, works for UTF8  and other unicode marked files, but I\'m looking for a way to detect code pages, like ibm850, windows1252. Thanks for your answers, this is what I\'ve done.The files we receive are from end-users, they do not have a clue about codepages. The receivers are also end-users, by now this is what they know about codepages: Codepages exist, and are annoying.Solution: You can\'t detect the codepage, you need to be told it. You can analyse the bytes and guess it, but that can give some bizarre (sometimes amusing) results. I can\'t find it now, but I\'m sure Notepad can be tricked into displaying English text in Chinese. Anyway, this is what you need to read: \nThe Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!).Specifically Joel says:The Single Most Important Fact About EncodingsIf you completely forget everything I just explained, please remember one extremely important fact. It does not make sense to have a string without knowing what encoding it uses. You can no longer stick your head in the sand and pretend that "plain" text is ASCII.\n  There Ain\'t No Such Thing As Plain Text.If you have a string, in memory, in a file, or in an email message, you have to know what encoding it is in or you cannot interpret it or display it to users correctly.If you\'re looking to detect non-UTF encodings (i.e. no BOM), you\'re basically down to heuristics and statistical analysis of the text. You might want to take a look at the Mozilla paper on universal charset detection (same link, with better formatting via Wayback Machine).Have you tried C# port for Mozilla Universal Charset DetectorExample from http://code.google.com/p/ude/You can\'t detect the codepageThis is clearly false. Every web browser has some kind of universal charset detector to deal with pages which have no indication whatsoever of an encoding. Firefox has one. You can download the code and see how it does it. See some documentation here. Basically, it is a heuristic, but one that works really well.Given a reasonable amount of text, it is even possible to detect the language.Here\'s another one I just found using Google:I know it\'s very late for this question and this solution won\'t appeal to some (because of its english-centric bias and its lack of statistical/empirical testing), but it\'s worked very well for me, especially for processing uploaded CSV data:http://www.architectshack.com/TextFileEncodingDetector.ashxAdvantages:Note: I\'m the one who wrote this class, so obviously take it with a grain of salt! :)Notepad++  has this feature out-of-the-box. It also supports changing it.Looking for different solution, I found that https://code.google.com/p/ude/this solution is kinda heavy.I needed some basic encoding detection, based on 4 first bytes and probably xml charset detection - so I\'ve took some sample source code from internet and added slightly modified version ofhttp://lists.w3.org/Archives/Public/www-validator/2002Aug/0084.htmlwritten for Java.It\'s enough to read probably first 1024 bytes from file, but I\'m loading whole file.I\'ve done something similar in Python. Basically, you need lots of sample data from various encodings, which are broken down by a sliding two-byte window and stored in a dictionary (hash), keyed on byte-pairs providing values of lists of encodings.Given that dictionary (hash), you take your input text and:If you\'ve also sampled UTF encoded texts that do not start with any BOM, the second step will cover those that slipped from the first step.So far, it works for me (the sample data and subsequent input data are subtitles in various languages) with diminishing error rates.The StreamReader class\'s constructor takes a \'detect encoding\' parameter.If someone is looking for a 93.9% solution. This works for me:The tool "uchardet" does this well using character frequency distribution models for each charset.   Larger files and more "typical" files have more confidence (obviously).On ubuntu, you just apt-get install uchardet.   On other systems, get the source, usage & docs here: https://github.com/BYVoid/uchardetIf you can link to a C library, you can use libenca.  See http://cihar.com/software/enca/.  From the man page:Enca reads given text files, or standard input when none are given,\n  and uses knowledge about their language (must be supported by you) and\n  a mixture of parsing, statistical analysis, guessing and black magic\n  to determine their encodings.It\'s GPL v2.Got the same problem but didn\'t found a good solution yet for detecting it automatically .\nNow im using PsPad (www.pspad.com) for that ;) Works fineSince it basically comes down to heuristics, it may help to use the encoding of previously received files from the same source as a first hint.Most people (or applications) do stuff in pretty much the same order every time, often on the same machine, so its quite likely that when Bob creates a .csv file and sends it to Mary it\'ll always be using Windows-1252 or whatever his machine defaults to.Where possible a bit of customer training never hurts either :-)I was actually looking for a generic, not programming way of detecting the file encoding, but I didn\'t find that yet.\nWhat I did find by testing with different encodings was that my text was UTF-7.So where I first was doing:\nStreamReader file = File.OpenText(fullfilename);I had to change it to:\nStreamReader file = new StreamReader(fullfilename, System.Text.Encoding.UTF7);OpenText assumes it\'s UTF-8.you can also create the StreamReader like this\nnew StreamReader(fullfilename, true), the second parameter meaning that it should try and detect the encoding from the byteordermark of the file, but that didn\'t work in my case.As addon to ITmeze post, I\'ve used this function to convert the output of C# port for Mozilla Universal Charset DetectorMSDNOpen file in AkelPad(or just copy/paste a garbled text), go to Edit -> Selection -> Recode... -> check "Autodetect".I use this code to detect Unicode and windows default ansi codepage when reading a file. For other codings a check of content is necessary, manually or by programming. This can de used to save the text with the same encoding as when it was opened. (I use VB.NET)