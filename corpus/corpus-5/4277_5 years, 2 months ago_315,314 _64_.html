I have several hundred PDFs under a directory in UNIX. The names of the PDFs are really long (approx. 60 chars).When I try to delete all PDFs together using the following command:I get the following error:What is the solution to this error?\nDoes this error occur for mv and cp commands as well? If yes, how to solve for these commands?The reason this occurs is because bash actually expands the asterisk to every matching file, producing a very long command line.Try this:Warning: this is a recursive search and will find (and delete) files in subdirectories as well. Tack on -f to the rm command only if you are sure you don\'t want confirmation.If you\'re on Linux, you can do the following to make the command non-recursive:Another option is to use find\'s -delete flag:It\'s a kernel limitation on the size of the command line argument. Use a for loop instead.This is a system issue, related to execve and ARG_MAX constant. There is plenty of documentation about that (see man execve, debian\'s wiki). Basically, the expansion produce a command (with its parameters) that exceeds the ARG_MAX limit.\nOn kernel 2.6.23, the limit was set at 128 kB. This constant has been increased and you can get its value by executing:Use a for loop as it\'s recommended on BashFAQ/095 and there is no limit except for RAM/memory space:Also this is a portable approach as glob have strong and consistant behavior among shells (part of POSIX spec). If you insist, you can use find but really don\'t use xargs as it "is dangerous (broken, exploitable, etc.) when reading non-NUL-delimited input":find has a -delete action:Another answer is to force xargs to process the commands in batches. For instance to delete the files 100 at a time, cd into the directory and run this:echo *.pdf | xargs -n 100 rmOr you can try:You could use a bash array:This way it will erase in batches of 1000 files per step.you can try this:EDIT:\nThiefMaster comment suggest me not to disclose such dangerous practice to young shell\'s jedis, so I\'ll add a more "safer" version (for the sake of preserving things when someone has a "-rf . ..pdf" file)After running the above, just open the /tmp/dummy.sh file in your fav. editor and check every single line for dangerous filenames, commenting them out if found.Then copy the dummy.sh script in your working dir and run it.All this for security reasons.you can use this commend i was facing same problem while copying form source directory to destinationsource directory had files ~3 lakcsi used cp with option -r and it\'s worked for mecp -r abc/ def/it will copy all files from abc to def without giving warning of Argument list too longThe rm command has a limitation of files which you can remove simultaneous.One possibility you can remove them using multiple times the rm command bases on your file patterns, like:You can also remove them trough find command:If they are filenames with spaces or special characters, use:This sentence search all files in the current directory (-maxdepth 1) with extension pdf (-name \'*.pdf\'), and then, delete each one (-exec rm "{}").The expression {} replace the name of the file, and, "{}" set the filename as string, including spaces or special characters.I ran into this problem a few times.  Many of the solutions will run the rm command for each individual file that needs to be deleted. This is very inefficient:I ended up writing a python script to delete the files based on the first 4 characters in the file-name:This worked very well for me. I was able to clear out over 2 million temp files in a folder in about 15 minutes. I commented the tar out of the little bit of code so anyone with minimal to no python knowledge can manipulate this code.And another one:I only know a way around this.\nThe idea is to export that list of pdf files you have into a file. Then split that file into several parts. Then remove pdf files listed in each part.wc -l is to count how many line the list.txt contains. When you have the idea of how long it is, you can decide to split it in half, forth or something. Using split -l command\nFor example, split it in 600 lines each.this will create a few file named xaa,xab,xac and so on depends on how you split it.\nNow to "import" each list in those file into command rm, use this:Sorry for my bad english.I found that for extremely large lists of files (>1e6), these answers were too slow. Here is a solution using parallel processing in python. I know, I know, this isn\'t linux... but nothing else here worked.  (This saved me hours)Using GNU parallel (sudo apt install parallel) is super easyIt runs the commands multithreaded where \'{}\' is the argument passedE.g.ls /tmp/myfiles* | parallel \'rm {}\'Suppose input directory name is input and output directory name is output.\nThen you can use simple loop to copy allI had the same problem with a folder full of temporary images that was growing day by day and this command helped me to clear the folderThe difference with the other commands is the mtime parameter that will take only the files older than X days (in the example 50 days)Using that multiple times, decreasing on every execution the day range, I was able to remove all the unnecessary filesIf you have similar problems with grep, the easiest solution is stepping one dir back and do a recursive search.So instead of you can use:Note it will recursively search subfolders of "search_in_this_dir" directory as well.A bit safer version than using xargs, also not recursive:\n\nls -p | grep -v \'/$\' |  grep \'\\.pdf$\' | while read file; do rm "$file"; done\nFiltering our directories here is a bit unnecessary as \'rm\' won\'t delete it anyway, and it can be removed for simplicity, but why run something that will definitely return error?The below option seems simple to this problem. I got this info from some other thread but it helped me. Just run the above one command and it will do the task.