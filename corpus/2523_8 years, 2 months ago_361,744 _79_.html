I think what I want to do is a fairly common task but I\'ve found no reference on the web. I have text, with punctuation, and I want list of the words. should beBut Python\'s str.split() only works with one argument... So I have all words with the punctuation after I split with whitespace. Any ideas?A case where regular expressions are justified:re.split()re.split(pattern, string[, maxsplit=0])Split string by the occurrences of pattern. If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list. (Incompatibility note: in the original Python 1.5 release, maxsplit was ignored. This has been fixed in later releases.)Another quick way to do this without a regexp is to replace the characters first, as below:So many answers, yet I can\'t find any solution that does efficiently what the title of the questions literally asks for (splitting with multiple separators\xe2\x80\x94instead, many answers remove anything that is not a word, which is different). So here is an answer to the question in the title, that relies on Python\'s standard and efficient re module:where:This re.split() precisely "splits with multiple separators", as asked for in the question title.This solution also does not suffer from problems with non-ASCII characters in words, as well (see the first comment to ghostdog74\'s answer).The re module is much more efficient than doing Python loops and tests "by hand".Another way, without regexPro-Tip: Use string.translate for the fastest string operations Python has.Some proof...First, the slow way (sorry pprzemek):Next, we use re.findall() (as given by the suggested answer). MUCH faster:Finally, we use translate:Explanation:string.translate is implemented in C and unlike many string manipulation functions in Python, string.translate does not produce a new string. So it\'s about as fast as you can get for string substitution.It\'s a bit awkward, though, as it needs a translation table in order to do this magic. You can make a translation table with the maketrans() convenience function. The objective here is to translate all unwanted characters to spaces. A one-for-one substitute. Again, no new data is produced. So this is fast!Next, we use good old split(). split() by default will operate on all whitespace characters, grouping them together for the split. The result will be the list of words that you want. And this approach is almost 4x faster than re.findall()!Kinda late answer :), but I had a similar dilemma and didn\'t want to use \'re\' module.Then this becomes a three-liner:ExplanationThis is what in Haskell is known as the List monad. The idea behind the monad is that once "in the monad" you "stay in the monad" until something takes you out. For example in Haskell, say you map the python range(n) -> [1,2,...,n] function over a List. If the result is a List, it will be append to the List in-place, so you\'d get something like map(range, [3,4,1]) -> [0,1,2,0,1,2,3,0]. This is known as map-append (or mappend, or maybe something like that). The idea here is that you\'ve got this operation you\'re applying (splitting on a token), and whenever you do that, you join the result into the list.You can abstract this into a function and have tokens=string.punctuation by default. Advantages of this approach:First, I want to agree with others that the regex or str.translate(...) based solutions are most performant.  For my use case the performance of this function wasn\'t significant, so I wanted to add ideas that I considered with that criteria.My main goal was to generalize ideas from some of the other answers into one solution that could work for strings containing more than just regex words (i.e., blacklisting the explicit subset of punctuation characters vs whitelisting word characters).Note that, in any approach, one might also consider using string.punctuation in place of a manually defined list.I was surprised to see no answer so far uses re.sub(...).  I find it a simple and natural approach to this problem.In this solution, I nested the call to re.sub(...) inside re.split(...) \xe2\x80\x94 but if performance is critical, compiling the regex outside could be beneficial \xe2\x80\x94 for my use case, the difference wasn\'t significant, so I prefer simplicity and readability.This is a few more lines, but it has the benefit of being expandable without having to check whether you need to escape a certain character in regex.It would have been nice to be able to map the str.replace to the string instead, but I don\'t think it can be done with immutable strings, and while mapping against a list of characters would work, running every replacement against every character sounds excessive. (Edit: See next option for a functional example.)(In Python 2, reduce is available in global namespace without importing it from functools.)try this:this will print [\'Hey\', \'you\', \'what\', \'are\', \'you\', \'doing\', \'here\']Use replace two times:results in: I\'m re-acquainting myself with Python and needed the same thing.\nThe findall solution may be better, but I came up with this:I like re, but here is my solution without it:sep.__contains__ is a method used by \'in\' operator. Basically it is the same asbut is more convenient here.groupby gets our string and function. It splits string in groups using that function:  whenever a value of function changes - a new group is generated. So, sep.__contains__ is exactly what we need.groupby returns a sequence of pairs, where pair[0] is a result of our function and pair[1] is a group. Using \'if not k\' we filter out groups with separators (because a result of sep.__contains__ is True on separators). Well, that\'s all - now we have a sequence of groups where each one is a word (group is actually an iterable so we use join to convert it to string).This solution is quite general, because it uses a function to separate string (you can split by any condition you need). Also, it doesn\'t create intermediate strings/lists (you can remove join and the expression will become lazy, since each group is an iterator)Another way to achieve this is to use the Natural Language Tool Kit (nltk).This prints: [\'Hey\', \'you\', \'what\', \'are\', \'you\', \'doing\', \'here\']The biggest drawback of this method is that you need to install the nltk package.The benefits are that you can do a lot of fun stuff with the rest of the nltk package once you get your tokens.got same problem as @ooboo and find this topic\n@ghostdog74 inspired me, maybe someone finds my solution usefullinput something in space place and split using same character if you dont want to split at spaces.Here is my go at a split with multiple deliminaters:I like the replace() way the best. The following procedure changes all separators defined in a string splitlist to the first separator in splitlist and then splits the text on that one separator. It also accounts for if splitlist happens to be an empty string. It returns a list of words, with no empty strings in it.Here is the usage:First of all, I don\'t think that your intention is to actually use punctuation as delimiters in the split functions.  Your description suggests that you simply want to eliminate punctuation from the resultant strings.I come across this pretty frequently, and my usual solution doesn\'t require re.(requires import string):As a traditional function, this is still only two lines with a list comprehension (in addition to import string):It will also naturally leave contractions and hyphenated words intact. You can always use text.replace("-", " ") to turn hyphens into spaces before the split.For a more general solution (where you can specify the characters to eliminate), and without a list comprehension, you get:Of course, you can always generalize the lambda function to any specified string of characters as well.First of all, always use re.compile() before performing any RegEx operation in a loop because it works faster than normal operation.so for your problem first compile the pattern and then perform action on it.Here is the answer with some explanation.or in one line, we can do like this:updated answerI think the following is the best answer to suite your needs :\\W+ maybe suitable for this case, but may not be suitable for other cases.Heres my take on it....Create a function that takes as input two string (the source string to be split and the splitlist string of delimiters) and outputs a list of split words:You want Python\'s RegEx module\'s findall() method:http://www.regular-expressions.info/python.htmlExampleUse list comprehensions for this stuff...it seems easierI find this easier to comprehend (read..maintain) than using regexp, simply because I am not that good at regexp...which is the case with most of us :) . Also if you know what set of separators you might be using, you can keep them in a set. With a very huge set, this might be slower...but the \'re\' module is slow as well.