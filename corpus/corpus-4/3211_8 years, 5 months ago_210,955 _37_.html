My application runs as a background process on Linux. It is currently started at the command line in a Terminal window.Recently a user was executing the application for a while and it died mysteriously. The text:Killedwas on the terminal. This happened two times. I asked if someone at a different Terminal used the kill command to kill the process? No.Under what conditions would Linux decide to kill my process?  I believe the shell displayed "Killed" because the process died after receiving the kill(9) signal.  If Linux sent the kill signal should there be a message in a system log somewhere that explains why it was killed?If the user or sysadmin did not kill the program the kernel may have. The kernel would only kill a process under exceptional circumstances such as extreme resource starvation (think mem+swap exhaustion).This looks like a good article on the subject: Taming the OOM killer.The gist is that Linux overcommits memory. When a process asks for more space, Linux will give it that space, even if it is claimed by another process, under the assumption that nobody actually uses all of the memory they ask for. The process will get exclusive use of the memory it has allocated when it actually uses it, not when it asks for it. This makes allocation quick, and might allow you to "cheat" and allocate more memory than you really have. However, once processes start using this memory, Linux might realize that it has been too generous in allocating memory it doesn\'t have, and will have to kill off a process to free some up. The process to be killed is based on a score taking into account runtime (long-running processes are safer), memory usage (greedy processes are less safe), and a few other factors, including a value you can adjust to make a process less likely to be killed. It\'s all described in the article in a lot more detail.Edit: And here is another article that explains pretty well how a process is chosen (annotated with some kernel code examples). The great thing about this is that it includes some commentary on the reasoning behind the various badness() rules.Try:Where -B100 signifies the number of lines before the kill happened.Say you have 512 RAM + 1GB Swap memory. So in theory, your CPU has access to total of 1.5GB of virtual memory.Now, for some time everything is running fine within 1.5GB of total memory. But all of sudden (or gradually) your system has started consuming more and more memory and it reached at a point around 95% of total memory used.Now say any process has requested large chunck of memory from the kernel. Kernel check for the available memory and find that there is no way it can allocate your process more memory. So it will try to free some memory calling/invoking OOMKiller (http://linux-mm.org/OOM).OOMKiller has its own algorithm to score the rank for every process. Typically which process uses more memory becomes the victim to be killed.Typically in /var/log directory. Either /var/log/kern.log or /var/log/dmesgHope this will help you.As dwc and Adam Jaskiewicz have stated, the culprit is likely the OOM Killer. However, the next question that follows is: How do I prevent this?There are several ways:I found (2) to be especially easy to implement, thanks to this article. This is the Linux out of memory manager (OOM). Your process was selected due to \'badness\' - a combination of recentness, resident size (memory in use, rather than just allocated) and other factors.You\'ll see a message like:The PAM module to limit resources caused exactly the results you described: My process died mysteriously with the text Killed on the console window. No log output, neither in syslog nor in kern.log. The top program helped me to discover that exactly after one minute of CPU usage my process gets killed. A tool like systemtap (or a tracer) can monitor kernel signal-transmission logic and report.  e.g., https://sourceware.org/systemtap/examples/process/sigmon.stpThe filtering if block in that script can be adjusted to taste, or eliminated to trace systemwide signal traffic.  Causes can be further isolated by collecting backtraces (add a print_backtrace() and/or print_ubacktrace() to the probe, for kernel- and userspace- respectively).In an lsf environment (interactive or otherwise) if the application exceeds memory utilization beyond some preset threshold by the admins on the queue or the resource request in submit to the queue the processes will be killed so other users don\'t fall victim to a potential run away. It doesn\'t always send an email when it does so, depending on how its set up.One solution in this case is to find a queue with larger resources or define larger resource requirements in the submission.You may also want to review man ulimitAlthough I don\'t remember ulimit resulting in Killed its been a while since I needed that.We have had recurring problems under Linux at a customer site (Red Hat, I think), with OOMKiller (out-of-memory killer) killing both our principle application (i.e. the reason the server exists) and it\'s data base processes.  In each case OOMKiller simply decided that the processes were using to much resources... the machine wasn\'t even about to fail for lack of resources.  Neither the application nor it\'s database has problems with memory leaks (or any other resource leak).I am not a Linux expert, but I rather gathered it\'s algorithm for deciding when to kill something and what to kill is complex.  Also, I was told (I can\'t speak as to the accuracy of this) that OOMKiller is baked into the Kernel and you can\'t simply not run it.The user has the ability to kill his own programs, using kill or Control+C, but I get the impression that\'s not what happened, and that the user complained to you.root has the ability to kill programs of course, but if someone has root on your machine and is killing stuff you have bigger problems.If you are not the sysadmin, the sysadmin may have set up quotas on CPU, RAM, ort disk usage and auto-kills processes that exceed them.Other than those guesses, I\'m not sure without more info about the program.I encountered this problem lately. Finally, I found my processes were killed just after Opensuse zypper update was called automatically. To disable zypper update solved my problem.