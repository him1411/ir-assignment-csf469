How can I request a random row (or as close to truly random as is possible) in pure SQL?See this post: SQL to Select a random row from a database table. It goes through methods for doing this in MySQL, PostgreSQL, Microsoft SQL Server, IBM DB2 and Oracle (the following is copied from that link):Select a random row with MySQL:Select a random row with PostgreSQL:Select a random row with Microsoft SQL Server:Select a random row with IBM DB2Select a random record with Oracle:Solutions like Jeremies:work, but they need a sequential scan of all the table (because the random value associated with each row needs to be calculated - so that the smallest one can be determined), which can be quite slow for even medium sized tables. My recommendation would be to use some kind of indexed numeric column (many tables have these as their primary keys), and then write something like:This works in logarithmic time, regardless of the table size, if num_value is indexed. One caveat: this assumes that num_value is equally distributed in the range 0..MAX(num_value). If your dataset strongly deviates from this assumption, you will get skewed results (some rows will appear more often than others).I don\'t know how efficient this is, but I\'ve used it before:Because GUIDs are pretty random, the ordering means you get a random row.takes 7.4 millisecondstakes 0.0065 milliseconds!I will definitely go with latter method.You didn\'t say which server you\'re using. In older versions of SQL Server, you can use this:In SQL Server 2005 and up, you can use TABLESAMPLE to get a random sample that\'s repeatable:For SQL Servernewid()/order by will work, but will be very expensive for large result sets because it has to generate an id for every row, and then sort them.TABLESAMPLE() is good from a performance standpoint, but you will get clumping of results (all rows on a page will be returned).For a better performing true random sample, the best way is to filter out rows randomly. I found the following code sample in the SQL Server Books Online article Limiting Results Sets by Using TABLESAMPLE:If you really want a random sample of\n  individual rows, modify your query to\n  filter out rows randomly, instead of\n  using TABLESAMPLE. For example, the\n  following query uses the NEWID\n  function to return approximately one\n  percent of the rows of the\n  Sales.SalesOrderDetail table:The SalesOrderID column is included in\n  the CHECKSUM expression so that\n  NEWID() evaluates once per row to\n  achieve sampling on a per-row basis.\n  The expression CAST(CHECKSUM(NEWID(),\n  SalesOrderID) & 0x7fffffff AS float /\n  CAST (0x7fffffff AS int) evaluates to\n  a random float value between 0 and 1.When run against a table with 1,000,000 rows, here are my results:If you can get away with using TABLESAMPLE, it will give you the best performance. Otherwise use the newid()/filter method. newid()/order by should be last resort if you have a large result set.If possible, use stored statements to avoid the inefficiency of both indexes on RND() and creating a record number field.For SQL Server 2005 and 2008, if we want a random sample of individual rows (from Books Online):Best way is putting a random value in a new column just for that purpose, and using something like this (pseude code + SQL):This is the solution employed by the MediaWiki code. Of course, there is some bias against smaller values, but they found that it was sufficient to wrap the random value around to zero when no rows are fetched.newid() solution may require a full table scan so that each row can be assigned a new guid, which will be much less performant.rand() solution may not work at all (i.e. with MSSQL) because the function will be evaluated just once, and every row will be assigned the same "random" number.As pointed out in @BillKarwin\'s comment on @cnu\'s answer...When combining with a LIMIT, I\'ve found that it performs much better (at least with PostgreSQL 9.1) to JOIN with a random ordering rather than to directly order the actual rows: e.g.\n\nSELECT * FROM tbl_post AS t\nJOIN ...\nJOIN ( SELECT id, CAST(-2147483648 * RANDOM() AS integer) AS rand\n       FROM tbl_post\n       WHERE create_time >= 1349928000\n     ) r ON r.id = t.id\nWHERE create_time >= 1349928000 AND ...\nORDER BY r.rand\nLIMIT 100\nJust make sure that the \'r\' generates a \'rand\' value for every possible key value in the complex query which is joined with it but still limit the number of rows of \'r\' where possible.The CAST as Integer is especially helpful for PostgreSQL 9.2 which has specific sort optimisation for integer and single precision floating types.Most of the solutions here aim to avoid sorting, but they still need to make a sequential scan over a table.There is also a way to avoid the sequential scan by switching to index scan. If you know the index value of your random row you can get the result almost instantially. The problem is - how to guess an index value.The following solution works on PostgreSQL 8.4:I above solution you guess 10 various random index values from range 0 .. [last value of id]. The number 10 is arbitrary - you may use 100 or 1000 as it (amazingly) doesn\'t have a big impact on the response time. There is also one problem - if you have sparse ids you might miss. The solution is to have a backup plan :) In this case an pure old order by random() query. When combined id looks like this:Not the union ALL clause. In this case if the first part returns any data the second one is NEVER executed!In late, but got here via Google, so for the sake of posterity, I\'ll add an alternative solution. Another approach is to use TOP twice, with alternating orders. I don\'t know if it is "pure SQL", because it uses a variable in the TOP, but it works in SQL Server 2008. Here\'s an example I use against a table of dictionary words, if I want a random word.Of course, @idx is some randomly-generated integer that ranges from 1 to COUNT(*) on the target table, inclusively. If your column is indexed, you\'ll benefit from it too. Another advantage is that you can use it in a function, since NEWID() is disallowed.Lastly, the above query runs in about 1/10 of the exec time of a NEWID()-type of query on the same table. YYMV.You may also try using new id() function.Just write a your query and use order by new id() function. It quite random.For MySQL to get random recordMore detail http://jan.kneschke.de/projects/mysql/order-by-rand/Didn\'t quite see this variation in the answers yet. I had an additional constraint where I needed, given an initial seed, to select the same set of rows each time.For MS SQL:Minimum example:Normalized execution time: 1.00NewId() example:Normalized execution time: 1.02NewId() is insignificantly slower than rand(checksum(*)), so you may not want to use it against large record sets.Selection with Initial Seed:If you need to select the same set given a seed, this seems to work.In MSSQL (tested on 11.0.5569) using is significantly faster thanInsted of using RAND(), as it is not encouraged, you may simply get max ID (=Max):get a random between 1..Max (=My_Generated_Random)and then run this SQL:Note that it will check for any rows which Ids are EQUAL or HIGHER than chosen value.\nIt\'s also possible to hunt for the row down in the table, and get an equal or lower ID than the My_Generated_Random, then modify the query like this:In SQL Server you can combine TABLESAMPLE with NEWID() to get pretty good randomness and still have speed.  This is especially useful if you really only want 1, or a small number, of rows.I have to agree with CD-MaN: Using "ORDER BY RAND()" will work nicely for small tables or when you do your SELECT only a few times.I also use the "num_value >= RAND() * ..." technique, and if I really want to have random results I have a special "random" column in the table that I update once a day or so. That single UPDATE run will take some time (especially because you\'ll have to have an index on that column), but it\'s much faster than creating random numbers for every row each time the select is run.Be careful because TableSample doesn\'t actually return a random sample of rows. It directs your query to look at a random sample of the 8KB pages that make up your row. Then, your query is executed against the data contained in these pages. Because of how data may be grouped on these pages (insertion order, etc), this could lead to data that isn\'t actually a random sample. See: http://www.mssqltips.com/tip.asp?tip=1308This MSDN page for TableSample includes an example of how to generate an actualy random sample of data.http://msdn.microsoft.com/en-us/library/ms189108.aspx It seems that many of the ideas listed still use orderingHowever, if you use a temporary table, you are able to assign a random index (like many of the solutions have suggested), and then grab the first one that is greater than an arbitrary number between 0 and 1.For example (for DB2):A simple and efficient way from http://akinas.com/pages/en/blog/mysql_random_row/There is better solution for Oracle instead of using dbms_random.value, while it requires full scan to order rows by dbms_random.value and it is quite slow for large tables.Use this instead:For Firebird: