One of the stated reasons for knowing assembler is that, on occasion, it can be employed to write code that will be more performant than writing that code in a higher-level language, C in particular.  However, I\'ve also heard it stated many times that although that\'s not entirely false, the cases where assembler can actually be used to generate more performant code are both extremely rare and require expert knowledge of and experience with assembler.  This question doesn\'t even get into the fact that assembler instructions will be machine-specific and non-portable, or any of the other aspects of assembler.  There are plenty of good reasons for knowing assembler besides this one, of course, but this is meant to be a specific question soliciting examples and data, not an extended discourse on assembler versus higher-level languages.Can anyone provide some specific examples of cases where assembler will be faster than well-written C code using a modern compiler, and can you support that claim with profiling evidence?  I am pretty confident these cases exist, but I really want to know exactly how esoteric these cases are, since it seems to be a point of some contention.Here is a real world example: Fixed point multiplies. These don\'t only come handy on devices without floating point, they shine when it comes to precision as they give you 32 bits of precision with a predictable error (float only has 23 bit and it\'s harder to predict precision loss)One way to write a fixed point multiply on a 32 bit architecture looks like this:The problem with this code is that we do something that can\'t be directly expressed in the C-language. We want to multiply two 32 bit numbers and get a 64 bit result of which we return the middle 32 bit. However, in C this multiply does not exist. All you can do is to promote the integers to 64 bit and do a 64*64 = 64 multiply. The x86 (ARM, MIPS and others) can however do the multiply in a single instruction. Lots of compilers still ignore this fact and generate code that calls a runtime library function to do the multiply. The shift by 16 is also often done by a library routine (also the x86 can do such shifts).So we\'re left with one or two library calls just for a multiply. This has serious consequences. Not only is the shift slower, registers must be preserved across the function calls and it does not help inlining and code-unrolling either. If you rewrite the same code in assembler you can gain a significant speed boost.In addition to this: using ASM is not the best way to solve the problem. Most compilers allow you to use some assembler instructions in intrinsic form if you can\'t express them in C. The VS.NET2008 compiler for example exposes the 32*32=64 bit mul as __emul and the 64 bit shift as __ll_rshift.Using intrinsics you can rewrite the function in a way that the C-compiler has a chance to understand what\'s going on. This allows the code to be inlined, register allocated, common subexpression elimination and constant propagation can be done as well. You\'ll get a huge performance improvement over the hand-written assembler code that way.For reference: The end-result for the fixed-point mul for the VS.NET compiler is:Btw - The performance difference of fixed point divides are even worse. I had improvements up to factor 10 for division heavy fixed point code by writing a couple of asm-lines.Edit:Using Visual c++ 2013 gives thesame assembly code for both ways.Many years ago I was teaching someone to program in C. The exercise was to rotate a graphic through 90 degrees. He came back with a solution that took several minutes to complete, mainly because he was using multiplies and divides etc. I showed him how to recast the problem using bit shifts, and the time to process came down to about 30 seconds on the non-optimizing compiler he had. I had just got an optimizing compiler and same code rotated the graphic in < 5 seconds. I looked at the assembly code that the compiler was generating, and from what I saw decided there and then that my days writing assembler were over.Pretty much anytime the compiler sees floating point code, a hand written version will be quicker. The primary reason is that the compiler can\'t perform any robust optimisations. See this article from MSDN for a discussion on the subject. Here\'s an example where the assembly version is twice the speed as the C version (compiled with VS2K5):And some numbers from my PC running a default release build*:Out of interest, I swapped the loop with a dec/jnz and it made no difference to the timings - sometimes quicker, sometimes slower. I guess the memory limited aspect dwarves other optimisations.Whoops, I was running a slightly different version of the code and it outputted the numbers the wrong way round (i.e. C was faster!). Fixed and updated the results.Without giving any specific example or profiler evidence, you can write better assembler than the compiler when you know more than the compiler.In the general case, a modern C compiler knows much more about how to optimize the code in question: it knows how the processor pipeline works, it can try to reorder instructions quicker than a human can, and so on - it\'s basically the same as a computer being as good as or better than the best human player for boardgames, etc. simply because it can make searches within the problem space faster than most humans. Although you theoretically can perform as well as the computer in a specific case, you certainly can\'t do it at the same speed, making it infeasible for more than a few cases (i.e. the compiler will most certainly outperform you if you try to write more than a few routines in assembler).On the other hand, there are cases where the compiler does not have as much information - I\'d say primarily when working with different forms of external hardware, of which the compiler has no knowledge. The primary example probably being device drivers, where assembler combined with a human\'s intimate knowledge of the hardware in question can yield better results than a C compiler could do.Others have mentioned special purpose instructions, which is what I\'m talking in the paragraph above - instructions of which the compiler might have limited or no knowledge at all, making it possible for a human to write faster code.Only when using some special purpose instruction sets the compiler doesn\'t support.To maximize the computing power of a modern CPU with multiple pipelines and predictive branching you need to structure the assembly program in a way that makes it a) almost impossible for a human to write b) even more impossible to maintain.Also, better algorithms, data structures and memory management will give you at least an order of magnitude more performance than the micro-optimizations you can do in assembly.In my job, there are three reasons for me to know and use assembly.  In order of importance:Debugging - I often get library code that has bugs or incomplete documentation.  I figure out what it\'s doing by stepping in at the assembly level.  I have to do this about once a week.  I also use it as a tool to debug problems in which my eyes don\'t spot the idiomatic error in C/C++/C#.  Looking at the assembly gets past that.Optimizing - the compiler does fairly well in optimizing, but I play in a different ballpark than most.  I write image processing code that usually starts with code that looks like this:the "do something part" typically happens on the order of several million times (ie, between 3 and 30).  By scraping cycles in that "do something" phase, the performance gains are hugely magnified.  I don\'t usually start there - I usually start by writing the code to work first, then do my best to refactor the C to be naturally better (better algorithm, less load in the loop etc).  I usually need to read assembly to see what\'s going on and rarely need to write it.  I do this maybe every two or three months.doing something the language won\'t let me.  These include - getting the processor architecture and specific processor features, accessing flags not in the CPU (man, I really wish C gave you access to the carry flag), etc.  I do this maybe once a year or two years.Although C is "close" to the low-level manipulation of 8-bit, 16-bit, 32-bit, 64-bit data, there are a few mathematical operations not supported by C which can often be performed elegantly in certain assembly instruction sets:Fixed-point multiplication: The product of two 16-bit numbers is a 32-bit number. But the rules in C says that the product of two 16-bit numbers is a 16-bit number, and the product of two 32-bit numbers is a 32-bit number -- the bottom half in both cases. If you want the top half of a 16x16 multiply or a 32x32 multiply, you have to play games with the compiler. The general method is to cast to a larger-than-necessary bit width, multiply, shift down, and cast back:In this case the compiler may be smart enough to know that you\'re really just trying to get the top half of a 16x16 multiply and do the right thing with the machine\'s native 16x16multiply. Or it may be stupid and require a library call to do the 32x32 multiply that\'s way overkill because you only need 16 bits of the product -- but the C standard doesn\'t give you any way to express yourself.Certain bitshifting operations (rotation/carries): This is not too inelegant in C, but again, unless the compiler is smart enough to realize what you are doing, it\'s going to do a lot of "unnecessary" work. Many assembly instruction sets allow you to rotate or shift left/right with the result in the carry register, so you could accomplish the above in 34 instructions: load a pointer to the beginning of the array, clear the carry, and perform 32 8-bit right-shifts, using auto-increment on the pointer.For another example, there are linear feedback shift registers (LFSR) that are elegantly performed in assembly: Take a chunk of N bits (8, 16, 32, 64, 128, etc), shift the whole thing right by 1 (see above algorithm), then if the resulting carry is 1 then you XOR in a bit pattern that represents the polynomial.Having said that, I wouldn\'t resort to these techniques unless I had serious performance constraints. As others have said, assembly is much harder to document/debug/test/maintain than C code: the performance gain comes with some serious costs.edit: 3. Overflow detection is possible in assembly (can\'t really do it in C), this makes some algorithms much easier.Short answer?  Sometimes.Technically every abstraction has a cost and a programming language is an abstraction for how the CPU works.  C however is very close.  Years ago I remember laughing out loud when I logged onto my UNIX account and got the following fortune message (when such things were popular):The C Programming Language -- A\n  language which combines the\n  flexibility of assembly language with\n  the power of assembly language.It\'s funny because it\'s true: C is like portable assembly language.It\'s worth noting that assembly language just runs however you write it.  There is however a compiler in between C and the assembly language it generates and that is extremely important because how fast your C code is has an awful lot to do with how good your compiler is.When gcc came on the scene one of the things that made it so popular was that it was often so much better than the C compilers that shipped with many commercial UNIX flavours.  Not only was it ANSI C (none of this K&R C rubbish), was more robust and typically produced better (faster) code.  Not always but often.         I tell you all this because there is no blanket rule about the speed of C and assembler because there is no objective standard for C.Likewise, assembler varies a lot depending on what processor you\'re running, your system spec, what instruction set you\'re using and so on.  Historically there have been two CPU architecture families: CISC and RISC.  The biggest player in CISC was and still is the Intel x86 architecture (and instruction set).  RISC dominated the UNIX world (MIPS6000, Alpha, Sparc and so on).  CISC won the battle for the hearts and minds.Anyway, the popular wisdom when I was a younger developer was that hand-written x86 could often be much faster than C because the way the architecture worked, it had a complexity that benefitted from a human doing it.  RISC on the other hand seemed designed for compilers so noone (I knew) wrote say Sparc assembler.  I\'m sure such people existed but no doubt they\'ve both gone insane and been institutionalized by now.Instruction sets are an important point even in the same family of processors.  Certain Intel processors have extensions like SSE through SSE4.  AMD had their own SIMD instructions.  The benefit of a programming language like C was someone could write their library so it was optimized for whichever processor you were running on.  That was hard work in assembler.There are still optimizations you can make in assembler that no compiler could make and a well written assembler algoirthm will be as fast or faster than it\'s C equivalent.  The bigger question is: is it worth it?Ultimately though assembler was a product of its time and was more popular at a time when CPU cycles were expensive.  Nowadays a CPU that costs $5-10 to manufacture (Intel Atom) can do pretty much anything anyone could want.  The only real reason to write assembler these days is for low level things like some parts of an operating system (even so the vast majority of the Linux kernel is written in C), device drivers, possibly embedded devices (although C tends to dominate there too) and so on.  Or just for kicks (which is somewhat masochistic).A use case which might not apply anymore but for your nerd pleasure: On the Amiga, the CPU and the graphics/audio chips would fight for accessing a certain area of RAM (the first 2MB of RAM to be specific). So when you had only 2MB RAM (or less), displaying complex graphics plus playing sound would kill the performance of the CPU.In assembler, you could interleave your code in such a clever way that the CPU would only try to access the RAM when the graphics/audio chips were busy internally (i.e. when the bus was free). So by reordering your instructions, clever use of the CPU cache, the bus timing, you could achieve some effects which were simply not possible using any higher level language because you had to time every command, even insert NOPs here and there to keep the various chips out of each others radar.Which is another reason why the NOP (No Operation - do nothing) instruction of the CPU can actually make your whole application run faster.[EDIT] Of course, the technique depends on a specific hardware setup. Which was the main reason why many Amiga games couldn\'t cope with faster CPUs: The timing of the instructions was off.Point one which is not the answer.\nEven if you never program in it, I find it useful to know at least one assembler instruction set. This is part of the programmers never-ending quest to know more and therefore be better. Also useful when stepping into frameworks you don\'t have the source code to and having at least a rough idea what is going on. It also helps you to understand JavaByteCode and .Net IL as they are both similar to assembler.To answer the question when you have a small amount of code or a large amount of time. Most useful for use in embedded chips, where low chip complexity and poor competition in compilers targeting these chips can tip the balance in favour of humans. Also for restricted devices you are often trading off code size/memory size/performance in a way that would be hard to instruct a compiler to do. e.g. I know this user action is not called often so I will have small code size and poor performance, but this other function that look similar is used every second so I will have a larger code size and faster performance. That is the sort of trade off a skilled assembly programmer can use.I would also like to add there is a lot of middle ground where you can code in C compile and examine the Assembly produced, then either change you C code or tweak and maintain as assembly. My friend works on micro controllers, currently chips for controlling small electric motors. He works in a combination of low level c and Assembly. He once told me of a good day at work where he reduced the main loop from 48 instructions to 43. He is also faced with choices like the code has grown to fill the 256k chip and the business is wanting a new feature, do you I would like to add as a commercial developer with quite a portfolio or languages, platforms, types of applications I have never once felt the need to dive into writing  assembly. I have how ever always appreciated the knowledge I gained about it. And sometimes debugged into it.I know I have far more answered the question "why should I learn assembler" but I feel it is a more important question then when is it faster.so lets try once more\nYou should be thinking about assembly\nRemember to compare your assembly to compiler generated to see which is faster/smaller/better.\nDavid.I\'m surprised no one said this. The strlen() function is much faster if written in assembly! In C, the best thing you can do iswhile in assembly you can speed it up considerably:the length is in ecx. This compares 4 characters at time, so it\'s 4 times faster. And think using the high order word of eax and ebx, it will become 8 times faster that the previous C routine!Matrix operations using SIMD instructions is probably faster than compiler generated code.I can\'t give the specific examples because it was too many years ago, but there were plenty of cases where hand-written assembler could out-perform any compiler. Reasons why:You could deviate from calling conventions, passing arguments in registers.You could carefully consider how to use registers, and avoid storing variables in memory.For things like jump tables, you could avoid having to bounds-check the index.Basically, compilers do a pretty good job of optimizing, and that is nearly always "good enough", but in some situations (like graphics rendering) where you\'re paying dearly for every single cycle, you can take shortcuts because you know the code, where a compiler could not because it has to be on the safe side.In fact, I have heard of some graphics rendering code where a routine, like a line-draw or polygon-fill routine, actually generated a small block of machine code on the stack and executed it there, so as to avoid continual decision-making about line style, width, pattern, etc.That said, what I want a compiler to do is generate good assembly code for me but not be too clever, and they mostly do that. In fact, one of the things I hate about Fortran is its scrambling the code in an attempt to "optimize" it, usually to no significant purpose.Usually, when apps have performance problems, it is due to wasteful design. These days, I would never recommend assembler for performance unless the overall app had already been tuned within an inch of its life, still was not fast enough, and was spending all its time in tight inner loops.Added: I\'ve seen plenty of apps written in assembly language, and the main speed advantage over a language like C, Pascal, Fortran, etc. was because the programmer was far more careful when coding in assembler. He or she is going to write roughly 100 lines of code a day, regardless of language, and in a compiler language that\'s going to equal 3 or 400 instructions.You don\'t actually know whether your well-written C code is really fast if you haven\'t looked at the disassembly of what compiler produces. Many times you look at it and see that "well-written" was subjective.So it\'s not necessary to write in assembler to get fastest code ever, but it\'s certainly worth to know assembler for the very same reason.A few examples from my experience:Access to instructions that are not accessible from C. For instance, many architectures (like x86-64, IA-64, DEC Alpha, and 64-bit MIPS or PowerPC) support a 64 bit by 64 bit multiplication producing a 128 bit result. GCC recently added an extension providing access to such instructions, but before that assembly was required. And access to this instruction can make a huge difference on 64-bit CPUs when implementing something like RSA - sometimes as much as a factor of 4 improvement in performance.Access to CPU-specific flags. The one that has bitten me a lot is the carry flag; when doing a multiple-precision addition, if you don\'t have access to the CPU carry bit one must instead compare the result to see if it overflowed, which takes 3-5 more instructions per limb; and worse, which are quite serial in terms of data accesses, which kills performance on modern superscalar processors. When processing thousands of such integers in a row, being able to use addc is a huge win (there are superscalar issues with contention on the carry bit as well, but modern CPUs deal pretty well with it).SIMD. Even autovectorizing compilers can only do relatively simple cases, so if you want good SIMD performance it\'s unfortunately often necessary to write the code directly. Of course you can use intrinsics instead of assembly but once you\'re at the intrinsics level you\'re basically writing assembly anyway, just using the compiler as a register allocator and (nominally) instruction scheduler. (I tend to use intrinsics for SIMD simply because the compiler can generate the function prologues and whatnot for me so I can use the same code on Linux, OS X, and Windows without having to deal with ABI issues like function calling conventions, but other than that the SSE intrinsics really aren\'t very nice - the Altivec ones seem better though I don\'t have much experience with them). As examples of things a (current day) vectorizing compiler can\'t figure out, read about bitslicing AES or SIMD error correction - one could imagine a compiler that could analyze algorithms and generate such code, but it feels to me like such a smart compiler is at least 30 years away from existing (at best).On the other hand, multicore machines and distributed systems have shifted many of the biggest performance wins in the other direction - get an extra 20% speedup writing your inner loops in assembly, or 300% by running them across multiple cores, or 10000% by running them across a cluster of machines. And of course high level optimizations (things like futures, memoization, etc) are often much easier to do in a higher level language like ML or Scala than C or asm, and often can provide a much bigger performance win. So, as always, there are tradeoffs to be made.More often than you think, C needs to do things that seem to be unneccessary from an Assembly coder\'s point of view just because the C standards say so.Integer promotion, for example. If you want to shift a char variable in C, one would usually expect that the code would do in fact just that, a single bit shift.The standards, however, enforce the compiler to do a sign extend to int before the shift and truncate the result to char afterwards which might complicate code depending on the target processor\'s architecture.I think the general case when assembler is faster is when a smart assembly programmer looks at the compiler\'s output and says "this is a critical path for performance and I can write this to be more efficient" and then that person tweaks that assembler or rewrites it from scratch.Tight loops, like when playing with images, since an image may cosist of millions of pixels. Sitting down and figuring out how to make best use of the limited number of processor registers can make a difference. Here\'s a real life sample:http://danbystrom.se/2008/12/22/optimizing-away-ii/Then often processors have some esoteric instructions which are too specialized for a compiler to bother with, but on occasion an assembler programmer can make good use of them. Take the XLAT instruction for example. Really great if you need to do table look-ups in a loop and the table is limited to 256 bytes!Updated: Oh, just come to think of what\'s most crucial when we speak of loops in general: the compiler has often no clue on how many iterations that will be the common case! Only the programmer know that a loop will be iterated MANY times and that it therefore will be beneficial to prepare for the loop with some extra work, or if it will be iterated so few times that the set-up actually will take longer than the iterations expected.It all depends on your workload.For day-to-day operations, C and C++ are just fine, but there are certain workloads (any transforms involving video (compression, decompression, image effects, etc)) that pretty much require assembly to be performant.They also usually involve using CPU specific chipset extensions (MME/MMX/SSE/whatever) that are tuned for those kinds of operation.I have an operation of transposition of bits that needs to be done, on 192 or 256 bits every interrupt, that happens every 50 microseconds.It happens by a fixed map(hardware constraints). Using C, it took around 10 microseconds to make. When I translated this to Assembler, taking into account the specific features of this map, specific register caching, and using bit oriented operations; it took less than 3.5 microsecond to perform.One of the posibilities to the CP/M-86 version of PolyPascal (sibling to Turbo Pascal) was to replace the "use-bios-to-output-characters-to-the-screen" facility with a machine language routine which in essense was given the x, and y, and the string to put there.This allowed to update the screen much, much faster than before!There was room in the binary to embed machine code (a few hundred bytes) and there was other stuff there too, so it was essential to squeeze as much as possible.It turnes out that since the screen was 80x25 both coordinates could fit in a byte each, so both could fit in a two-byte word.  This allowed to do the calculations needed in fewer bytes since a single add could manipulate both values simultaneously.  To my knowledge there is no C compilers which can merge multiple values in a register, do SIMD instructions on them and split them out again later (and I don\'t think the machine instructions will be shorter anyway).LInux assembly howto, asks this question and gives the pros and cons of using assembly.One of the more famous snippets of assembly is from Michael Abrash\'s texture mapping loop (expained in detail here):Nowadays most compilers express advanced CPU specific instructions as intrinsics, i.e., functions that get compiled down to the actual instruction. MS Visual C++ supports intrinsics for MMX, SSE, SSE2, SSE3, and SSE4, so you have to worry less about dropping down to assembly to take advantage of platform specific instructions. Visual C++ can also take advantage of the actual architecture you are targetting with the appropriate /ARCH setting. http://cr.yp.to/qhasm.html has many examples.gcc has become a widely used compiler.  Its optimizations in general are not that good.  Far better than the average programmer writing assembler, but for real performance, not that good.  There are compilers that are simply incredible in the code they produce.  So as a general answer there are going to be many places where you can go into the output of the compiler and tweak the assembler for performance, and/or simply re-write the routine from scratch. The simple answer... One who knows assembly well (aka has the reference beside him, and is taking advantage of every little processor cache and pipeline feature etc) is guaranteed to be capable of producing much faster code than any compiler.However the difference these days just doesn\'t matter in the typical application.It might be worth looking at Optimizing Immutable and Purity by Walter Bright it\'s not a profiled test but shows you one good example of a difference between handwritten and compiler generated ASM. Walter Bright writes optimising compilers so it might be worth looking at his other blog posts.Given the right programmer, Assembler programs can always be made faster than their C counterparts (at least marginally).  It would be difficult to create a C program where you couldn\'t take out at least one instruction of the Assembler.Longpoke, there is just one limitation: time. When you don\'t have the resources to optimize every single change to code and spend your time allocating registers, optimize few spills away and what not, the compiler will win every single time. You do your modification to the code, recompile and measure. Repeat if necessary.Also, you can do a lot in the high-level side. Also, inspecting the resulting assembly may give the IMPRESSION that the code is crap, but in practice it will run faster than what you think would be quicker. Example:int y = data[i];\n// do some stuff here..\ncall_function(y, ...);The compiler will read the data, push it to stack (spill) and later read from stack and pass as argument. Sounds shite? It might actually be very effective latency compensation and result in faster runtime.// optimized version\ncall_function(data[i], ...); // not so optimized after all..The idea with the optimized version was, that we have reduced register pressure and avoid spilling. But in truth, the "shitty" version was faster!Looking at the assembly code, just looking at the instructions and concluding: more instructions, slower, would be a misjudgment.The thing here to pay attention is: many assembly experts think they know a lot, but know very little. The rules change from architecture to next, too. There is no silver-bullet x86 code, for example, which is always the fastest. These days is better to go by rules-of-thumb:Also, trusting too much into compiler magically transforming poorly-thought-out C/C++ code into "theoretically optimum" code is wishful thinking. You have to know the compiler and tool chain you use if you care about "performance" at this low-level.Compilers in C/C++ are generally not very good at re-ordering sub-expressions because the functions have side effects, for starters. Functional languages don\'t suffer from this caveat but don\'t fit the current ecosystem that well. There are compiler options to allow relaxed precision rules which allow order of operations to be changed by the compiler/linker/code generator.This topic is a bit of a dead-end; for most it\'s not relevant, and the rest, they know what they are doing already anyway.It all boils down to this: "to understand what you are doing", it\'s a bit different from knowing what you are doing.How about creating machine code at run-time? My brother once (around 2000) realised an extremely fast real-time ray-tracer by generating code at run-time. I can\'t remember the details, but there was some kind of main module which was looping through objects, then it was preparing and executing some machine code which was specific to each object. However, over time, this method was outruled by new graphics hardware, and it became useless.Today, I think that possibly some operations on big-data (millions of records) like pivot tables, drilling, calculations on-the-fly, etc. could be optimized with this method. The question is: is the effort worth it? 