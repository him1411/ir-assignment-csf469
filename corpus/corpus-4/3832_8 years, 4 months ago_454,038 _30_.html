There is an online file (such as http://www.example.com/information.asp) I need to grab and save to a directory. I know there are several methods for grabbing and reading online files (URLs) line-by-line, but is there a way to just download and save the file using Java?Give a try to Java NIO:Using transferFrom() is potentially much more efficient than a simple loop that reads from the source channel and writes to this channel. Many operating systems can transfer bytes directly from the source channel into the filesystem cache without actually copying them.Check more about it here.Note: The third parameter in transferFrom is the maximum number of bytes to transfer.  Integer.MAX_VALUE will transfer at most 2^31 bytes, Long.MAX_VALUE will allow at most 2^63 bytes (larger than any file in existence). Use apache commons-io, just one line code:Simpler nio usage:You\'ll need to handle exceptions, probably external to this method.Downloading a file requires you to read it, either way you will have to go through the file in some way.  Instead of line by line, you can just read it by bytes from the stream:When using Java 7+ use the following method to download a file from the Internet and save it to some directory:Documentation here.This answer is almost exactly like selected answer but with two enhancements:  it\'s a method and it closes out the FileOutputStream object:Personally, I\'ve found Apache\'s HttpClient to be more than capable of everything I\'ve needed to do with regards to this.  Here is a great tutorial on using HttpClientThis is another java7 variant based on Brian Risk\'s answer with usage of try-with statement:There are many elegant and efficient answers here. But the conciseness can make us lose some useful information. In particular, one often does not want to consider a connection error an Exception, and one might want to treat differently some kind of network-related errors - for example, to decide if we should retry the download.Here\'s a method that does not throw Exceptions for network errors (only for truly exceptional problems, as malformed url or problems writing to the file)There is an issue with simple usage of:if you need to download and save very large files, or in general if you need automatic retries in case connection is dropped.What I suggest in such cases is Apache HttpClient along with org.apache.commons.io.FileUtils. For example:It\'s possible to download the file with with Apache\'s HttpComponents instead of Commons-IO. This code allows you to download a file in Java according to its URL and save it at the specific destination.In contrast to the single line of code:this code will give you more control over a process and let you specify not only time outs but User-Agent and Referer values, which are critical for many web-sites.To summarize (and somehow polish and update) previous answers. The three following methods are practically equivalent. (I added explicit timeouts because I think they are a must, nobody wants a download to freeze forever when the connection is lost.)I don\'t find significant differences, all seem right to me. They are safe and efficient. (Differences in speed seem hardly relevant - I write 180Mb from local server to a SSD disk in times that fluctuate around 1.2 to 1.5 segs). They don\'t require external libraries. All work with arbitrary sizes and (to my experience) HTTP redirections.Additionally, all throw FileNotFoundException if the resource is not found (error 404, typically), and java.net.UnknownHostException if the DNS resolution failed; other IOException correspond to errors during transmission.(Marked as community wiki, feel free to add info or corrections)You can do this in 1 line using netloader for Java:There is method $.fetch() in underscore-lodash library.pom.xml:Code example: