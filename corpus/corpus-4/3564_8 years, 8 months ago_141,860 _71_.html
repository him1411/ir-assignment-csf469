Sometimes I see \xce\x98(n) with the strange \xce\x98 symbol with something in the middle of it, and sometimes just O(n). Is it just laziness of typing because nobody knows how to type this symbol, or does it mean something different?If an algorithm is of \xce\x98(g(n)), it means that the running time of the algorithm as n (input size) gets larger is proportional to g(n).If an algorithm is of O(g(n)), it means that the running time of the algorithm as n gets larger is at most proportional to g(n).Normally, even when people talk about O(g(n)) they actually mean \xce\x98(g(n)) but technically, there is a difference. O(n) represents upper bound. \xce\x98(n) means tight bound.\n\xce\xa9(n) represents lower bound.f(x) = \xce\x98(g(x)) iff f(x) =\n    O(g(x)) and f(x) = \xce\xa9(g(x))For example, an upper bound for the naive recursive approach to compute Fibonacci sequence is:Fib(x) = O(2n)but the tight bound is Fib(x) = \xce\x98(Fn) where Fn is the Fibonacci sequence.which is also a valid upper bound.Basically when we say an algorithm is of O(n), it\'s also O(n2), O(n1000000), O(2n), ... but a \xce\x98(n) algorithm is not \xce\x98(n2).In fact, since f(n) = \xce\x98(g(n)) means for sufficiently large values of n, f(n) can be bound within c1g(n) and c2g(n) for some values of c1 and c2, i.e. the growth rate of f is asymptotically equal to g: g can be a lower bound and and an upper bound of f. This directly implies f can be a lower bound and an upper bound of g as well. Consequently,f(x) = \xce\x98(g(x)) iff g(x) = \xce\x98(f(x))Similarly, to show f(n) = \xce\x98(g(n)), it\'s enough to show g is an upper bound of f (i.e. f(n) = O(g(n))) and f is a lower bound of g (i.e. f(n) = \xce\xa9(g(n)) which is the exact same thing as g(n) = O(f(n))). Concisely,f(x) = \xce\x98(g(x)) iff f(x) = O(g(x)) and g(x) = O(f(x))There are also small-oh and small-omega (\xcf\x89) notations representing loose upper and loose lower bounds of a function. To summarize:f(x) = O(g(x)) (big-oh) means that\n  the growth rate of f(x) is\n  asymptotically less than or equal\n  to to the growth rate of g(x).f(x) = \xce\xa9(g(x)) (big-omega) means\n  that the growth rate of f(x) is\n  asymptotically greater than or\n  equal to the growth rate of g(x)f(x) = o(g(x)) (small-oh) means that\n  the growth rate of f(x) is\n  asymptotically less than the\n  growth rate of g(x).f(x) = \xcf\x89(g(x)) (small-omega) means\n  that the growth rate of f(x) is\n  asymptotically greater than the\n  growth rate of g(x)f(x) = \xce\x98(g(x)) (theta) means that\n  the growth rate of f(x) is\n  asymptotically equal to the\n  growth rate of g(x)For a more detailed discussion, you can read the definition on Wikipedia or consult a classic textbook like Introduction to Algorithms by Cormen et al.There\'s a simple way (a trick, I guess) to remember which notation means what.All of the Big-O notations can be considered to have a bar. When looking at a \xce\xa9, the bar is at the bottom, so it is an (asymptotic) lower bound.When looking at a \xce\x98, the bar is obviously in the middle. So it is an (asymptotic) tight bound. When handwriting O, you usually finish at the top, and draw a squiggle. Therefore O(n) is the upper bound of the function. To be fair, this one doesn\'t work with most fonts, but it is the original justification of the names.one is Big "O"one is Big Thetahttp://en.wikipedia.org/wiki/Big_O_notationBig O means your algorithm will execute in no more steps than in given expression(n^2)Big Omega means your algorithm will execute in no fewer steps than in the given expression(n^2)When both condition are true for the same expression, you can use the big theta notation....Rather than provide a theoretical definition, which are beautifully summarized here already, I\'ll give a simple example:Assume the run time of f(i) is O(1). Below is a code fragment whose asymptotic runtime is \xce\x98(n). It always calls the function f(...) n times. Both the lower and the upper bound is n.The second code fragment below has the asymptotic runtime of O(n). It calls the function f(...) at most n times. The upper bound is n, but the lower bound could be \xce\xa9(1) or \xce\xa9(log(n)), depending on what happens inside f2(i). A chart could make the previous answers easier to understand: In English,On the left, note that there is an upper bound and a lower bound that are both of the same order of magnitude (i.e. g(n) ). Ignore the constants, and if the upper bound and lower bound have the same order of magnitude, one can validly say g(n) is the Theta of f(n).Starting with the right, the simpler example, it is saying the upper bound (i.e. big O) of f(n) is g(n). Note that g(n) is simply the order of magnitude and ignores the constant c (just as all big O notation does).Think of saying Theta = afunction as a shortcut way of saying Big-O = afunction AND Omega = afunctionWhen the big O of a function and Omega of the function are the same, Theta is a shorthand way to refer to that special situation.Thus, if you say Theta = some expression, then it is still correct to say O = some expression, and Omega = some expression. The only difference is that saying Theta = some expression contains more information.Rough analogy:O says "that animal has less than or equal to 5 legs."\nOmega says "that animal has more than or equal to 5 lets."Theta is like saying "that animal has 5 legs".In other words, if an animal has 5 legs (Theta), then both the following statements are true:Just keep in mind, the expressions aren\'t necessarily specific numbers, but functions of varying orders of magnitude ( log(n), n, n^2, etc ).f(n) belongs to O(n) if exists positive k as f(n)<=k*nf(n) belongs to \xce\x98(n) if exists positive k1, k2 as k1*n<=f(n)<=k2*nWikipedia article on Big O NotationLet\'s consider f(n) > 0 and g(n) > 0 for all n. It\'s ok to consider this, because the fastest real algorithm has at least one operation and completes its execution after the start. This will simplify the calculus, because we can use the value (f(n)) instead of the absolute value (|f(n)|).General:For g(n) = n:Examples:Counterexamples:General:For g(n) = n:Examples:Counterexamples:Conclusion: we regard big O, big \xce\xb8 and big \xce\xa9 as the same thing. Why? I will tell the reason below:Firstly, I will clarify one wrong statement, some people think that we just care the worst time complexity, so we always use big O instead of big \xce\xb8. I will say this man is bullshitting. Upper and lower bound are used to describe one function, not used to describe the time complexity. The worst time function has its upper and lower bound; the best time function has its upper and lower bound too.In order to explain clearly the relation between big O and big \xce\xb8, I will explain the relation between big O and small o first. From the definition, we can easily know that small o is a subset of big O. For example\xef\xbc\x9aT(n)= n^2 + n, we can say T(n)=O(n^2), T(n)=O(n^3), T(n)=O(n^4). But for small o, T(n)=o(n^2) does not meet the definition of small o. So just T(n)=o(n^3), T(n)=o(n^4) are correct for small o. The redundant T(n)=O(n^2) is what? It\'s big \xce\xb8!Generally, we say big O is O(n^2), hardly to say T(n)=O(n^3), T(n)=O(n^4). Why? Because we regard big O as big \xce\xb8 subconsciously.Similarly, we also regard big \xce\xa9 as big \xce\xb8 subconsciously.In one word, big O, big \xce\xb8 and big \xce\xa9 are not the same thing from the definitions, but they are the same thing in our mouth and brain.