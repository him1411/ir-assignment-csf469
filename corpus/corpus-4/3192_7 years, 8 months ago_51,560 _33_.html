Does anyone know what is the worst possible asymptotic slowdown that can happen when programming purely functionally as opposed to imperatively (i.e. allowing side-effects)?Clarification from comment by itowlson: is there any problem for which the best known non-destructive algorithm is asymptotically worse than the best known destructive algorithm, and if so by how much?According to Pippenger [1996], when comparing a Lisp system that is purely functional (and has strict evaluation semantics, not lazy) to one that can mutate data, an algorithm written for the impure Lisp that runs in O(n) can be translated to an algorithm in the pure Lisp that runs in O(n log n) time (based on work by Ben-Amram and Galil [1992] about simulating random access memory using only pointers). Pippenger also establishes that there are algorithms for which that is the best you can do; there are problems which are O(n) in the impure system which are \xce\xa9(n log n) in the pure system.There are a few caveats to be made about this paper. The most significant is that it does not address lazy functional languages, such as Haskell. Bird, Jones and De Moor [1997] demonstrate that the problem constructed by Pippenger can be solved in a lazy functional language in O(n) time, but they do not establish (and as far as I know, no one has) whether or not a lazy functional language can solve all problems in the same asymptotic running time as a language with mutation.The problem constructed by Pippenger requires \xce\xa9(n log n) is specifically constructed to achieve this result, and is not necessarily representative of practical, real-world problems. There are a few restrictions on the problem that are a bit unexpected, but necessary for the proof to work; in particular, the problem requires that results are computed on-line, without being able to access future input, and that the input consists of a sequence of atoms from an unbounded set of possible atoms, rather than a fixed size set. And the paper only establishes (lower bound) results for an impure algorithm of linear running time; for problems that require a greater running time, it is possible that the extra O(log n) factor seen in the linear problem may be able to be "absorbed" in the process of extra operations necessary for algorithms with greater running times. These clarifications and open questions are explored briefly by Ben-Amram [1996].In practice, many algorithms can be implemented in a pure functional language at the same efficiency as in a language with mutable data structures. For a good reference on techniques to use for implementing purely functional data structures efficiently, see Chris Okasaki\'s "Purely Functional Data Structures" [Okasaki 1998] (which is an expanded version of his thesis [Okasaki 1996]).Anyone who needs to implement algorithms on purely-functional data structures should read Okasaki. You can always get at worst an O(log n) slowdown per operation by simulating mutable memory with a balanced binary tree, but in many cases you can do considerably better than that, and Okasaki describes many useful techniques, from amortized techniques to real-time ones that do the amortized work incrementally. Purely functional data structures can be a bit difficult to work with and analyze, but they provide many benefits like referential transparency that are helpful in compiler optimization, in parallel and distributed computing, and in implementation of features like versioning, undo, and rollback.Note also that all of this discusses only asymptotic running times. Many techniques for implementing purely functional data structures give you a certain amount of constant factor slowdown, due to extra bookkeeping necessary for them to work, and implementation details of the language in question. The benefits of purely functional data structures may outweigh these constant factor slowdowns, so you will generally need to make trade-offs based on the problem in question.There are indeed several algorithms and data structures for which no asymptotically efficient purely functional solution (t.i. one implementable in pure lambda calculus) is known, even with laziness.However, we assume that in "imperative" languages access to memory is O(1) whereas in theory that can\'t be so asymptotically (i.e. for unbounded problem sizes) and access to memory within a huge dataset is always O(log n), which can be emulated in a functional language.Also, we must remember that actually all modern functional languages provide mutable data, and Haskell even provides it without sacrificing purity (the ST monad).This article claims that the known purely functional implementations of the union-find algorithm all have worse asymptotic complexity than the one they publish, which has a purely functional interface but uses mutable data internally.The fact that other answers claim that there can never be any difference and that for instance, the only "drawback" of purely functional code is that it can be parallelized gives you an idea of the informedness/objectivity of the functional programming community on these matters.EDIT:Comments below point out that a biased discussion of the pros and cons of pure functional programming may not come from the \xe2\x80\x9cfunctional programming community\xe2\x80\x9d. Good point. Perhaps the advocates I see are just, to cite a comment, \xe2\x80\x9cilliterate\xe2\x80\x9d.For instance, I think that this blog post is written by someone who could be said to be representative of the functional programming community, and since it\'s a list of \xe2\x80\x9cpoints for lazy evaluation\xe2\x80\x9d, it would be a good place to mention any drawback that lazy and purely functional programming might have. A good place would have been in place of the following (technically true, but biased to the point of not being funny) dismissal:If strict a function has O(f(n)) complexity in a strict language then it has complexity O(f(n)) in a lazy language as well.  Why worry? :)With a fixed upper bound on memory usage, there should be no difference.Proof sketch:\nGiven a fixed upper bound on memory usage, one should be able to write a virtual machine which executes an imperative instruction set with the same asymptotic complexity as if you were actually executing on that machine.  This is so because you can manage the mutable memory as a persistent data structure, giving O(log(n)) read and writes, but with a fixed upper bound on memory usage, you can have a fixed amount of memory, causing these to decay to O(1). Thus the functional implementation can be the imperative version running in the functional implementation of the VM, and so they should both have the same asymptotic complexity.I\'d suggest reading on performance of Haskell, and then taking a look at Alioth Language Shootout performances for functional languages vs. procedural/OO ones."Functional" is a bunch of different features, each of which is independently useful, and its found more useful to look at each individually.ImmutabilityNow that I\'m familiar with it, any time I can get away with returning an immutable result, I always try to do that, even in an object oriented program. It\'s easier to reason about the program if you have value-type data.Functions as First Class TypesWhatever you want to call it, passing around delegates, actions, or functions, is a really handy way to solve a whole class of real world problems, like the "hole in the middle pattern".Being able to compose functions (for instance turning Action into just an Action is also quite useful in some scenarios.We should also note Lambda syntax here, because you only get Lambda syntax when you promote functions to first class types. Lambda syntax can be very expressive and concise.MonadsThis is a subtle but very powerful construct. It\'s as powerful as the yield keyword used to create IEnumerable classes in C#. Essentially it\'s building a state machine for you under the covers, but your logic looks linear.Lazy Evaluation & RecursionI put these together because while they\'re always lumped in as features of functional programming, they\'ve made their way so quickly into otherwise-imperative languages that it\'s hard to call them functional anymore.S-ExpressionsI guess I\'m not sure where to put this, but the ability to treat the un-compiled code as an object (and inspect/modify it), such as Lisp S-Expressions, or LINQ Expressions, is, in some ways, the most powerful tool of functional programming. Most new .NET "fluent" interfaces, and DSLs, use a combination of lambda syntax and LINQ Expressions to create some very concise APIs. Not to mention Linq2Sql/Linq2Nhibernate where your C# code is "magically" executed as SQL instead of as C# code.