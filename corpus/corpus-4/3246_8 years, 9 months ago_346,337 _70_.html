How to wait in a bash script for several subprocesses spawned from that script to finish and return exit code !=0 when any of the subprocesses ends with code !=0 ?Simple script:The above script will wait for all 10 spawned subprocesses, but it will always give exit status 0 (see help wait). How can I modify this script so it will discover exit statuses of spawned subprocesses and return exit code 1 when any of subprocesses ends with code !=0?Is there any better solution for that than collecting PIDs of the subprocesses, wait for them in order and sum exit statuses?wait also (optionally) takes the PID of the process to wait for, and with $! you get the PID of the last command launched in background.\nModify the loop to store the PID of each spawned sub-process into an array, and then loop again waiting on each PID.http://jeremy.zawodny.com/blog/archives/010717.html :If you have GNU Parallel installed you can do:GNU Parallel will give you exit code:0 - All jobs ran without error.1-253 - Some of the jobs failed. The exit status gives the number of failed jobs254 - More than 253 jobs failed.255 - Other error.Watch the intro videos to learn more: http://pi.dk/110 seconds installation:Here\'s what I\'ve come up with so far.  I would like to see how to interrupt the sleep command if a child terminates, so that one would not have to tune WAITALL_DELAY to one\'s usage.How about simply:Update:As pointed by multiple commenters, the above waits for all processes to be completed before continuing, but does not exit and fail if one of them fails, it can be made to do with the following modification suggested by @Bryan, @SamBrightman, and others: Here is simple example using wait.Run some processes:Then wait for them with wait command:Or just wait (without arguments) for all.This will wait for all jobs in the background are completed.If the -n option is supplied, waits for the next job to terminate and returns its exit status.See: help wait and help jobs for syntax.However the downside is that this will return on only the status of the last ID, so you need to check the status for each subprocess and store it in the variable.Or make your calculation function to create some file on failure (empty or with fail log), then check of that file if exists, e.g.To parallelize this...Translate it to this...Here\'s a simplified working example...I see lots of good examples listed on here, wanted to throw mine in as well.I use something very similar to start/stop servers/services in parallel and check each exit status. Works great for me. Hope this helps someone out!I don\'t believe it\'s possible with Bash\'s builtin functionality.You can get notification when a child exits:However there\'s no apparent way to get the child\'s exit status in the signal handler.Getting that child status is usually the job of the wait family of functions in the lower level POSIX APIs.  Unfortunately Bash\'s support for that is limited - you can wait for one specific child process (and get its exit status) or you can wait for all of them, and always get a 0 result.What it appears impossible to do is the equivalent of waitpid(-1), which blocks until any child process returns. The following code will wait for completion of all calculations and return exit status 1 if any of doCalculations fails.If you have bash 4.2 or later available the following might be useful to you. It uses associative arrays to store task names and their "code" as well as task names and their pids. I have also built in a simple rate-limiting method which might come handy if your tasks consume a lot of CPU or I/O time and you want to limit the number of concurrent tasks.The script launches all tasks in the first loop and consumes the results in the second one.This is a bit overkill for simple cases but it allows for pretty neat stuff. For example one can store error messages for each task in another associative array and print them after everything has settled down.Here\'s my version that works for multiple pids, logs warnings if execution takes too long, and stops the subprocesses if execution takes longer than a given value.Example, wait for all three processes to finish, log a warning if execution takes loger than 5 seconds, stop all processes if execution takes longer than 120 seconds. Don\'t exit program on failures.Just store the results out of the shell, e.g. in a file.I\'ve had a go at this and combined all the best parts from the other examples here. This script will execute the checkpids function when any background process exits, and output the exit status without resorting to polling. I\'ve just been modifying a script to background and parallelise a process.I did some experimenting (on Solaris with both bash and ksh) and discovered that \'wait\' outputs the exit status if it\'s not zero , or a list of jobs that return non-zero exit when no PID argument is provided. E.g.Bash:Ksh:This output is written to stderr, so a simple solution to the OPs example could be:While this:will also return a count but without the tmp file. This might also be used this way, for example:But this isn\'t very much more useful than the tmp file IMO. I couldn\'t find a useful way to avoid the tmp file whilst also avoiding running the "wait" in a subshell, which wont work at all.unfortunately this won\'t handle the case when a process in the background exits with a non-zero exit status.  (the loop won\'t terminate immediately.  it will wait for the previous processes to complete.)trap is your friend. You can trap on ERR in a lot of systems. You can trap EXIT, or on DEBUG to perform a piece of code after every command.This in addition to all the standard signals.There are already a lot of answers here, but I am surprised no one seems to have suggested using arrays... So here\'s what I did - this might be useful to some in the future.This works, should be just as a good if not better than @HoverHell\'s answer!I used this recently (thanks to Alnitak):From there one can easily extrapolate, and have a trigger (touch a file, send a signal) and change the counting criteria (count files touched, or whatever) to respond to that trigger. Or if you just want \'any\' non zero rc, just kill the lock from save_status.I needed this, but the target process wasn\'t a child of current shell, in which case wait $PID doesn\'t work. I did find the following alternative instead:That relies on the presence of procfs, which may not be available (Mac doesn\'t provide it for example). So for portability, you could use this instead:Trapping CHLD signal may not work because you can lose some signals if they arrived simultaneously.The set -e at top makes your script stop on failure.expect will return 1 if any subjob failed.I\'m thinking maybe run doCalculations; echo "$?" >>/tmp/acc in a subshell that is sent to the background, then the wait, then /tmp/acc would contain the exit statuses, one per line.  I don\'t know about any consequences of the multiple processes appending to the accumulator file, though.Here\'s a trial of this suggestion:File: doCalcualtionsFile: tryOutput of running ./try