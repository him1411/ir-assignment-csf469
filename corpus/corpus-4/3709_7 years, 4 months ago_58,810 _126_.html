Answering to another Stack Overflow question (this one) I stumbled upon an interesting sub-problem. What is the fastest way to sort an array of 6 ints?As the question is very low level:Really this question is a kind of Golf where the goal is not to minimize source length but execution time. I call it \'Zening` code as used in the title of the book Zen of Code optimization by Michael Abrash and its sequels.As for why it is interesting, there is several layers:Here is my reference (naive, not optimized) implementation and my test set.As number of variants is becoming large, I gathered them all in a test suite that can be found here. The actual tests used are a bit less naive than those showed above, thanks to Kevin Stock. You can compile and execute it in your own environment. I\'m quite interested by behavior on different target architecture/compilers. (OK guys, put it in answers, I will +1 every contributor of a new resultset). I gave the answer to Daniel Stutzbach (for golfing) one year ago as he was at the source of the fastest solution at that time (sorting networks).Linux 64 bits, gcc 4.6.1 64 bits, Intel Core 2 Duo E8400, -O2Linux 64 bits, gcc 4.6.1 64 bits, Intel Core 2 Duo E8400, -O1I included both -O1 and -O2 results because surprisingly for several programs O2 is less efficient than O1. I wonder what specific optimization has this effect ?Insertion Sort (Daniel Stutzbach)As expected minimizing branches is indeed a good idea.Sorting Networks (Daniel Stutzbach)Better than insertion sort. I wondered if the main effect was not get from avoiding the external loop. I gave it a try by unrolled insertion sort to check and indeed we get roughly the same figures (code is here).Sorting Networks (Paul R)The best so far. The actual code I used to test is here. Don\'t know yet why it is nearly two times as fast as the other sorting network implementation. Parameter passing ? Fast max ?Sorting Networks 12 SWAP with Fast SwapAs suggested by Daniel Stutzbach, I combined his 12 swap sorting network with branchless fast swap (code is here). It is indeed faster, the best so far with a small margin (roughly 5%) as could be expected using 1 less swap. It is also interesting to notice that the branchless swap seems to be much (4 times) less efficient than the simple one using if on PPC architecture.Calling Library qsortTo give another reference point I also tried as suggested to just call library qsort (code is here). As expected it is much slower : 10 to 30 times slower...  as it became obvious with the new test suite, the main problem seems to be the initial load of the library after the first call, and it compares not so poorly with other version. It is just between 3 and 20 times slower on my Linux. On some architecture used for tests by others it seems even to be faster (I\'m really surprised by that one, as library qsort use a more complex API).Rank orderRex Kerr proposed another completely different method : for each item of the array compute directly its final position. This is efficient because computing rank order do not need branch. The drawback of this method is that it takes three times the amount of memory of the array (one copy of array and variables to store rank orders). The performance results are very surprising (and interesting). On my reference architecture with 32 bits OS and Intel Core2 Quad E8300, cycle count was slightly below 1000 (like sorting networks with branching swap). But when compiled and executed on my 64 bits box (Intel Core2 Duo) it performed much better : it became the fastest so far. I finally found out the true reason. My 32bits box use gcc 4.4.1 and my 64bits box gcc 4.4.3 and the last one seems much better at optimising this particular code (there was very little difference for other proposals).update:As published figures above shows this effect was still enhanced by later versions of gcc and Rank Order became consistently twice as fast as any other alternative.Sorting Networks 12 with reordered SwapThe amazing efficiency of the Rex Kerr proposal with gcc 4.4.3 made me wonder : how could a program with 3 times as much memory usage be faster than branchless sorting networks? My hypothesis was that it had less dependencies of the kind read after write, allowing for better use of the superscalar instruction scheduler of the x86. That gave me an idea: reorder swaps to minimize read after write dependencies. More simply put: when you do SWAP(1, 2); SWAP(0, 2); you have to wait for the first swap to be finished before performing the second one because both access to a common memory cell. When you do SWAP(1, 2); SWAP(4, 5);the processor can execute both in parallel. I tried it and it works as expected, the sorting networks is running about 10% faster. Sorting Networks 12 with Simple SwapOne year after the original post Steinar H. Gunderson suggested, that we should not try to outsmart the compiler and keep the swap code simple. It\'s indeed a good idea as the resulting code is about 40% faster! He also proposed a swap optimized by hand using x86 inline assembly code that can still spare some more cycles. The most surprising (it says volumes on programmer\'s psychology) is that one year ago none of used tried that version of swap. Code I used to test is here. Others suggested other ways to write a C fast swap, but it yields the same performances as the simple one with a decent compiler.The "best" code is now as follow:If we believe our test set (and, yes it is quite poor, it\'s mere benefit is being short, simple and easy to understand what we are measuring), the average number of cycles of the resulting code for one sort is below 40 cycles (6 tests are executed). That put each swap at an average of 4 cycles. I call that amazingly fast. Any other improvements possible ?For any optimization, it\'s always best to test, test, test.  I would try at least sorting networks and insertion sort.  If I were betting, I\'d put my money on insertion sort based on past experience.Do you know anything about the input data?  Some algorithms will perform better with certain kinds of data.  For example, insertion sort performs better on sorted or almost-sorted dat, so it will be the better choice if there\'s an above-average chance of almost-sorted data.The algorithm you posted is similar to an insertion sort, but it looks like you\'ve minimized the number of swaps at the cost of more comparisons.  Comparisons are far more expensive than swaps, though, because branches can cause the instruction pipeline to stall.Here\'s an insertion sort implementation:Here\'s how I\'d build a sorting network.  First, use this site to generate a minimal set of SWAP macros for a network of the appropriate length.  Wrapping that up in a function gives me:Here\'s an implementation using sorting networks:You really need very efficient branchless min and max implementations for this, since that is effectively what this code boils down to - a sequence of min and max operations (13 of each, in total). I leave this as an exercise for the reader.Note that this implementation lends itself easily to vectorization (e.g. SIMD - most SIMD ISAs have vector min/max instructions) and also to GPU implementations (e.g. CUDA - being branchless there are no problems with warp divergence etc).See also: Fast algorithm implementation to sort very small listSince these are integers and compares are fast, why not compute the rank order of each directly:Looks like I got to the party a year late, but here we go...Looking at the assembly generated by gcc 4.5.2 I observed that loads and stores are being done for every swap, which really isn\'t needed. It would be better to load the 6 values into registers, sort those, and store them back into memory. I ordered the loads at stores to be as close as possible to there the registers are first needed and last used. I also used Steinar H. Gunderson\'s SWAP macro. Update: I switched to Paolo Bonzini\'s SWAP macro which gcc converts into something similar to Gunderson\'s, but gcc is able to better order the instructions since they aren\'t given as explicit assembly.I used the same swap order as the reordered swap network given as the best performing, although there may be a better ordering. If I find some more time I\'ll generate and test a bunch of permutations.I changed the testing code to consider over 4000 arrays and show the average number of cycles needed to sort each one. On an i5-650 I\'m getting ~34.1 cycles/sort (using -O3), compared to the original reordered sorting network getting ~65.3 cycles/sort (using -O1, beats -O2 and -O3).I changed modified the test suite to also report clocks per sort and run more tests (the cmp function was updated to handle integer overflow as well), here are the results on some different architectures. I attempted testing on an AMD cpu but rdtsc isn\'t reliable on the X6 1100T I have available.I stumbled onto this question from Google a few days ago because I also had a need to quickly sort a fixed length array of 6 integers. In my case however, my integers are only 8 bits (instead of 32) and I do not have a strict requirement of only using C. I thought I would share my findings anyways, in case they might be helpful to someone...I implemented a variant of a network sort in assembly that uses SSE to vectorize the compare and swap operations, to the extent possible. It takes six "passes" to completely sort the array. I used a novel mechanism to directly convert the results of PCMPGTB (vectorized compare) to shuffle parameters for PSHUFB (vectorized swap), using only a PADDB (vectorized add) and in some cases also a PAND (bitwise AND) instruction.This approach also had the side effect of yielding a truly branchless function. There are no jump instructions whatsoever.It appears that this implementation is about 38% faster than the implementation which is currently marked as the fastest option in the question ("Sorting Networks 12 with Simple Swap"). I modified that implementation to use char array elements during my testing, to make the comparison fair.I should note that this approach can be applied to any array size up to 16 elements. I expect the relative speed advantage over the alternatives to grow larger for the bigger arrays.The code is written in MASM for x86_64 processors with SSSE3. The function uses the "new" Windows x64 calling convention. Here it is...You can compile this to an executable object and link it into your C project. For instructions on how to do this in Visual Studio, you can read this article. You can use the following C prototype to call the function from your C code:The test code is pretty bad; it overflows the initial array (don\'t people here read compiler warnings?), the printf is printing out the wrong elements, it uses .byte for rdtsc for no good reason, there\'s only one run (!), there\'s nothing checking that the end results are actually correct (so it\'s very easy to \xe2\x80\x9coptimize\xe2\x80\x9d into something subtly wrong), the included tests are very rudimentary (no negative numbers?) and there\'s nothing to stop the compiler from just discarding the entire function as dead code.That being said, it\'s also pretty easy to improve on the bitonic network solution; simply change the min/max/SWAP stuff toand it comes out about 65% faster for me (Debian gcc 4.4.5 with -O2, amd64, Core i7).While I really like the swap macro provided:I see an improvement (which a good compiler might make):We take note of how min and max work and pull the common sub-expression explicitly. This eliminates the min and max macros completely.I ported the test suite to a PPC architecture machine I can not identify (didn\'t have to touch code, just increase the iterations of the test, use 8 test cases to avoid polluting results with mods and replace the x86 specific rdtsc):Direct call to qsort library function : 101Naive implementation (insertion sort) : 299Insertion Sort (Daniel Stutzbach)     : 108Insertion Sort Unrolled               : 51Sorting Networks (Daniel Stutzbach)   : 26Sorting Networks (Paul R)             : 85Sorting Networks 12 with Fast Swap    : 117Sorting Networks 12 reordered Swap    : 116Rank Order                            : 56Never optimize min/max without benchmarking and looking at actual compiler generated assembly.  If I let GCC optimize min with conditional move instructions I get a 33% speedup:(280 vs. 420 cycles in the test code).  Doing max with ?: is more or less the same, almost lost in the noise, but the above is a little bit faster.  This SWAP is faster with both GCC and Clang.Compilers are also doing an exceptional job at register allocation and alias analysis, effectively moving d[x] into local variables upfront, and only copying back to memory at the end.  In fact, they do so even better than if you worked entirely with local variables (like d0 = d[0], d1 = d[1], d2 = d[2], d3 = d[3], d4 = d[4], d5 = d[5]).  I\'m writing this because you are assuming strong optimization and yet trying to outsmart the compiler on min/max. :)By the way, I tried Clang and GCC. They do the same optimization, but due to scheduling differences the two have some variation in the results, can\'t say really which is faster or slower. GCC is faster on the sorting networks, Clang on the quadratic sorts.Just for completeness, unrolled bubble sort and insertion sorts are possible too.  Here is the bubble sort:and here is the insertion sort:This insertion sort is faster than Daniel Stutzbach\'s, and is especially good on a GPU or a computer with predication because ITER can be done with only 3 instructions (vs. 4 for SWAP).  For example, here is the t = d[2]; ITER(1); ITER(0); line in ARM assembly:For six elements the insertion sort is competitive with the sorting network (12 swaps vs. 15 iterations balances 4 instructions/swap vs. 3 instructions/iteration); bubble sort of course is slower.  But it\'s not going to be true when the size grows, since insertion sort is O(n^2) while sorting networks are O(n log n).An XOR swap may be useful in your swapping functions.The if may cause too much divergence in your code, but if you have a guarantee that all your ints are unique this could be handy.Looking forward to trying my hand at this and learning from these examples, but first some timings from my 1.5 GHz PPC Powerbook G4 w/ 1 GB DDR RAM. (I borrowed a similar rdtsc-like timer for PPC from http://www.mcs.anl.gov/~kazutomo/rdtsc.html for the timings.) I ran the program a few times and the absolute results varied but the consistently fastest test was "Insertion Sort (Daniel Stutzbach)", with "Insertion Sort Unrolled" a close second. Here\'s the last set of times:Here is my contribution to this thread: an optimized 1, 4 gap shellsort for a 6-member int vector (valp) containing unique values.On my HP dv7-3010so laptop with a dual-core Athlon M300 @ 2 Ghz (DDR2 memory) it executes in 165 clock cycles. This is an average calculated from timing every unique sequence (6!/720 in all). Compiled to Win32 using OpenWatcom 1.8. The loop is essentially an insertion sort and is 16 instructions/37 bytes long.I do not have a 64-bit environment to compile on.If insertion sort is reasonably competitive here, I would recommend trying a shellsort. I\'m afraid 6 elements is probably just too little for it to be among the best, but it might be worth a try. Example code, untested, undebugged, etc. You want to tune the inc = 4 and inc -= 3 sequence to find the optimum (try inc = 2, inc -= 1 for example). I don\'t think this will win, but if someone posts a question about sorting 10 elements, who knows...According to Wikipedia this can even be combined with sorting networks:\nPratt, V (1979). Shellsort and sorting networks (Outstanding dissertations in the computer sciences). Garland. ISBN 0-824-04406-1This question is becoming quite old, but I actually had to solve the same problem these days: fast agorithms to sort small arrays. I thought it would be a good idea to share my knowledge. While I first started by using sorting networks, I finally managed to find other algorithms for which the total number of comparisons performed to sort every permutation of 6 values was smaller than with sorting networks, and smaller than with insertion sort. I didn\'t count the number of swaps; I would expect it to be roughly equivalent (maybe a bit higher sometimes).The algorithm sort6 uses the algorithm sort4 which uses the algorithm sort3. Here is the implementation in some light C++ form (the original is template-heavy so that it can work with any random-access iterator and any suitable comparison function).The following algorithm is an unrolled insertion sort. When two swaps (6 assignments) have to be performed, it uses 4 assignments instead:It looks a bit complex because the sort has more or less one branch for every possible permutation of the array, using 2~3 comparisons and at most 4 assignments to sort the three values.This one calls sort3 then performs an unrolled insertion sort with the last element of the array:This algorithm performs 3 to 6 comparisons and at most 5 swaps. It is easy to unroll an insertion sort, but we will be using another algorithm for the last sort...This one uses an unrolled version of what I called a double insertion sort. The name isn\'t that great, but it\'s quite descriptive, here is how it works:After the swap, the first element is always smaller than the last, which means that, when inserting them into the sorted sequence, there won\'t be more than N comparisons to insert the two elements in the worst case: for example, if the first element has been insert in the 3rd position, then the last one can\'t be inserted lower than the 4th position.My tests on every permutation of 6 values ever show that this algorithms always performs between 6 and 13 comparisons. I didn\'t compute the number of swaps performed, but I don\'t expect it to be higher than 11 in the worst case.I hope that this helps, even if this question may not represent an actual problem anymore :)EDIT: after putting it in the provided benchmark, it is cleary slower than most of the interesting alternatives. It tends to perform a bit better than the unrolled insertion sort, but that\'s pretty much it. Basically, it isn\'t the best sort for integers but could be interesting for types with an expensive comparison operation.I know I\'m super-late, but I was interested in experimenting with some different solutions. First, I cleaned up that paste, made it compile, and put it into a repository. I kept some undesirable solutions as dead-ends so that others wouldn\'t try it. Among this was my first solution, which attempted to ensure that x1>x2 was calculated once. After optimization, it is no faster than the other, simple versions.I added a looping version of rank order sort, since my own application of this study is for sorting 2-8 items, so since there are a variable number of arguments, a loop is necessary. This is also why I ignored the sorting network solutions.The test code didn\'t test that duplicates were handled correctly, so while the existing solutions were all correct, I added a special case to the test code to ensure that duplicates were handled correctly.Then, I wrote an insertion sort that is entirely in AVX registers. On my machine it is 25% faster than the other insertion sorts, but 100% slower than rank order. I did this purely for experiment and had no expectation of this being better due to the branching in insertion sort.Then, I wrote a rank order sort using AVX. This matches the speed of the other rank-order solutions, but is no faster. The issue here is that I can only calculate the indices with AVX, and then I have to make a table of indices. This is because the calculation is destination-based rather than source-based. See Converting from Source-based Indices to Destination-based IndicesThe repo can be found here: https://github.com/eyepatchParrot/sort6/I believe there are two parts to your question.I wouldn\'t worry too much about emptying pipelines (assuming current x86): branch prediction has come a long way. What I would worry about is making sure that the code and data fit in one cache line each (maybe two for the code). Once there fetch latencies are refreshingly low which will compensate for any stall. It also means that your inner loop will be maybe ten instructions or so which is right where it should be (there are two different inner loops in my sorting algorithm, they are 10 instructions/22 bytes and 9/22 long respectively). Assuming the code doesn\'t contain any divs you can be sure it will be blindingly fast.I found that at least on my system, the functions sort6_iterator() and sort6_iterator_local() defined below both ran at least as fast, and frequently noticeably faster, than the above current record holder:I passed this function a std::vector\'s iterator in my timing code. I suspect that using iterators gives g++ certain assurances about what can and can\'t happen to the memory that the iterator refers to, which it otherwise wouldn\'t have and it is these assurances that allow g++ to better optimize the sorting code (which if I remember correctly, is also part of the reason why so many std algorithms, such as std::sort(), generally have such obscenely good performance). However, while timing I noticed that the context (i.e. surrounding code) in which the call to the sorting function was made had a significant impact on performance, which is likely due to the function being inlined and then optimized. For instance, if the program was sufficiently simple then there usually wasn\'t much of a difference in performance between passing the sorting function a pointer versus passing it an iterator; otherwise using iterators usually resulted in noticeably better performance and never (in my experience so far at least) any noticeably worse performance. I suspect that this may be because g++ can globally optimize sufficiently simple code. Moreover, sort6_iterator() is sometimes (again, depending on the context in which the function is called) consistently outperformed by the following sorting function:Note that defining SWAP() as follows sometimes results in slightly better performance although most of the time it results in slightly worse performance or a negligible difference in performance.If you just want a sorting algorithm that gcc -O3 is consistently good at optimizing then, depending on how you pass the input, try one of the following two algorithms:OrThe reason for using the register keyword is because this is one of the few times that you know that you want these values in registers. Without register, the compiler will figure this out most of the time but sometimes it doesn\'t. Using the register keyword helps solve this issue. Normally, however, don\'t use the register keyword since it\'s more likely to slow your code than speed it up.Also, note the use of templates. This is done on purpose since, even with  the inline keyword, template functions are generally much more aggressively optimized by gcc than vanilla C functions (this has to do with gcc needing to deal with function pointers for vanilla C functions but not with template functions).Well, if it\'s only 6 elements and you can leverage parallelism, want to minimize conditional branching, etc. Why you don\'t generate all the combinations and test for order? I would venture that in some architectures, it can be pretty fast (as long as you have the memory preallocated)Here are three typical sorting methods that represent three different classes of Sorting Algorithms:But check out Stefan Nelsson discussion on the fastest sorting algorithm? where he discuss a solution that goes down to O(n log log n) .. check out its implementation in CThis Semi-Linear Sorting algorithm was presented by a paper in 1995:A. Andersson, T. Hagerup, S. Nilsson, and R. Raman. Sorting in linear\ntime? In Proceedings of the 27th Annual ACM Symposium on the Theory of\nComputing, pages 427-436, 1995.Try \'merging sorted list\' sort. :) Use two array. Fastest for small and big array.\nIf you concating, you only check where insert. Other bigger values you not need compare (cmp = a-b>0).\nFor 4 numbers, you can use system 4-5 cmp (~4.6) or 3-6 cmp (~4.9). Bubble sort use 6 cmp (6). Lots of cmp for big numbers slower code.\nThis code use 5 cmp (not MSL sort):\n\n    if (cmp(arr[n][i+0],arr[n][i+1])>0) {swap(n,i+0,i+1);}\n    if (cmp(arr[n][i+2],arr[n][i+3])>0) {swap(n,i+2,i+3);}\n    if (cmp(arr[n][i+0],arr[n][i+2])>0) {swap(n,i+0,i+2);}\n    if (cmp(arr[n][i+1],arr[n][i+3])>0) {swap(n,i+1,i+3);}\n    if (cmp(arr[n][i+1],arr[n][i+2])>0) {swap(n,i+1,i+2);}Principial MSL\n    9 8 7 6 5 4 3 2 1 0\n    89 67 45 23 01 ... concat two sorted lists, list length = 1\n    6789 2345 01 ... concat two sorted lists, list length = 2\n    23456789 01 ... concat two sorted lists, list length = 4\n    0123456789 ... concat two sorted lists, list length = 8\njs codeSort 4 items with usage cmp==0. \nNumbers of cmp is ~4.34 (FF native have ~4.52), but take 3x time than merging lists. But better less cmp operations, if you have big numbers or big text.\nEdit: repaired bugOnline test http://mlich.zam.slu.cz/js-sort/x-sort-x2.htm