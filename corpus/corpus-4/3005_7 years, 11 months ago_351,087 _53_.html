Which characters make a URL invalid?Are these valid URLs?In general URIs as defined by RFC 3986 (see Section 2: Characters) may contain any of the following characters: Any other character needs to be encoded with the percent-encoding (%hh). Each part of the URI has further restrictions about what characters need to be represented by an percent-encoded word.To add some clarification and directly address the question above, there are several classes of characters that cause problems for URLs and URIs.There are some characters that are disallowed and should never appear in a URL/URI, reserved characters (described below), and other characters that may cause problems in some cases, but are marked as "unwise" or "unsafe". Explanations for why the characters are restricted are clearly spelled out in RFC-1738 (URLs) and RFC-2396 (URIs). Note these explicit details are present but obscured in the newer RFC-3986 (update to RFC-1738).Excluded US-ASCII Characters disallowed within the URI syntax:List of unwise characters are allowed but may cause problems:The following characters are reserved within a query component and have special meaning within a URI/URL:The "reserved" syntax class above refers to those characters that are allowed within a URI, but which may not be allowed within a particular component of the generic URI syntax. Characters in the "reserved" set are not reserved in all contexts. The hostname, for example, can contain an optional username so it could be something like ftp://user@hostname/ where the \'@\' character has special meaning.Here is an example of a URL that has invalid and unwise characters (e.g. \'$\', \'[\', \']\') and should be properly encoded:Some of the character restrictions for URIs/URLs are programming language dependent. For example, the \'|\' (0x7C) character although only marked as "unwise" in the URI spec will throw a URISyntaxException in the Java java.net.URI constructor so a URL like http://api.google.com/q?exp=a|b is not allowed and must be encoded instead as http://api.google.com/q?exp=a%7Cb if using Java with a URI object instance.Most of the existing answers here are impractical because they totally ignore the real-world usage of addresses like:Okay, so according to RFC 3986, such addresses are not URIs (and therefore not URLs, since URLs are a type of URIs). If we consider ourselves beholden to the terminology of existing IETF standards, then we should properly call them IRIs (Internationalized Resource Identifiers), as defined in RFC 3987, which are technically not URIs but can be converted to URIs simply by percent-encoding all non-ASCII characters in the IRI. Normal people, though, have never heard of IRIs and simply call these URIs or URLs (and indeed there\'s a WHATWG effort underway to create a new, broader URL spec that simply classifies all "URIs" and "IRIs" as "URLs" to align with modern usage of those terms in the real world).Suppose we want to adopt this meaning of URL immediately (which puts as at odds with IETF spec, but aligns us with everyday usage). In that case, what characters are valid in a URL?First of all, we have two types of RFC 3986 reserved characters:Any of the reserved characters above can be legally used in a URI without encoding, either to serve their syntactic purpose or just as literal characters in data in some places where such use could not be misinterpreted as the character serving its syntactic purpose. (For example, although / has syntactic meaning in a URL, you can use it unencoded in a query string, because it doesn\'t have meaning in a query string.)RFC 3986 also specifies some unreserved characters, which can always be used simply to represent data without any encoding:Finally, the % character itself is allowed for percent-encodings.That leaves only the following ASCII characters that are forbidden from appearing in a URL:Every other character from ASCII can legally feature in a URL.Then RFC 3987 extends that set of unreserved characters with the following unicode character ranges:But those block choices seem bizarre and arbitrary given the latest Unicode block definitions; this is probably because the blocks have been added to in the decade since RFC 3987 was written. The WhatWG\'s in-progress spec has a more generous list:U+00A0 to U+D7FF, U+E000 to U+FDCF, U+FDF0 to U+FFFD, U+10000 to U+1FFFD, U+20000 to U+2FFFD, U+30000 to U+3FFFD, U+40000 to U+4FFFD, U+50000 to U+5FFFD, U+60000 to U+6FFFD, U+70000 to U+7FFFD, U+80000 to U+8FFFD, U+90000 to U+9FFFD, U+A0000 to U+AFFFD, U+B0000 to U+BFFFD, U+C0000 to U+CFFFD, U+D0000 to U+DFFFD, U+E0000 to U+EFFFD, U+F0000 to U+FFFFD, U+100000 to U+10FFFDOf course, it should be noted that simply knowing which characters can legally appear in a URL isn\'t sufficient to recognise whether some given string is a legal URL or not, since some characters are only legal in particular parts of the URL. For example, the reserved characters [ and ] are legal as part of an IPv6 literal host in a URL like http://[1080::8:800:200C:417A]/foo but aren\'t legal in any other context, so the OP\'s example of http://example.com/file[/].html is illegal.In your supplementary question you asked if www.example.com/file[/].html is a valid URL.That URL isn\'t valid because a URL is a type of URI and a valid URI must have a scheme like http: (see RFC 3986).If you meant to ask if http://www.example.com/file[/].html is a valid URL then the answer is still no because the square bracket characters aren\'t valid there.The square bracket characters are reserved for URLs in this format: http://[2001:db8:85a3::8a2e:370:7334]/foo/bar (i.e. an IPv6 literal instead of a host name)It\'s worth reading RFC 3986 carefully if you want to understand the issue fully.All valid characters that can be used in a URI (a URL is a type of URI) are defined in RFC 3986.All other characters can be used in a URL provided that they are "URL Encoded" first.  This involves changing the invalid character for specific "codes" (usually in the form of the percent symbol (%) followed by a hexadecimal number).This link, HTML URL Encoding Reference, contains a list of the encodings for invalid characters.Several of Unicode character ranges are valid HTML5, although it might still not be a good idea to use them.E.g., href docs say http://www.w3.org/TR/html5/links.html#attr-hyperlink-href:The href attribute on a and area elements must have a value that is a valid URL potentially surrounded by spaces.Then the definition of "valid URL" points to http://url.spec.whatwg.org/, which says it aims to:Align RFC 3986 and RFC 3987 with contemporary implementations and obsolete them in the process.That document defines URL code points as:ASCII alphanumeric, "!", "$", "&", "\'", "(", ")", "*", "+", ",", "-", ".", "/", ":", ";", "=", "?", "@", "_", "~", and code points in the ranges U+00A0 to U+D7FF, U+E000 to U+FDCF, U+FDF0 to U+FFFD, U+10000 to U+1FFFD, U+20000 to U+2FFFD, U+30000 to U+3FFFD, U+40000 to U+4FFFD, U+50000 to U+5FFFD, U+60000 to U+6FFFD, U+70000 to U+7FFFD, U+80000 to U+8FFFD, U+90000 to U+9FFFD, U+A0000 to U+AFFFD, U+B0000 to U+BFFFD, U+C0000 to U+CFFFD, U+D0000 to U+DFFFD, U+E1000 to U+EFFFD, U+F0000 to U+FFFFD, U+100000 to U+10FFFD. The term "URL code points" is then used in the statement:If c is not a URL code point and not "%", parse error.in a several parts of the parsing algorithm, including the schema, authority, relative path, query and fragment states: so basically the entire URL.Also, the validator http://validator.w3.org/ passes for URLs like "\xe4\xbd\xa0\xe5\xa5\xbd", and does not pass for URLs with characters like spaces "a b"Of course, as mentioned by Stephen C, it is not just about characters but also about context: you have to understand the entire algorithm. But since class "URL code points" is used on key points of the algorithm, it that gives a good idea of what you can use or not.See also: Unicode characters in URLsNot really an answer to your question but validating url\'s is really a serious p.i.t.a\nYou\'re probably just better off validating the domainname and leave query part of the url be. That is my experience.\nYou could also resort to pinging the url and seeing if it results in a valid response but that might be too much for such a simple task.Regular expressions to detect url\'s are abundant, google it :)I need to select character to split urls in string, so I decided to create list of characters which could not be found in URL by myself:So, the possible choices are the newline, tab, space, backslash and "<>{}^|. I guess I\'ll go with the space or newline. :)Use urlencode to allow arbitrary characters in your URL.I came up with a couple regular expressions for PHP that will convert urls in text to anchor tags. (First it converts all www. urls to http:// then converts all urls with https?:// to a href=... html links $string = preg_replace(\'/(https?:\\/\\/)([!#$&-;=?\\-\\[\\]_a-z~%]+)/sim\', \'<a href="$1$2">$2</a>\', \n     preg_replace(\'/(\\s)((www\\.)([!#$&-;=?\\-\\[\\]_a-z~%]+))/sim\', \'$1http://$2\', $string)\n);