Lately I have been playing a game on my iPhone called Scramble. Some of you may know this game as Boggle. Essentially, when the game starts you get a matrix of letters like so:The goal of the game is to find as many words as you can that can be formed by chaining letters together. You can start with any letter, and all the letters that surround it are fair game, and then once you move on to the next letter, all the letters that surround that letter are fair game, except for any previously used letters. So in the grid above, for example, I could come up with the words LOB, TUX, SEA, FAME, etc. Words must be at least 3 characters, and no more than NxN characters, which would be 16 in this game but can vary in some implementations.  While this game is fun and addictive, I am apparently not very good at it and I wanted to cheat a little bit by making a program that would give me the best possible words (the longer the word the more points you get).Sample Boggle http://www.boggled.org/sample.gifI am, unfortunately, not very good with algorithms or their efficiencies and so forth. My first attempt uses a dictionary such as this one (~2.3MB) and does a linear search trying to match combinations with dictionary entries. This takes a very long time to find the possible words, and since you only get 2 minutes per round, it is simply not adequate.I am interested to see if any Stackoverflowers can come up with more efficient solutions. I am mostly looking for solutions using the Big 3 Ps: Python, PHP, and Perl, although anything with Java or C++ is cool too, since speed is essential.CURRENT SOLUTIONS:BOUNTY:I am adding a bounty to this question as my way of saying thanks to all the people who pitched in with their programs. Unfortunately I can only give the accepted answer to one of you, so I\'ll measure who has the fastest boggle solver 7 days from now and award the winner the bounty.Bounty awarded. Thanks to everyone that participated.My answer works like the others here, but I\'ll post it because it looks a bit faster than the other Python solutions, from setting up the dictionary faster. (I checked this against John Fouhy\'s solution.) After setup, the time to solve is down in the noise.Sample usage:Edit: Filter out words less than 3 letters long.Edit 2: I was curious why Kent Fredric\'s Perl solution was faster; it turns out to use regular-expression matching instead of a set of characters. Doing the same in Python about doubles the speed.The fastest solution you\'re going to get will probably involve storing your dictionary in a trie.  Then, create a queue of triplets (x, y, s), where each element in the queue corresponds to a prefix s of a word which can be spelled in the grid, ending at location (x, y).  Initialize the queue with N x N elements (where N is the size of your grid), one element for each square in the grid.  Then, the algorithm proceeds as follows:If you store your dictionary in a trie, testing if s+c is a word or a prefix of a word can be done in constant time (provided you also keep some extra metadata in each queue datum, such as a pointer to the current node in the trie), so the running time of this algorithm is O(number of words that can be spelled).[Edit] Here\'s an implementation in Python that I just coded up:Example usage:Output:[\'fa\', \'xi\', \'ie\', \'io\', \'el\', \'am\', \'ax\', \'ae\', \'aw\', \'mi\', \'ma\', \'me\', \'lo\', \'li\', \'oe\', \'ox\', \'em\', \'ea\', \'ea\', \'es\', \'wa\', \'we\', \'wa\', \'bo\', \'bu\', \'as\', \'aw\', \'ae\', \'st\', \'se\', \'sa\', \'tu\', \'ut\', \'fam\', \'fae\', \'imi\', \'eli\', \'elm\', \'elb\', \'ami\', \'ama\', \'ame\', \'aes\', \'awl\', \'awa\', \'awe\', \'awa\', \'mix\', \'mim\', \'mil\', \'mam\', \'max\', \'mae\', \'maw\', \'mew\', \'mem\', \'mes\', \'lob\', \'lox\', \'lei\', \'leo\', \'lie\', \'lim\', \'oil\', \'olm\', \'ewe\', \'eme\', \'wax\', \'waf\', \'wae\', \'waw\', \'wem\', \'wea\', \'wea\', \'was\', \'waw\', \'wae\', \'bob\', \'blo\', \'bub\', \'but\', \'ast\', \'ase\', \'asa\', \'awl\', \'awa\', \'awe\', \'awa\', \'aes\', \'swa\', \'swa\', \'sew\', \'sea\', \'sea\', \'saw\', \'tux\', \'tub\', \'tut\', \'twa\', \'twa\', \'tst\', \'utu\', \'fama\', \'fame\', \'ixil\', \'imam\', \'amli\', \'amil\', \'ambo\', \'axil\', \'axle\', \'mimi\', \'mima\', \'mime\', \'milo\', \'mile\', \'mewl\', \'mese\', \'mesa\', \'lolo\', \'lobo\', \'lima\', \'lime\', \'limb\', \'lile\', \'oime\', \'oleo\', \'olio\', \'oboe\', \'obol\', \'emim\', \'emil\', \'east\', \'ease\', \'wame\', \'wawa\', \'wawa\', \'weam\', \'west\', \'wese\', \'wast\', \'wase\', \'wawa\', \'wawa\', \'boil\', \'bolo\', \'bole\', \'bobo\', \'blob\', \'bleo\', \'bubo\', \'asem\', \'stub\', \'stut\', \'swam\', \'semi\', \'seme\', \'seam\', \'seax\', \'sasa\', \'sawt\', \'tutu\', \'tuts\', \'twae\', \'twas\', \'twae\', \'ilima\', \'amble\', \'axile\', \'awest\', \'mamie\', \'mambo\', \'maxim\', \'mease\', \'mesem\', \'limax\', \'limes\', \'limbo\', \'limbu\', \'obole\', \'emesa\', \'embox\', \'awest\', \'swami\', \'famble\', \'mimble\', \'maxima\', \'embolo\', \'embole\', \'wamble\', \'semese\', \'semble\', \'sawbwa\', \'sawbwa\']Notes: This program doesn\'t output 1-letter words, or filter by word length at all.  That\'s easy to add but not really relevant to the problem.  It also outputs some words multiple times if they can be spelled in multiple ways.  If a given word can be spelled in many different ways (worst case: every letter in the grid is the same (e.g. \'A\') and a word like \'aaaaaaaaaa\' is in your dictionary), then the running time will get horribly exponential.  Filtering out duplicates and sorting is trivial to due after the algorithm has finished.For a dictionary speedup, there is one general transformation/process you can do to greatly reduce the dictionary comparisons ahead of time. Given that the above grid contains only 16 characters, some of them duplicate, you can greatly reduce the number of total keys in your dictionary by simply filtering out entries that have unattainable characters. I thought this was the obvious optimization but seeing nobody did it I\'m mentioning it. It reduced me from a dictionary of 200,000 keys to only 2,000 keys simply during the input pass. This at the very least reduces memory overhead, and that\'s sure to map to a speed increase somewhere as memory isn\'t infinitely fast. My implementation is a bit top-heavy because I placed importance on being able to know the exact path of every extracted string, not just the validity therein. I also have a few adaptions in there that would theoretically permit a grid with holes in it to function, and grids with different sized lines ( assuming you get the input right and it lines up somehow ). The early-filter is by far the most significant bottleneck in my application, as suspected earlier, commenting out that line bloats it from 1.5s to 7.5s. Upon execution it appears to think all the single digits are on their own valid words,  but I\'m pretty sure thats due to how the dictionary file works. Its a bit bloated, but at least I reuse Tree::Trie from cpanSome of it was inspired partially by the existing implementations, some of it I had in mind already. Constructive Criticism and ways it could be improved welcome ( /me notes he never searched CPAN for a boggle solver, but this was more fun to work out ) updated for new criteriaArch/execution info for comparison:The regex optimization I use is useless for multi-solve dictionaries, and for multi-solve you\'ll want a full dictionary, not a pre-trimmed one. However, that said, for one-off solves, its really fast. ( Perl regex are in C! :)  )Here is some varying code additions:  ps: 8.16 * 200 = 27 minutes. You could split the problem up into two pieces:Ideally, (2) should also include a way of testing whether a string is a prefix of a valid word \xe2\x80\x93 this will allow you to prune your search and save a whole heap of time.Adam Rosenfield\'s Trie is a solution to (2).  It\'s elegant and probably what your algorithms specialist would prefer, but with modern languages and modern computers, we can be a bit lazier.  Also, as Kent suggests, we can reduce our dictionary size by discarding words that have letters not present in the grid.  Here\'s some python:Wow; constant-time prefix testing.  It takes a couple of seconds to load the dictionary you linked, but only a couple :-) (notice that words <= prefixes)Now, for part (1), I\'m inclined to think in terms of graphs.  So I\'ll build a dictionary that looks something like this:i.e. graph[(x, y)] is the set of coordinates that you can reach from position (x, y).  I\'ll also add a dummy node None which will connect to everything.Building it\'s a bit clumsy, because there\'s 8 possible positions and you have to do bounds checking.  Here\'s some correspondingly-clumsy python code:This code also builds up a dictionary mapping (x,y) to the corresponding character.  This lets me turn a list of positions into a word:Finally, we do a depth-first search.  The basic procedure is:Python:Run the code as:and inspect res to see the answers.  Here\'s a list of words found for your example, sorted by size:The code takes (literally) a couple of seconds to load the dictionary, but the rest is instant on my machine.My attempt in Java. It takes about 2 s to read file and build trie, and around 50 ms to solve the puzzle.  I used the dictionary linked in the question (it has a few words that I didn\'t know exist in English such as fae, ima)Source code consists of 6 classes.  I\'ll post them below (if this is not the right practice on StackOverflow, please tell me).Surprisingly, no one attempted a PHP version of this.This is a working PHP version of John Fouhy\'s Python solution.Although I took some pointers from everyone else\'s answers, this is mostly copied from John. Here is a live link if you want to try it out. Although it takes ~2s in my local machine, it takes ~5s on my webserver. In either case, it is not very fast.  Still, though, it is quite hideous so I can imagine the time can be reduced significantly. Any pointers on how to accomplish that would be appreciated. PHP\'s lack of tuples made the coordinates weird to work with and my inability to comprehend just what the hell is going on didn\'t help at all.EDIT: A few fixes make it take less than 1s locally.I think you will probably spend most of your time trying to match words that can\'t possibly be built by your letter grid. So, the first thing I would do is try to speed up that step and that should get you most of the way there.For this, I would re-express the grid as a table of possible "moves" that you index by the letter-transition you are looking at.Start by assigning each letter a number from your entire alphabet (A=0, B=1, C=2, ... and so forth). Let\'s take this example:And for now, lets use the alphabet of the letters we have (usually you\'d probably want to use the same whole alphabet every time):Then you make a 2D boolean array that tells whether you have a certain letter transition available:Now go through your word list and convert the words to transitions:Then check if these transitions are allowed by looking them up in your table:If they are all allowed, there\'s a chance that this word might be found.For example the word "helmet" can be ruled out on the 4th transition (m to e: helMEt), since that entry in your table is false.And the word hamster can be ruled out, since the first (h to a) transition is not allowed (doesn\'t even exist in your table).Now, for the probably very few remaining words that you didn\'t eliminate, try to actually find them in the grid the way you\'re doing it now or as suggested in some of the other answers here. This is to avoid false positives that result from jumps between identical letters in your grid. For example the word "help" is allowed by the table, but not by the grid.Some further performance improvement tips on this idea:Instead of using a 2D array, use a 1D array and simply compute the index of the second letter yourself. So, instead of a 12x12 array like above, make a 1D array of length 144. If you then always use the same alphabet (i.e. a 26x26 = 676x1 array for the standard english alphabet), even if not all letters show up in your grid, you can pre-compute the indices into this 1D array that you need to test to match your dictionary words. For example, the indices for \'hello\' in the example above would beExtend the idea to a 3D table (expressed as a 1D array), i.e. all allowed 3-letter combinations. That way you can eliminate even more words immediately and you reduce the number of array lookups for each word by 1: For \'hello\', you only need 3 array lookups: hel, ell, llo. It will be very quick to build this table, by the way, as there are only 400 possible 3-letter-moves in your grid.Pre-compute the indices of the moves in your grid that you need to include in your table. For the example above, you need to set the following entries to \'True\':I\'m sure if you use this approach you can get your code to run insanely fast, if you have the dictionary pre-computed and already loaded into memory.BTW: Another nice thing to do, if you are building a game, is to run these sort of things immediately in the background. Start generating and solving the first game while the user is still looking at the title screen on your app and getting his finger into position to press "Play". Then generate and solve the next game as the user plays the previous one. That should give you a lot of time to run your code.(I like this problem, so I\'ll probably be tempted to implement my proposal in Java sometime in the next days to see how it would actually perform... I\'ll post the code here once I do.)UPDATE:Ok, I had some time today and implemented this idea in Java:Here are some results:For the grid from the picture posted in the original question (DGHI...):For the letters posted as the example in the original question (FXIE...)For the following 5x5-grid:it gives this:For this I used the TWL06 Tournament Scrabble Word List, since the link in the original question no longer works. This file is 1.85MB, so it\'s a little bit shorter. And the buildDictionary function throws out all words with less than 3 letters.Here are a couple of observations about the performance of this:It\'s about 10 times slower than the reported performance of Victor Nicollet\'s OCaml implementation. Whether this is caused by the different algorithm, the shorter dictionary he used, the fact that his code is compiled and mine runs in a Java virtual machine, or the performance of our computers (mine is an Intel Q6600 @ 2.4MHz running WinXP), I don\'t know. But it\'s much faster than the results for the other implementations quoted at the end of the original question. So, whether this algorithm is superior to the trie dictionary or not, I don\'t know at this point.The table method used in checkWordTriplets() yields a very good approximation to the actual answers. Only 1 in 3-5 words passed by it will fail the checkWords() test (See number of candidates vs. number of actual words above).Something you can\'t see above: The checkWordTriplets() function takes about 3.65ms and is therefore fully dominant in the search process. The checkWords() function takes up pretty much the remaining 0.05-0.20 ms.The execution time of the checkWordTriplets() function depends linearly on the dictionary size and is virtually independent of board size!The execution time of checkWords() depends on the board size and the number of words not ruled out by checkWordTriplets().The checkWords() implementation above is the dumbest first version I came up with. It is basically not optimized at all. But compared to checkWordTriplets() it is irrelevant for the total performance of the application, so I didn\'t worry about it. But, if the board size gets bigger, this function will get slower and slower and will eventually start to matter. Then, it would need to be optimized as well.One nice thing about this code is its flexibility:Ok, but I think by now this post is waaaay long enough. I can definitely answer any questions you might have, but let\'s move that to the comments.Not interested in VB? :) I couldn\'t resist. I\'ve solved this differently than many of the solutions presented here.My times are:EDIT: Dictionary load times on the web host server are running about 1 to 1.5 seconds longer than my home computer.I don\'t know how badly the times will deteriorate with a load on the server.I wrote my solution as a web page in .Net. myvrad.com/boggleI\'m using the dictionary referenced in the original question.Letters are not reused in a word. Only words 3 characters or longer are found.I\'m using a hashtable of all unique word prefixes and words instead of a trie. I didn\'t know about trie\'s so I learned something there. The idea of creating a list of prefixes of words in addition to the complete words is what finally got my times down to a respectable number.Read the code comments for additional details.Here\'s the code:As soon as I saw the problem statement, I thought "Trie". But seeing as several other posters made use of that approach, I looked for another approach just to be different. Alas, the Trie approach performs better. I ran Kent\'s Perl solution on my machine and it took 0.31 seconds to run, after adapting it to use my dictionary file. My own perl implementation required 0.54 seconds to run. This was my approach:Create a transition hash to model the legal transitions.Iterate through all 16^3 possible three letter combinations.Then loop through all words in the dictionary.Print out the words I found.I tried 3-letter and 4-letter sequences, but 4-letter sequences slowed the program down.In my code, I use /usr/share/dict/words for my dictionary. It comes standard on MAC OS X and many Unix systems. You can use another file if you want. To crack a different puzzle, just change the variable @puzzle. This would be easy to adapt for larger matrices. You would just need to change the %transitions hash and %legalTransitions hash. The strength of this solution is that the code is short, and the data structures simple.Here is the Perl code (which uses too many global variables, I know):I spent 3 months working on a solution to the 10 best point dense 5x5 Boggle boards problem.The problem is now solved and laid out with full disclosure on 5 web pages.  Please contact me with questions.The board analysis algorithm uses an explicit stack to pseudo-recursively traverse the board squares through a Directed Acyclic Word Graph with direct child information, and a time stamp tracking mechanism.  This may very well be the world\'s most advanced lexicon data structure.The scheme evaluates some 10,000 very good boards per second on a quad core. (9500+ points)Parent Web Page:DeepSearch.c - http://www.pathcom.com/~vadco/deep.htmlComponent Web Pages:Optimal Scoreboard - http://www.pathcom.com/~vadco/binary.htmlAdvanced Lexicon Structure - http://www.pathcom.com/~vadco/adtdawg.htmlBoard Analysis Algorithm - http://www.pathcom.com/~vadco/guns.htmlParallel Batch Processing - http://www.pathcom.com/~vadco/parallel.html-\nThis comprehensive body of work will only interest a person who demands the very best.I know I\'m super late but I made one of these a while ago in PHP - just for fun too...http://www.lostsockdesign.com.au/sandbox/boggle/index.php?letters=fxieamloewbxastu\nFound 75 words (133 pts) in 0.90108 secondsF.........X..I..............E...............\nA......................................M..............................L............................O...............................\nE....................W............................B..........................X\nA..................S..................................................T.................U....Gives some indication of what the program is actually doing - each letter is where it starts looking through the patterns while each \'.\' shows a path that it has tried to take. The more \'.\' there are the further it has searched.Let me know if you want the code... it is a horrible mix of PHP and HTML that was never meant to see the light of day so I dare not post it here :PDoes your search algorithm continually decrease the word list as your search continues?For instance, in the search above there are only 13 letters that your words can start with (effectively reducing to half as many starting letters).As you add more letter permutations it would further decrease the available word sets decreasing the searching necessary.I\'d start there.I\'d have to give more thought to a complete solution, but as a handy optimisation, I wonder whether it might be worth pre-computing a table of frequencies of digrams and trigrams (2- and 3-letter combinations) based on all the words from your dictionary, and use this to prioritise your search. I\'d go with the starting letters of words. So if your dictionary contained the words "India", "Water", "Extreme", and "Extraordinary", then your pre-computed table might be:Then search for these digrams in the order of commonality (first EX, then WA/IN)First, read how one of the C# language designers solved a related problem: \nhttp://blogs.msdn.com/ericlippert/archive/2009/02/04/a-nasality-talisman-for-the-sultana-analyst.aspx.Like him, you can start with a dictionary and the canonacalize words by creating a dictionary from an array of letters sorted alphabetically to a list of words that can be spelled from those letters. Next, start creating the possible words from the board and looking them up. I suspect that will get you pretty far, but there are certainly more tricks that might speed things up. I suggest making a tree of letters based on words.  The tree would be composed of a letter structs, like this:Then you build up the tree, with each depth adding a new letter.  In other words, on the first level there\'d be the alphabet; then from each of those trees, there\'d be another another 26 entries, and so on, until you\'ve spelled out all the words.  Hang onto this parsed tree, and it\'ll make all possible answers faster to look up.With this parsed tree, you can very quickly find solutions.  Here\'s the pseudo-code:This could be sped up with a bit of dynamic programming.  For example, in your sample, the two \'A\'s are both next to an \'E\' and a \'W\', which (from the point they hit them on) would be identical.  I don\'t have enough time to really spell out the code for this, but I think you can gather the idea.Also, I\'m sure you\'ll find other solutions if you Google for "Boggle solver".Just for fun, I implemented one in bash. \nIt is not super fast, but reasonable. http://dev.xkyle.com/bashboggle/Hilarious. I nearly posted the same question a few days ago due to the same damn game! I did not however because just searched google for boggle solver python and got all the answers I could want.I realize this question\'s time has come and gone, but since I was working on a solver myself, and stumbled onto this while googling about, I thought I should post a reference to mine as it seems a bit different from some of the others.I chose to go with a flat array for the game board, and to do recursive hunts from each letter on the board, traversing from valid neighbor to valid neighbor, extending the hunt if the current list of letters if a valid prefix in an index. While traversing the notion of the current word is list of indexes into board, not letters that make up a word. When checking the index, the indexes are translated to letters and the check done.The index is a brute force dictionary that\'s a bit like a trie, but allows for Pythonic queries of the index. If the words \'cat\' and \'cater\' are in the list, you\'ll get this in the dictionary:So if the current_word is \'ca\' you know that it is a valid prefix because \'ca\' in d returns True (so continue the board traversal). And if the current_word is \'cat\' then you know that it is a valid word because it is a valid prefix and \'cat\' in d[\'cat\'] returns True too.If felt like this allowed for some readable code that doesn\'t seem too slow. Like everyone else the expense in this system is reading/building the index. Solving the board is pretty much noise.The code is at http://gist.github.com/268079. It is intentionally vertical and naive with lots of explicit validity checking because I wanted to understand the problem without crufting it up with a bunch of magic or obscurity.I wrote my solver in C++. I implemented a custom tree structure. I\'m not sure it can be considered a trie but it\'s similar. Each node has 26 branches, 1 for each letter of the alphabet. I traverse the branches of the boggle board in parallel with the branches of my dictionary. If the branch does not exist in the dictionary, I stop searching it on the Boggle board. I convert all the letters on the board to ints. So \'A\' = 0. Since it\'s just arrays, lookup is always O(1). Each node stores if it completes a word and how many words exist in its children. The tree is pruned as words are found to reduce repeatedly searching for the same words. I believe pruning is also O(1).CPU: Pentium SU2700 1.3GHz\nRAM: 3gb  Loads dictionary of 178,590 words in < 1 second.\nSolves 100x100 Boggle (boggle.txt) in 4 seconds. ~44,000 words found.\nSolving a 4x4 Boggle is too fast to provide a meaningful benchmark. :)Fast Boggle Solver GitHub RepoGiven a Boggle board with N rows and M columns, let\'s assume the following:Under these assumptions, the complexity of this solution is O(N*M).I think comparing running times for this one example board in many ways misses the point but, for the sake of completeness, this solution completes in <0.2s on my modern MacBook Pro.This solution will find all possible paths for each word in the corpus.I have implemented a solution in OCaml. It pre-compiles a dictionary as a trie, and uses two-letter sequence frequencies to eliminate edges that could never appear in a word to further speed up processing. It solves your example board in 0.35ms (with an additional 6ms start-up time which is mostly related to loading the trie into memory). The solutions found: So I wanted to add another PHP way of solving this, since everyone loves PHP.\nThere\'s a little bit of refactoring I would like to do, like using a regexpression match against the dictionary file, but right now I\'m just loading the entire dictionary file into a wordList.I did this using a linked list idea. Each Node has a character value, a location value, and a next pointer.The location value is how I found out if two nodes are connected.So using that grid, I know two nodes are connected if the first node\'s location equals the second nodes location +/- 1 for the same row, +/- 9, 10, 11 for the row above and below.I use recursion for the main search. It takes a word off the wordList, finds all the possible starting points, and then recursively finds the next possible connection, keeping in mind that it can\'t go to a location it\'s already using (which is why I add $notInLoc).Anyway, I know it needs some refactoring, and would love to hear thoughts on how to make it cleaner, but it produces the correct results based on the dictionary file I\'m using. Depending on the number of vowels and combinations on the board, it takes about 3 to 6 seconds. I know that once I preg_match the dictionary results, that will reduce significantly.Here is my java implementation: https://github.com/zouzhile/interview/blob/master/src/com/interview/algorithms/tree/BoggleSolver.javaTrie build took 0 hours, 0 minutes, 1 seconds, 532 milliseconds\nWord searching took 0 hours, 0 minutes, 0 seconds, 92 millisecondsNote:\nI used the dictionary and character matrix at the beginning of this thread. The code was run on my MacBookPro, below is some information about the machine. Model Name:   MacBook Pro\n  Model Identifier: MacBookPro8,1\n  Processor Name:   Intel Core i5\n  Processor Speed:  2.3 GHz\n  Number Of Processors: 1\n  Total Number Of Cores:    2\n  L2 Cache (per core):  256 KB\n  L3 Cache: 3 MB\n  Memory:   4 GB\n  Boot ROM Version: MBP81.0047.B0E\n  SMC Version (system): 1.68f96I solved this too, with Java. My implementation is 269 lines long and pretty easy to use. First you need to create a new instance of the Boggler class and then call the solve function with the grid as a parameter. It takes about 100 ms to load the dictionary of 50 000 words on my computer and it finds the words in about 10-20 ms. The found words are stored in an ArrayList, foundWords.I solved this in c. It takes around 48 ms to run on my machine (with around 98% of the time spent loading the dictionary from disk and creating the trie). The dictionary is /usr/share/dict/american-english which has 62886 words.Source codeI solved this perfectly and very fast. I put it into an android app. View the video at the play store link to see it in action. Word Cheats is an app that "cracks" any matrix style word game. This app was built\nto to help me cheat at word scrambler. It can be used for word searches,\nruzzle, words, word finder, word crack, boggle, and more!It can be seen here\nhttps://play.google.com/store/apps/details?id=com.harris.wordcrackerView the app in action in the video\nhttps://www.youtube.com/watch?v=DL2974WmNAII have solved this in C#, using a DFA algorithm. You can check out my code athttps://github.com/attilabicsko/wordshuffler/In addition to finding words in a matrix, my algorithm saves the actual paths for the words, so for designing a word finder game, you can check wether there is a word on an actual path. How about simple sorting and using the binary search in the dictionary?Returns whole list in 0.35 sec and can be further optimized (by for instance removing words with unused letters etc.).A Node.JS JavaScript solution. Computes all 100 unique words in less than a second which includes reading dictionary file (MBA 2012).Output:\n["FAM","TUX","TUB","FAE","ELI","ELM","ELB","TWA","TWA","SAW","AMI","SWA","SWA","AME","SEA","SEW","AES","AWL","AWE","SEA","AWA","MIX","MIL","AST","ASE","MAX","MAE","MAW","MEW","AWE","MES","AWL","LIE","LIM","AWA","AES","BUT","BLO","WAS","WAE","WEA","LEI","LEO","LOB","LOX","WEM","OIL","OLM","WEA","WAE","WAX","WAF","MILO","EAST","WAME","TWAS","TWAE","EMIL","WEAM","OIME","AXIL","WEST","TWAE","LIMB","WASE","WAST","BLEO","STUB","BOIL","BOLE","LIME","SAWT","LIMA","MESA","MEWL","AXLE","FAME","ASEM","MILE","AMIL","SEAX","SEAM","SEMI","SWAM","AMBO","AMLI","AXILE","AMBLE","SWAMI","AWEST","AWEST","LIMAX","LIMES","LIMBU","LIMBO","EMBOX","SEMBLE","EMBOLE","WAMBLE","FAMBLE"]Code: