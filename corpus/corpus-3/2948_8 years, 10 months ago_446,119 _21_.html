I have a web directory where I store some config files. I\'d like to use wget to pull those files down and maintain their current structure. For instance, the remote directory looks like:.vim holds multiple files and directories. I want to replicate that on the client using wget. Can\'t seem to find the right combo of wget flags to get this done. Any ideas?For me I have to pass the --no-parent option, otherwise it will follow the link in the directory index on my site to the parent directory. So the command would look like this:Edit 1: To avoid downloading the index.html files, use this command:Edit 2: To avoid downloading the directory structure as well:To download a directory recursively, which rejects index.html* files and downloads without the hostname, parent directory and the whole directory structure :For anyone else that having similar issues. Wget follows robots.txt which might not allow you to grab the site. No worries, you can turn it off:http://www.gnu.org/software/wget/manual/html_node/Robot-Exclusion.htmlHere\'s the complete wget command that worked for me to download files from a server\'s directory (ignoring robots.txt):You should use the -m (mirror) flag, as that takes care to not mess with timestamps and to recurse indefinitely.If you add the points mentioned by others in this thread, it would be:works for me.Perhaps you have a .wgetrc which is interfering with it?If --no-parent not help, you might use --include option.Directory struct:And you want to download downloads/good but not downloads/bad directory:To fetch a directory recursively with username and password, use the following command: You should be able to do it simply by adding a -rWget 1.18 may work better, e.g., I got bitten by a version 1.12 bug where......only retrieves index.html instead of all files.Workaround was to notice some 301 redirects and try the new location \xe2\x80\x94 given the new URL, wget got all the files in the directory.