I\'m a programmer with a decent background in math and computer science. I\'ve studied computability, graph theory, linear algebra, abstract algebra, algorithms, and a little probability and statistics (through a few CS classes) at an undergraduate level.I feel, however, that I don\'t know enough about statistics. Statistics are increasingly useful in computing, with statistical natural language processing helping fuel some of Google\'s algorithms for search and machine translation, with performance analysis of hardware, software, and networks needing proper statistical grounding to be at all believable, and with fields like bioinformatics becoming more prevalent every day. I\'ve read about how "Google uses Bayesian filtering the way Microsoft uses the if statement", and I know the power of even fairly na\xc3\xafve, simple statistical approaches to problems from Paul Graham\'s A Plan for Spam and Better Bayesian Filtering, but I\'d like to go beyond that.I\'ve tried to look into learning more statistics, but I\'ve gotten a bit lost. The Wikipedia article has a long list of related topics, but I\'m not sure which I should look into. I feel like from what I\'ve seen, a lot of statistics makes the assumption that everything is a combination of factors that linearly combine, plus some random noise in a Gaussian distribution; I\'m wondering what I should learn beyond linear regression, or if I should spend the time to really understand that before I move on to other techniques. I\'ve found a few long lists of books to look at; where should I start?So I\'m wondering where to go from here; what to learn, and where to learn it. In particular, I\'d like to know:To clarify what I am looking for, I am interested in what problems that programmers typically need to deal with can benefit from a statistical approach, and what kind of statistical tools can be useful. For instance:Interesting question. As a statistician whose interest is more and more aligned with computer science perhaps I could provide a few thoughts...Don\'t learn frequentist hypothesis testing. While the bulk of my work is done in this paradigm, it doesn\'t match the needs of business or data mining. Scientists generally have specific hypotheses in mind, and might wish to gauge the probability that, given their hypothesis isn\'t true, the data would be as extreme as it is. This is rarely the type of answer a computer scientist wants.Bayesian is useful, even if you don\'t know why you are assuming the priors that you are using. A baysian analysis can give you a precise probability estimate for various contingencies, but it is important to realize that the only reason you have this precise estimate is because you made a fuzzy decision regarding the prior probability. (For those not in the know, with baysian inference, you can specify an arbitrary prior probability, and update this based on the data collected to get a better estimate).Machine learning and classification might be a good place to get started. The machine learning literature is more focused on computer science problems, though it\'s mission is almost identical to that of statistics ( see: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/ ).Since you spoke of large databases with large numbers of variables, here are a few algorithms that come in handy in this domain.This is by no means complete, but should give you a good jumping off point. A very good and accessible book on the subject is Duda, Hart, Stork: Pattern ClassificationAlso, a big part of statistics is descriptive visualizations and analysis. These are of particular interest to the programmer because they allow him/her to convey information back to the user. In R, ggplot2 is my package of choice for creating visualizations. On the descriptive analysis side (and useful in text analysis) is multi-dimensional scaling, which can give a spacial interpretation of non-spacial data (for example the ideologies of senators http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoas/1223908041).Just as a point, not as a critic, but your question should be formulated in a different way: "what statistics should any person know?". Fact is, unfortunately we all deal with statistics. It\'s a fact of life. Polls, weather forecast, drug effectiveness, insurances, and of course some parts of computer science. Being able to critically analyze the presented data gives the line between picking the right understanding or being scammed, whatever that means.Said that, I think the following points are important to understandAll these points are critical not only to you as a computer scientist, but also as a human being. I will give you some examples.The evaluation of the null hypothesis is critical for testing of the effectiveness of a method. For example, if a drug works, or if a fix to your hardware had a concrete result or it\'s just a matter of chance. Say you want to improve the speed of a machine, and change the hard drive. Does this change matters? you could do sampling of performance with the old and new hard disk, and check for differences. Even if you find that the average with the new disk is lower, that does not mean the hard disk has an effect at all. Here enters Null hypothesis testing, and it will give you a confidence interval, not a definitive answer, like : there\'s a 90 % probability that changing the hard drive has a concrete effect on the performance of your machine.Correlation is important to find out if two entities "change alike". As the internet mantra "correlation is not causation" teaches, it should be taken with care. The fact that two random variables show correlation does not mean that one causes the other, nor that they are related by a third variable (which you are not measuring). They could just behave in the same way. Look for pirates and global warming to understand the point. A correlation reports a possible signal, it does not report a finding.Bayesian. We all know the spam filter. but there\'s more. Suppose you go to a medical checkup and the result tells you have cancer (I seriously hope not, but it\'s to illustrate a point). Fact is: most of the people at this point would think "I have cancer". That\'s not true. A positive testing for cancer moves your probability of having cancer from the baseline for the population (say, 8 per thousands people have cancer, picked out of thin air number) to a higher value, which is not 100 %. How high is this number depends on the accuracy of the test. If the test is lousy, you could just be a false positive. The more accurate the method, the higher is the skew, but still not 100 %. \nOf course, if multiple independent tests all confirm that you have cancer, then it\'s very probable you actually have it, but still it\'s not 100 %. maybe it\'s 99.999 %. This is a point many people don\'t understand about bayesian statistics.Plotting methods. That\'s another thing that is always left unattended. Analysis of data does not mean anything if you cannot convey effectively what they mean via a simple plot. Depending on what information you want to put into focus, or the kind of data you have, you will prefer a xy plot, a histogram, a violin plot, or a pie chart. Now, let\'s go to your questions. I think I overindulged in just a quick note, but since my answer was voted up quite a lot, I feel it\'s better if I answer properly to your questions as much as my knowledge allows (and here is vacation, so I can indulge as much as I want over it)What kind of problems in programming,\n  software engineering, and computer\n  science are statistical methods well\n  suited for? Where am I going to get\n  the biggest payoffs?Normally, everything that has to do with data comparison which involves numerical (or reduced to numerical) input from unreliable sources. A signal from an instrument, a bunch of pages and the number of words they contain. When you get these data, and have to find a distilled answer out of the bunch, then you need statistics. Think for example to the algorithm to perform click detection on the iphone. You are using a trembling, fat stylus to refer to an icon which is much smaller than the stylus itself. Clearly, the hardware (capacitive screen) will send you a bunch of data about the finger, plus a bunch of data about random noise (air? don\'t know how it works). The driver must make sense out of this mess and give you a x,y coordinate on the screen. That needs (a lot of) statistics.What kind of statistical methods\n  should I spend my time learning?The ones I told you are more than enough, also because to understand them, you have to walk through other stuff.What resources should I use to learn\n  this? Books, papers, web sites. I\'d\n  appreciate a discussion of what each\n  book (or other resource) is about, and\n  why it\'s relevant.I learned statistics mostly from standard university courses. My first book was the "train wreck book", and it\'s very good. I also tried this one, which focuses on R \nbut it did not satisfy me particularly. You have to know things and R to get through it.Programmers frequently need to deal\n  with large databases of text in\n  natural languages, and help to\n  categorize, classify, search, and\n  otherwise process it. What statistical\n  techniques are useful here?That depends on the question you need to answer using your dataset. Programmers are frequently asked to\n  produce high-performance systems, that\n  scale well under load. But you can\'t\n  really talk about performance unless\n  you can measure it. What kind of\n  experimental design and statistical\n  tools do you need to use to be able to\n  say with confidence that the results\n  are meaningful?There are a lot of issues with measuring. Measuring is a fine and delicate art. Proper measuring is almost beyond human. The fact is that sampling introduces bias, either from the sampler, or from the method, or from the nature of the sample, or from the nature of nature. A good sampler knows these things and tries to reduce unwanted bias as much into a random distribution. The examples from the blog you posted are relevant. Say you have a startup time for a database. If you take performance measures within that time, all your measures will be biased. There\'s no statistical method that can tell you this. Only your knowledge of the system can.  Are there other problems commonly\n  encountered by programmers that would\n  benefit from a statistical approach?Every time you have an ensemble of data producers, you have statistics, so scientific computing and data analysis is obviously one place. Folksonomy and social networking is pretty much all statistics. Even stackoverflow is, in some sense, statistical. The fact that an answer is highly voted does not mean that it\'s the right one. It means that there\'s a high probability that is right, according to the evaluation of a statistical ensemble of independent evaluators. How these evaluators behave make the difference between stackoverflow, reddit and digg.I have not much to add, but it happens that I just started to read this book: D. S. Sivia with J. Skilling, \xe2\x80\x98Data Analysis\xe2\x80\x94a Bayesian tutorial\xe2\x80\x99, 2nd Edition, 2006, Oxford University Press.What caught my attention is the preface, where the author refers to a common dissatisfaction to those who approach the study of statistics:PrefaceAs an undergraduate, I always found\n  the subject of statistics to be rather\n  mysterious. This topic wasn\xe2\x80\x99t entirely\n  new to me, as we had been taught a\n  little bit about probability earlier\n  at high school; for example, I was\n  already familiar with the binomial,\n  Poisson and normal distributions. Most\n  of this made sense, but only seemed to\n  relate to things like rolling dice,\n  flipping coins, shuffling cards and so\n  on. However, having aspirations of\n  becoming a scientist, what I really\n  wanted to know was how to analyse\n  experimental data. Thus, I eagerly\n  looked forward to the lectures on\n  statistics. Sadly, they were a great\n  disappointment. Although many of the\n  tests and procedures expounded were\n  intuitively reasonable, there was\n  something deeply unsatisfactory about\n  the whole affair: there didn\xe2\x80\x99t seem to\n  be any underlying basic principles!\n  Hence, the course on \xe2\x80\x98probability and\n  statistics\xe2\x80\x99 had led to an unfortunate\n  dichotomy: probability made sense,\n  but was just a game; statistics was\n  important, but it was a bewildering\n  collection of tests with little\n  obvious rhyme or reason. While not\n  happy with this situation, I decided\n  to put aside the subject and\n  concentrate on real science. After\n  all, the predicament was just a\n  reflection of my own inadequacies and\n  I\xe2\x80\x99d just have to work at it when the\n  time came to really analyse my data.The story above is not just my own,\n  but is the all too common experience\n  of many scientists. Fortunately, it\n  doesn\xe2\x80\x99t have to be like this. What we\n  were not told in our undergraduate\n  lectures is that there is an\n  alternative approach to the whole\n  subject of data analysis which uses\n  only probability theory. In one\n  sense, it makes the topic of\n  statistics entirely superfluous. In\n  another, it provides the logical\n  justification for many of the\n  prevalent statistical tests and\n  procedures, making explicit the\n  conditions and approximations\n  implicitly assumed in their use.This book is intended to be a tutorial\n  guide to this alternative Bayesian\n  approach, including modern\n  developments such as maximum entropy....I hope this book will maintain its promises.There are a couple of preview chapters from the first edition here, from a course in Cognitive Psychology/AI where this book was adopted, and other materials from the same course here. Related software by second author here. Also a more extended preview from Google Books here.Boy, some of these answers are good. I came from much the same background and have had to get into biostatistics largely by books and by osmosis from colleagues. Here are my recommendations:Start with a solid grounding in probability, including conditional probability, Bayes\' theorem, Markov models, and some of the basic statistical distributions.If you don\'t have it, get some linear algebra, so you don\'t get scared off by matrices. If you are faced with tricky algebra and calculus, knuckle down and work through it. It\'s worth it.Statistics theory falls into two camps, frequentist and Bayesian. Frequentist is older and solid. Bayesian is newer, more flexible, and more exciting. In particular, there are the exciting things that can be done with Markov Chain Monte Carlo and related techniques.In my area, pharmacometrics, there is high payoff in being able to extract meaningful results from sparse and expensive data, so an ability in statistics is very important.Added: Here are some favorite books (not a complete list):"Statistical Distributions", 2nd Edition, by Evans, Hastings, and Peacock. It gives a very complete list of distributions, tells how they are related, how to sample and estimate them. I use it daily."Matrices and Linear Algebra", Schneider, and Barker. Very good and easy to follow."Bayesian Statistics: An Introduction", 2nd Edition, by Lee. Very concise and easy to read."Markov Chain Monte Carlo in Practice", by Gilks, Richardson, and Spiegelhalter. Loved this book, and these are the guys who make WinBugs, a terrific free package.More probability than statistics, but Bayesian Probabilty can be very useful (it underpins spam filters) and IMO more software should use it to infer a user\'s habits.Head First Statistics is an excellent book to learn statistics (a mathematician/statistician informs me that it has not so much a few errors but a few simplications of the theoretical stuff).I almost forgot to mention: How to Lie with StatisticsGreat question! I actually think it is worthwhile to step back for a minute and get to the broader picture. E.g. what I liked in Zed\'s rant was near the beginning: I question their metrics and they try\n  to back it up with lame attempts at\n  statistical reasoning. I really can\xe2\x80\x99t\n  blame them since they were probably\n  told in college that logic and reason\n  are superior to evidence and\n  observation.which to me stresses the need for empiricism.  Of course, I hear you say, you knew that and that is why you profile. Well, yes, but there is really is more than that.  Zed comes back to this in the rant about averages, and I think this rings true:  show distributions, plot the data, look at tail behaviour.So what I trying to get to is that the answer is not so much in a single book, but more in way to think about problems, about seeing the world as probabilistic. And I too find that R helps a ton in thinking and programming with and about data. One good resource about programming is "Artificial Intelligence: A Modern Approach" by Russell and Norvig. It can be a really useful resource to understand statistics-based machine learning techniques. Here\'s an excellent book, available free on the web:  \'The Elements of Statistical Learning\', by Hastie, Tsibshirani and Freidman.It covers a range of useful topics, and should be a good introduction to the machine learning field.  It\'s explanation of overfitting models is the best that I\'ve seen in ~20-30 stat books I\'ve read.What a great thread. There\'s plenty of good information in the question itself and in the answers, but I am really surprised nobody has mentioned the book Programming Collective Intelligence yet.It\'s the best book I know if you are a novice in this subject (like me) and want to put machine learning and statistics theory into practice.This book explains:Bayesian filtering, used in spam filters for classifying documents\n  based on word types and other featuresUsing decision trees not only to make predictions, but to model the way\n  decisions are made Apart from that, there\'s a great talk on TED on why everybody should learn Statistics.I hope it\'s ok with Mr. Shaw and everyone else if most of us programmers never need to know anything about statistics, or probability, or much mathematics at all.That\'s been my experience in the last 30 years, despite excellent grades in math.So, maybe the title of this question should be, "What statistics should a programmer know if he needs to know statistics?"I\'m surprised no one has mentioned a keen understanding of graphics as essential to good statistical practice.  Machine learning and Bayesian analysis are great (try Gelman\'s book if you want a formal but approachable and applied introduction to Bayes), but you can get amazingly far at understanding a problem with really good visualizations.  Tufte\'s classic is a good place to start, and the classic semiology and grammar of graphics books are worth a read.  Finally, take a look at the R ggplot2 package for a simple way to begin implementing complex graphical ideas.Perhaps take a look at the (free) book, Think Stats: Probability and Statistics for Programmers. Although it uses Python to demonstrate statistical concepts, anyone with experience in other programming languages should be able to follow along.Description from the site:Think Stats is an introduction to\n  Probability and Statistics for Python\n  programmers.If you have basic skills in Python,\n  you can use them to learn concepts in\n  probability and statistics. This new\n  book emphasizes simple techniques you\n  can use to explore real data sets and\n  answer interesting statistical\n  questions.It just depends on the area you are working on.. As an example if you are working on applications that involves sampling and data analysis the areas like Distributions (Normal, t and Chi Square) will be useful. And if your application is something like prediction software you may need a knowledge about distributions like poisson as well.If your tool is going to get some decisions based on previous data the ideas of mean, variance and standard deviation might be useful. (With Hypothesis testing)Update : Most universities provide courses on statistics. I\'ve seen some lecture notes that can be considered as short but still good. ExampleYou can do quite a bit with mean and standard deviation.It depends entirely on what problems you\'re going to be working on.I would say the stuff in "All of Statistics" (2004), by Larry Wasserman it right on.  It also has the advantage of being presented with the idea of bringing the gap between Statistics and Computer Science.Hopes it helps.My short answer is this: latent variable statistics, including both structural equation modelling and finite mixture modelling (latent class/profile). These cover an impressive number of statistical models. It\'s amazing that no one has mentioned the Bootstrap Method, Principal Component Analysis or the LASSO algorithm. They cover data reduction, simulation, and exploratory data analysis, to name a few. 