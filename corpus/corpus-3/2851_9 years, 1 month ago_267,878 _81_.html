What would be the most efficient way to compare two double or two float values?Simply doing this is not correct:But something like:Seems to waste processing.Does anyone know a smarter float comparer?Be extremely careful using any of the other suggestions. It all depends on context. I have spent a long time tracing a bugs in a system that presumed a==b if |a-b|<epsilon. The underlying problems were:The implicit presumption in an algorithm that if a==b and b==c then a==c. Using the same epsilon for lines measured in inches and lines measured in mils (.001 inch). That is a==b but 1000a!=1000b. (This is why AlmostEqual2sComplement asks for the epsilon or max ULPS).The use of the same epsilon for both the cosine of angles and the length of lines!Using such a compare function to sort items in a collection. (In this case using the builtin C++ operator == for doubles produced correct results.)Like I said: it all depends on context and the expected size of a and b.BTW, std::numeric_limits<double>::epsilon() is the "machine epsilon". It is the difference between 1.0 and the next value representable by a double. I guess that it could be used in the compare function but only if the expected values are less than 1. (This is in response to @cdv\'s answer...)Also, if you basically have int arithmetic in doubles (here we use doubles to hold int values in certain cases) your arithmetic will be correct. For example 4.0/2.0 will be the same as 1.0+1.0. This is as long as you do not do things that result in fractions (4.0/3.0) or do not go outside of the size of an int.The comparison with an epsilon value is what most people do (even in game programming).You should change your implementation a little though:Edit: Christer has added a stack of great info on this topic on a recent blog post. Enjoy.I found that the Google C++ Testing Framework contains a nice cross-platform template-based implementation of AlmostEqual2sComplement which works on both doubles and floats. Given that it is released under the BSD license, using it in your own code should be no problem, as long as you retain the license. I extracted the below code from http://code.google.com/p/googletest/source/browse/trunk/include/gtest/internal/gtest-internal.h and added the license on top.Be sure to #define GTEST_OS_WINDOWS to some value (or to change the code where it\'s used to something that fits your codebase - it\'s BSD licensed after all).Usage example:Here\'s the code:EDIT: This post is 4 years old. It\'s probably still valid, and the code is nice, but some people found improvements. Best go get the latest version of AlmostEquals right from the Google Test source code, and not the one I pasted up here.Comparing floating point numbers for depends on the context.  Since even changing the order of operations can produce different results, it is important to know how "equal" you want the numbers to be.Comparing floating point numbers by Bruce Dawson is a good place to start when looking at floating point comparison.  The following definitions are from The art of computer programming by Knuth: Of course, choosing epsilon depends on the context, and determines how equal you want the numbers to be.  Another method of comparing floating point numbers is to look at the ULP (units in last place) of the numbers.  While not dealing specifically with comparisons, the paper What every computer scientist should know about floating point numbers is a good resource for understanding how floating point works and what the pitfalls are, including what ULP is. For a more in depth approach read Comparing floating point numbers. Here is the code snippet from that link:The portable way to get epsilon in C++ isThen the comparison function becomesRealizing this is an old thread but this article is one of the most straight forward ones I have found on comparing floating point numbers and if you want to explore more it has more detailed references as well and it the main site covers a complete range of issues dealing with floating point numbers The Floating-Point Guide :Comparison.We can find a somewhat more practical article in Floating-point tolerances revisited and notes there is absolute tolerance test, which boils down to this in C++:and relative tolerance test:The article notes that the absolute test fails when x and y are large and fails in the relative case when they are small. Assuming he absolute and relative tolerance is the same a combined test would look like this:The code you wrote is bugged :The correct code would be :(...and yes this is different)I wonder if fabs wouldn\'t make you lose lazy evaluation in some case. I would say it depends on the compiler. You might want to try both. If they are equivalent in average, take the implementation with fabs.If you have some info on which of the two float is more likely to be bigger than then other, you can play on the order of the comparison to take better advantage of the lazy evaluation.Finally you might get better result by inlining this function. Not likely to improve much though...Edit: OJ, thanks for correcting your code. I erased my comment accordingly`return fabs(a - b) < EPSILON;This is fine if:But otherwise it\'ll lead you into trouble.  Double precision numbers have a resolution of about 16 decimal places.  If the two numbers you are comparing are larger in magnitude than EPSILON*1.0E16, then you might as well be saying:I\'ll examine a different approach that assumes you need to worry about the first issue and assume the second is fine your application.  A solution would be something like:This is expensive computationally, but it is sometimes what is called for.  This is what we have to do at my company because we deal with an engineering library and inputs can vary by a few dozen orders of magnitude.Anyway, the point is this (and applies to practically every programming problem): Evaluate what your needs are, then come up with a solution to address your needs -- don\'t assume the easy answer will address your needs.  If after your evaluation you find that fabs(a-b) < EPSILON will suffice, perfect -- use it!  But be aware of its shortcomings and other possible solutions too.As others have pointed out, using a fixed-exponent epsilon (such as 0.0000001) will be useless for values away from the epsilon value. For example, if your two values are 10000.000977 and 10000, then there are NO 32-bit floating-point values between these two numbers -- 10000 and 10000.000977 are as close as you can possibly get without being bit-for-bit identical. Here, an epsilon of less than 0.0009 is meaningless; you might as well use the straight equality operator.Likewise, as the two values approach epsilon in size, the relative error grows to 100%.Thus, trying to mix a fixed point number such as 0.00001 with floating-point values (where the exponent is arbitrary) is a pointless exercise. This will only ever work if you can be assured that the operand values lie within a narrow domain (that is, close to some specific exponent), and if you properly select an epsilon value for that specific test. If you pull a number out of the air ("Hey! 0.00001 is small, so that must be good!"), you\'re doomed to numerical errors. I\'ve spent plenty of time debugging bad numerical code where some poor schmuck tosses in random epsilon values to make yet another test case work.If you do numerical programming of any kind and believe you need to reach for fixed-point epsilons, READ BRUCE\'S ARTICLE ON COMPARING FLOATING-POINT NUMBERS.Comparing Floating Point NumbersGeneral-purpose comparison of floating-point numbers is generally meaningless. How to compare really depends on a problem at hand. In many problems, numbers are sufficiently discretized to allow comparing them within a given tolerance. Unfortunately, there are just as many problems, where such trick doesn\'t really work. For one example, consider working with a Heaviside (step) function of a number in question (digital stock options come to mind) when your observations are very close to the barrier. Performing tolerance-based comparison wouldn\'t do much good, as it would effectively shift the issue from the original barrier to two new ones. Again, there is no general-purpose solution for such problems and the particular solution might require going as far as changing the numerical method in order to achieve stability.Unfortunately, even your "wasteful" code is incorrect. EPSILON is the smallest value that could be added to 1.0 and change its value. The value 1.0 is very important \xe2\x80\x94 larger numbers do not change when added to EPSILON. Now, you can scale this value to the numbers you are comparing to tell whether they are different or not. The correct expression for comparing two doubles is:This is at a minimum. In general, though, you would want to account for noise in your calculations and ignore a few of the least significant bits, so a more realistic comparison would look like:If comparison performance is very important to you and you know the range of your values, then you should use fixed-point numbers instead.My class based on previously posted answers. Very similar to Google\'s code but I use a bias which pushes all NaN values above 0xFF000000. That allows a faster check for NaN.This code is meant to demonstrate the concept, not be a general solution. Google\'s code already shows how to compute all the platform specific values and I didn\'t want to duplicate all that. I\'ve done limited testing on this code.I ended up spending quite some time going through material in this great thread. I doubt everyone wants to spend so much time so I would highlight the summary of what I learned and the solution I implemented.Quick SummaryUtility Functions Implementation (C++11)I\'d be very wary of any of these answers that involves floating point subtraction (e.g., fabs(a-b) < epsilon).  First, the floating point numbers become more sparse at greater magnitudes and at high enough magnitudes where the spacing is greater than epsilon, you might as well just be doing a == b.  Second, subtracting two very close floating point numbers (as these will tend to be, given that you\'re looking for near equality) is exactly how you get catastrophic cancellation.While not portable, I think grom\'s answer does the best job of avoiding these issues.There are actually cases in numerical software where you want to check whether two floating point numbers are exactly equal.  I posted this on a similar questionhttps://stackoverflow.com/a/10973098/1447411So you can not say that "CompareDoubles1" is wrong in general.It depends on how precise you want the comparison to be. If you want to compare for exactly the same number, then just go with ==. (You almost never want to do this unless you actually want exactly the same number.) On any decent platform you can also do the following:as fabs tends to be pretty fast. By pretty fast I mean it is basically a bitwise AND, so it better be fast.And integer tricks for comparing doubles and floats are nice but tend to make it more difficult for the various CPU pipelines to handle effectively. And it\'s definitely not faster on certain in-order architectures these days due to using the stack as a temporary storage area for values that are being used frequently. (Load-hit-store for those who care.)In terms of the scale of quantities:If epsilon is the small fraction of the magnitude of quantity (i.e. relative value) in some certain physical sense and A and B types is comparable in the same sense, than I think, that the following is quite correct:I used this function for my small project and it works, but note the following:Double precision error can create a surprise for you. Let\'s say epsilon = 1.0e-6, then 1.0 and 1.000001 should NOT be considered equal according to the above code, but on my machine the function considers them to be equal, this is because 1.000001 can not be precisely translated to a binary format, it is probably 1.0000009xxx. I test it with 1.0 and 1.0000011 and this time I get the expected result.I use this code:My way may not be correct but usefulConvert both float to strings and then do string compareoperator overlaoding can also be doneYou cannot compare two double with a fixed EPSILON. Depending on the value of double, EPSILON varies.A better double comparison would be:In a more generic way:Why not perform bitwise XOR? Two floating point numbers are equal if their corresponding bits are equal. I think, the decision to place the exponent bits before mantissa was made to speed up comparison of two floats.\nI think, many answers here are missing the point of epsilon comparison. Epsilon value only depends on to what precision floating point numbers are compared. For example, after doing some arithmetic with floats you get two numbers: 2.5642943554342 and 2.5642943554345. They are not equal, but for the solution only 3 decimal digits matter so then they are equal: 2.564 and 2.564. In this case you choose epsilon equal to 0.001. Epsilon comparison is also possible with bitwise XOR. Correct me if I am wrong.