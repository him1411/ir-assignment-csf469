Is there something similar to pipefail for multiple commands, like a \'try\' statement but within bash. I would like to do something like this:And at any point, if any command fails, drop out and echo out the error of that command. I don\'t want to have to do something like:And so on... or anything like:Because the arguments of each command I believe (correct me if I\'m wrong) will interfere with each other. These two methods seem horribly long-winded and nasty to me so I\'m here appealing for a more efficient method.You can write a function that launches and tests the command for you. Assume command1 and command2 are environment variables that have been set to a command.What do you mean by "drop out and echo the error"?  If you mean you want the script to terminate as soon as any command fails, then just do at the start of the script.  Do not bother echoing the error message: let the failing command handle that.  In other words, if you do:and command2 fails, while printing an error message to stderr, then it seems that you have achieved what you want.  (Unless I misinterpret what you want!)As a corollary, any command that you write must behave well: it must report errors to stderr instead of stdout (the sample code in the question prints errors to stdout) and it must exit with a non-zero status when it fails. I have a set of scripting functions that I use extensively on my Red Hat system. They use the system functions from /etc/init.d/functions to print green [  OK  ] and red [FAILED] status indicators.You can optionally set the $LOG_STEPS variable to a log file name if you want to log which commands fail.For what it\'s worth, a shorter way to write code to check each command for success is:It\'s still tedious but at least it\'s readable.Instead of creating runner functions or using set -e, use a trap:The trap even has access to the line number and the command line of the command that triggered it. The variables are $BASH_LINENO and $BASH_COMMAND.An alternative is simply to join the commands together with && so that the first one to fail prevents the remainder from executing:This isn\'t the syntax you asked for in the question, but it\'s a common pattern for the use case you describe.  In general the commands should be responsible for printing failures so that you don\'t have to do so manually (maybe with a -q flag to silence errors when you don\'t want them).  If you have the ability to modify these commands, I\'d edit them to yell on failure, rather than wrap them in something else that does so.Notice also that you don\'t need to do:You can simply say:Personally I much prefer to use a lightweight approach, as seen here;Example usage:I\'ve developed an almost flawless try & catch implementation in bash, that allows you to write code like:You can even nest the try-catch blocks inside themselves!The code is a part of my bash boilerplate/framework. It further extends the idea of try & catch with things like error handling with backtrace and exceptions (plus some other nice features). Here\'s the code that\'s responsible just for try & catch:Feel free to use, fork and contribute - it\'s on GitHub.Sorry that I can not make a comment to the first answer\nBut you should use new instance to execute the command: cmd_output=$($@)For fish shell users who stumble on this thread.Let foo be a function that does not "return" (echo) a value, but it sets the exit code as usual.\nTo avoid checking $status after calling the function, you can do:And if it\'s too long to fit on one line:Checking status in functional mannerUsage:Output