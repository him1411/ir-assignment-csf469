I\'m learning more details in table variable. It says that temp tables are always on disk, and table variables are in memory, that is to say, the performance of table variable is better than temp table because table variable uses less IO operations than temp table.But sometimes, if there are too many records in a table variable that can not be contained in memory, the table variable will be put on disk like the temp table.But I don\'t know what the "too many records" is. 100,000 records? or 1000,000 records? How can I know if a table variable I\'m using is in memory or is on disk? Is there any function or tool in SQL Server 2005 to measure the scale of the table variable or letting me know when the table variable is put on disk from memory?Your question shows you have succumbed to some of the common misconceptions surrounding table variables and temporary tables.I have written quite an extensive answer on the DBA site looking at the differences between the two object types. This also addresses your question about disc vs memory (I didn\'t see any significant difference in behaviour between the two).Regarding the question in the title though as to when to use a table variable vs a local temporary table you don\'t always have a choice. In functions, for example, it is only possible to use a table variable and if you need to write to the table in a child scope then only a #temp table will do \n(table-valued parameters allow readonly access).Where you do have a choice some suggestions are below.Use a table variable if for a very small quantity of data (thousands of bytes)Use a temporary table for a lot of dataAnother way to think about it: if you think you might benefit from an index, automated statistics, or any SQL optimizer goodness, then your data set is probably too large for a table variable.In my example, I just wanted to put about 20 rows into a format and modify them as a group, before using them to UPDATE / INSERT a permanent table. So a table variable is perfect.But I am also running SQL to back-fill thousands of rows at a time, and I can definitely say that the temporary tables perform much better than table variables.This is not unlike how CTE\'s are a concern for a similar size reason - if the data in the CTE is very small, I find a CTE performs as good as or better than what the optimizer comes up with, but if it is quite large then it hurts you bad.My understanding is mostly based on http://www.developerfusion.com/article/84397/table-variables-v-temporary-tables-in-sql-server/, which has a lot more detail.Microsoft says here Table variables does not have distribution statistics, they will not trigger recompiles. Therefore, in many cases, the optimizer will build a query plan on the assumption that the table variable has no rows. For this reason, you should be cautious about using a table variable if you expect a larger number of rows (greater than 100). Temp tables may be a better solution in this case.I totally agree with Abacus (sorry - don\'t have enough points to comment).Also, keep in mind it doesn\'t necessarily come down to how many records you have, but the size of your records.For instance, have you considered the performance difference between 1,000 records with 50 columns each vs 100,000 records with only 5 columns each?Lastly, maybe you\'re querying/storing more data than you need? Here\'s a good read on SQL optimization strategies. Limit the amount of data you\'re pulling, especially if you\'re not using it all (some SQL programmers do get lazy and just select everything even though they only use a tiny subset). Don\'t forget the SQL query analyzer may also become your best friend. Variable table is available only to the current session, for example, if you need to EXEC another stored procedure within the current one you will have to pass the table as Table Valued Parameter and of course this will affect the performance, with temporary tables you can do this with only passing the temporary table name To test a Temporary table: To test a Variable table: Something else I have experienced is: If your schema doesn\'t have GRANT privilege to create tables then use variable tables. Moreover the temporary tables located in tempdb database and the table variables can be located on both memory and the disk of tempdb database. See here: https://www.linkedin.com/pulse/highlighted-differences-between-sql-server-temporary-tables-andrew writing data in tables declared declare @tb and after joining with other tables, I realized that the response time compared to temporary tables tempdb .. # tb is much higher.When I join them with @tb the time is much longer to return the result, unlike #tm, the return is almost instantaneous.I did tests with a 10,000 rows join and join with 5 other tables