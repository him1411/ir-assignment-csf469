How could one convert a string to upper case. The examples I have found from googling only have to deal with chars.Boost string algorithms:Short solution using C++11 and toupper().Note: A couple of problems with the top solution:21.5 Null-terminated sequence utilitiesThe contents of these headers shall be the same as the Standard C Library headers <ctype.h>, <wctype.h>, <string.h>, <wchar.h>, and <stdlib.h> [...]Which means that the cctype members may well be macros not suitable for direct consumption in standard algorithms.Another problem with the same example is that it does not cast the argument or verify that this is non-negative; this is especially dangerous for systems where plain char is signed. (The reason being: if this is implemented as a macro it will probably use a lookup table and your argument indexes into that table. A negative index will give you UB.)Do you have ASCII or International characters in strings? If it\'s the latter case, "uppercasing" is not that simple, and it depends on the used alphabet. There are bicameral and unicameral alphabets. Only bicameral alphabets have different characters for upper and lower case. Also, there are composite characters, like Latin capital letter \'DZ\' (\\u01F1 \'DZ\') which use the so called title case. This means that only the first character (D) gets changed.I suggest you look into ICU, and difference between Simple and Full Case Mappings. This might help:http://userguide.icu-project.org/transforms/casemappingsOr,The faster one if you use only ASCII characters:This problem is vectorizable with SIMD for the ASCII character set.  Preliminary testing with x86-64 gcc 5.2 -O3 -march=native on a Core2Duo (Merom).  The same string of 120 characters (mixed lowercase and non-lowercase ASCII), converted in a loop 40M times (with no cross-file inlining, so the compiler can\'t optimize away or hoist any of it out of the loop).  Same source and dest buffers, so no malloc overhead or memory/cache effects: data is hot in L1 cache the whole time, and we\'re purely CPU-bound.boost::to_upper_copy<char*, std::string>():  198.0s.  Yes, Boost 1.58 on Ubuntu 15.10 is really this slow.  I profiled and single-stepped the asm in a debugger, and it\'s really, really bad:  there\'s a dynamic_cast of a locale variable happening per character!!!  (dynamic_cast takes multiple calls to strcmp).  This happens with LANG=C and with LANG=en_CA.UTF-8.I didn\'t test using a RangeT other than std::string.  Maybe the other form of to_upper_copy optimizes better, but I think it will always new/malloc space for the copy, so it\'s harder to test.  Maybe something I did differs from a normal use-case, and maybe normally stopped g++ can hoist the locale setup stuff out of the per-character loop.  My loop reading from a std::string and writing to a char dstbuf[4096] makes sense for testing.loop calling glibc toupper: 6.67s  (not checking the int result for potential multi-byte UTF-8, though.  This matters for Turkish.)See also this question about toupper() being slow on Windows when a locale is set.I was shocked that Boost is an order of magnitude slower than the other options.  I double-checked that I had -O3 enabled, and even single-stepped the asm to see what it was doing.  It\'s almost exactly the same speed with clang++ 3.8.  It has huge overhead inside the per-character loop.  The perf record / report result (for the cycles perf event) is:Gcc and clang will only auto-vectorize loops when the iteration count is known ahead of the loop.  (i.e. search loops like plain-C implementation of strlen won\'t autovectorize.)Thus, for strings small enough to fit in cache, we get a significant speedup for strings ~128 chars long from doing strlen first.  This won\'t be necessary for explicit-length strings (like C++ std::string).Any decent libc will have an efficient strlen that\'s much faster than looping a byte at a time, so separate vectorized strlen and toupper loops are faster.Baseline: a loop that checks for a terminating 0 on the fly.Times for 40M iterations, on a Core2 (Merom) 2.4GHz.  gcc 5.2 -O3 -march=native.  (Ubuntu 15.10).  dst != src (so we make a copy), but they don\'t overlap (and aren\'t nearby).  Both are aligned.Some results are a bit different with clang.The microbenchmark loop that calls the function is in a separate file.   Otherwise it inlines and strlen() gets hoisted out of the loop, and it runs dramatically faster, esp. for 16 char strings (0.187s).This has the major advantage that gcc can auto-vectorize it for any architecture, but the major disadvantage that it\'s slower for the usually-common case of small strings.So there are big speedups, but compiler auto-vectorization doesn\'t make great code, esp. for cleanup of the last up-to-15 characters.Based on my case-flip function that inverts the case of every alphabetic character.  It takes advantage of the "unsigned compare trick", where you can do low < a && a <= high with a single unsigned comparison by range shifting, so that any value less than low wraps to a value that\'s greater than high.  (This works if low and high aren\'t too far apart.)  SSE only has a signed compare-greater, but we can still use the "unsigned\ncompare" trick by range-shifting to the bottom of the signed range: Subtract \'a\'+128, so the alphabetic characters range from -128 to -128+25 (-128+\'z\'-\'a\')Note that adding 128 and subtracting 128 are the same thing for 8bit integers.  There\'s nowhere for the carry to go, so it\'s just xor (carryless add), flipping the high bit.Given this function that works for one vector, we can call it in a loop to process a whole string.  Since we\'re already targeting SSE2, we can do a vectorized end-of-string check at the same time.We can also do much better for the "cleanup" of the last up-to-15 bytes left over after doing vectors of 16B:  upper-casing is idempotent, so re-processing some input bytes is fine.  We do an unaligned load of the last 16B of the source, and store it into the dest buffer overlapping the last 16B store from the loop.The only time this doesn\'t work is when the whole string is under 16B: Even when dst=src, non-atomic read-modify-write is not the same thing as not touching some bytes at all, and can break multithreaded code.We have a scalar loop for that, and also to get src aligned.  Since we don\'t know where the terminating 0 will be, an unaligned load from src might cross into the next page and segfault.  If we need any bytes in an aligned 16B chunk, it\'s always safe to load the whole aligned 16B chunk.Full source: in a github gist.Times for 40M iterations, on a Core2 (Merom) 2.4GHz.  gcc 5.2 -O3 -march=native.  (Ubuntu 15.10).  dst != src (so we make a copy), but they don\'t overlap (and aren\'t nearby).  Both are aligned.(Actually timed with _mm_store in the loop, not _mm_storeu, because storeu is slower on Merom even when the address is aligned.  It\'s fine on Nehalem and later.  I\'ve also left the code as-is for now, instead of fixing the failure to copy the terminating 0 in some cases, because I don\'t want to re-time everything.)So for short strings longer than 16B, this is dramatically faster than auto-vectorized.  Lengths one-less-than-a-vector-width don\'t present a problem.  They might be a problem when operating in-place, because of a store-forwarding stall.  (But note that it\'s still fine to process our own output, rather than the original input, because toupper is idempotent).There\'s a lot of scope for tuning this for different use-cases, depending on what the surrounding code wants, and the target microarchitecture.  Getting the compiler to emit nice code for the cleanup portion is tricky.  Using ffs(3) (which compiles to bsf or tzcnt on x86) seems to be good, but obviously that bit needs a re-think since I noticed a bug after writing up most of this answer (see the FIXME comments).Vector speedups for even smaller strings can be obtained with movq or movd loads/stores.  Customize as necessary for your use-case.We can detect when our vector has any bytes with the high bit set, and in that case fall back to a scalar utf-8-aware loop for that vector.  The dst point can advance by a different amount than the src pointer, but once we get back to an aligned src pointer, we\'ll still just do unaligned vector stores to dst.For text that\'s UTF-8, but mostly consists of the ASCII subset of UTF-8, this can be good: high performance in the common case with correct behaviour in all cases.  When there\'s a lot of non-ASCII, it will probably be worse than staying in the scalar UTF-8 aware loop all the time, though.  Making English faster at the expense of other languages is not a future-proof decision if the downside is significant.In the Turkish locale (tr_TR), the correct result from toupper(\'i\') is \'\xc4\xb0\' (U0130), not \'I\' (plain ASCII).  See Martin Bonner\'s comments on a question about tolower() being slow on Windows.We can also check for an exception-list and fallback to scalar there, like for multi-byte UTF8 input characters.With this much complexity, SSE4.2 PCMPISTRM or something might be able to do a lot of our checks in one go.Use a lambda.The following works for me.This will perform better than all the answers that use the global toupper function, and is presumably what boost::to_upper is doing underneath.This is because ::toupper has to look up the locale - because it might\'ve been changed by a different thread - for every invocation, whereas here only the call to locale() has this penalty. And looking up the locale generally involves taking a lock.This also works with C++98 after you replace the auto, use of the new non-const str.data(), and add a space to break the template closing (">>" to "> >") like this:try the toupper() function (#include <ctype.h>). it accepts characters as arguments, strings are made up of characters, so you\'ll have to iterate over each individual character that when put together comprise the stringHere is the latest code with C++11not sure there is a built in function.  Try this:Include either the ctype.h OR cctype libraries, as well as the stdlib.h as part of the preprocessor directives. ALL of these solutions on this page are harder than they need to be. Do thisRegName is your string. \nGet your string size don\'t use string.size() as your actual tester, very messy and \ncan cause issues.\nthen. the most basic for loop.  remember string size returns the delimiter too so use < and not <= in your loop test.output will be:\nsome string that you want convertedWithout using any libraries:If you are only concerned with 8 bit characters (which all other answers except Milan Babu\xc5\xa1kov assume as well) you can get the fastest speed by generating a look-up table at compile time using metaprogramming. On ideone.com this runs 7x faster than the library function and 3x faster than a hand written version (http://ideone.com/sb1Rup). It is also customizeable through traits with no slow down. with use case:For an in depth (many page) decription of how it works allow me to shamelessly plug my blog: http://metaporky.blogspot.de/2014/07/part-4-generating-look-up-tables-at.htmlI use this solution.   I know you\'re not supposed to modify that data area.... but I think that\'s mostly for buffer overrun bugs and null character.... upper casing things isn\'t the same.In all the machines I tested, it was faster. Perhaps because he is not concerned with a very wide range of characters. Or because using switch() it makes a jump table, do not know how it works in the assembly ... just know that is faster :P