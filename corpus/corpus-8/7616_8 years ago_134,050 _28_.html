What is the difference between Big-O notation O(n) and Little-O notation o(n)?f \xe2\x88\x88 O(g) says, essentiallyFor at least one choice of a constant k > 0, you can find a constant a such that the inequality 0 <= f(x) <= k g(x) holds for all x > a. Note that O(g) is the set of all functions for which this condition holds.f \xe2\x88\x88 o(g) says, essentiallyFor every choice of a constant k > 0, you can find a constant a such that the inequality 0 <= f(x) < k g(x) holds for all x > a.Once again, note that o(g) is a set.In Big-O, it is only necessary that you find a particular multiplier k for which the inequality holds beyond some minimum x. In Little-o, it must be that there is a minimum x after which the inequality holds no matter how small you make k, as long as it is not negative or zero.These both describe upper bounds, although somewhat counter-intuitively, Little-o is the stronger statement. There is a much larger gap between the growth rates of f and g if f \xe2\x88\x88 o(g) than if f \xe2\x88\x88 O(g). One illustration of the disparity is this: f \xe2\x88\x88 O(f) is true, but f \xe2\x88\x88 o(f) is false. Therefore, Big-O can be read as "f \xe2\x88\x88 O(g) means that f\'s asymptotic growth is no faster than g\'s", whereas "f \xe2\x88\x88 o(g) means that f\'s asymptotic growth is strictly slower than g\'s". It\'s like <= versus <.More specifically, if the value of g(x) is a constant multiple of the value of f(x), then f \xe2\x88\x88 O(g) is true. This is why you can drop constants when working with big-O notation.However, for f \xe2\x88\x88 o(g) to be true, then g must include a higher power of x in its formula, and so the relative separation between f(x) and g(x) must actually get larger as x gets larger.To use purely math examples (rather than referring to algorithms):The following are true for Big-O, but would not be true if you used little-o:The following are true for little-o:Note that if f \xe2\x88\x88 o(g), this implies f \xe2\x88\x88 O(g). e.g. x^2 \xe2\x88\x88 o(x^3) so it is also true that x^2 \xe2\x88\x88 O(x^3), (again, think of O as <= and o as <)Big-O is to little-o as \xe2\x89\xa4 is to <. Big-O is an inclusive upper bound, while little-o is a strict upper bound.For example, the function f(n) = 3n is:Analogously, the number 1 is:Here\'s a table, showing the general idea:(Note: the table is a good guide but its limit definition should be in terms of the superior limit instead of the normal limit. For example, 3 + (n mod 2)  oscillates between 3 and 4 forever. It\'s in O(1) despite not having a normal limit, because it still has a lim sup: 4.)I recommend memorizing how the Big-O notation converts to asymptotic comparisons. The comparisons are easier to remember, but less flexible because you can\'t say things like n^O(1) = P.I find that when I can\'t conceptually grasp something, thinking about why one would use X is helpful to understand X. (Not to say you haven\'t tried that, I\'m just setting the stage.)[stuff you know]A common way to classify algorithms is by runtime, and by citing the big-Oh complexity of an algorithm, you can get a pretty good estimation of which one is "better" -- whichever has the "smallest" function in the O! Even in the real world, O(N) is "better" than O(N^2), barring silly things like super-massive constants and the like.[/stuff you know]Let\'s say there\'s some algorithm that runs in O(N). Pretty good, huh? But let\'s say you (you brilliant person, you) come up with an algorithm that runs in O(N/loglogloglogN). YAY! Its faster! But you\'d feel silly writing that over and over again when you\'re writing your thesis. So you write it once, and you can say "In this paper, I have proven that algorithm X, previously computable in time O(N), is in fact computable in o(n)."Thus, everyone knows that your algorithm is faster --- by how much is unclear, but they know its faster. Theoretically. :)