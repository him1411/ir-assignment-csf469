I want a random selection of rows in PostgreSQL, I tried this:But some other recommend this:I have a very large table with 500 Million rows, I want it to be fast.Which approach is better?  What are the differences?  What is the best way to select random rows?Given your specifications (plus additional info in the comments),The query below does not need a sequential scan of the big table, only an index scan.First, get estimates for the main query:The only possibly expensive part is the count(*) (for huge tables). Given above specifications, you don\'t need it. An estimate will do just fine, available at almost no cost (detailed explanation here):As long as ct isn\'t much smaller than id_span, the query will outperform other approaches.    Generate random numbers in the id space. You have "few gaps", so add 10 % (enough to easily cover the blanks) to the number of rows to retrieve.Each id can be picked multiple times by chance (though very unlikely with a big id space), so group the generated numbers (or use DISTINCT).Join the ids to the big table. This should be very fast with the index in place.Finally trim surplus ids that have not been eaten by dupes and gaps. Every row has a completely equal chance to be picked.You can simplify this query. The CTE in the query above is just for educational purposes:Especially if you are not so sure about gaps and estimates.We can work with a smaller surplus in the base query. If there are too many gaps so we don\'t find enough rows in the first iteration, the rCTE continues to iterate with the recursive term. We still need relatively few gaps in the ID space or the recursion may run dry before the limit is reached - or we have to start with a large enough buffer which defies the purpose of optimizing performance.Duplicates are eliminated by the UNION in the rCTE.The outer LIMIT makes the CTE stop as soon as we have enough rows.This query is carefully drafted to use the available index, generate actually random rows and not stop until we fulfill the limit (unless the recursion runs dry). There are a number of pitfalls here if you are going to rewrite it.For repeated use with varying parameters:Call:You could even make this generic to work for any table: Take the name of the PK column and the table as polymorphic type and use EXECUTE ... But that\'s beyond the scope of this question. See:IF your requirements allow identical sets for repeated calls (and we are talking about repeated calls) I would consider a materialized view. Execute above query once and write the result to a table. Users get a quasi random selection at lightening speed. Refresh your random pick at intervals or events of your choosing.It\'s very fast, but the result is not exactly random. The manual:The SYSTEM method is significantly faster than the BERNOULLI method\n  when small sampling percentages are specified, but it may return a\n  less-random sample of the table as a result of clustering effects.And the number of rows returned can vary wildly. For our example, to get roughly 1000 rows, try:Where n is a percentage. The manual:The BERNOULLI and SYSTEM sampling methods each accept a single\n  argument which is the fraction of the table to sample, expressed as a\n  percentage between 0 and 100. This argument can be any real-valued expression.Bold emphasis mine.Related:Or install the additional module tsm_system_rows to get the number of requested rows exactly (if there are enough) and allow for the more convenient syntax:See Evan\'s answer for details.But that\'s still not exactly random.You can examine and compare the execution plan of both by using A quick test on a large table1 shows, that the ORDER BY first sorts the complete table and then picks the first 1000 items. Sorting a large table not only reads that table but also involves reading and writing temporary files. The where random() < 0.1 only scans the complete table once.For large tables this might not what you want as even one complete table scan might take to long.A third proposal would beThis one stops the table scan as soon as 1000 rows have been found and therefore returns sooner. Of course this bogs down the randomness a bit, but perhaps this is good enough in your case.Edit: Besides of this considerations, you might check out the already asked questions for this. Using the query [postgresql] random returns quite a few hits.And a linked article of depez outlining several more approaches:1 "large" as in "the complete table will not fit into the memory".The one with the ORDER BY is going to be the slower one.select * from table where random() < 0.01; goes record by record, and decides to randomly filter it or not. This is going to be O(N) because it only needs to check each record once.select * from table order by random() limit 1000; is going to sort the entire table, then pick the first 1000. Aside of any voodoo magic behind the scenes, the order by is O(N * log N).The downside to the random() < 0.01 one is that you\'ll get a variable number of output records.Note, there is a better way to shuffling a set of data than sorting by random: The Fisher-Yates Shuffle, which runs in O(N). Implementing the shuffle in SQL sounds like quite the challenge, though.Starting with PostgreSQL 9.5, there\'s a new syntax dedicated to getting random elements from a table :This example will give you 5% of elements from mytable.See more explanation on this blog post: http://www.postgresql.org/docs/current/static/sql-select.htmlIf you want just one row, you can use a calculated offset derived from count.If you know how many rows you want, check out tsm_system_rows.module provides the table sampling method SYSTEM_ROWS, which can be used in the TABLESAMPLE clause of a SELECT command.This table sampling method accepts a single integer argument that is the maximum number of rows to read. The resulting sample will always contain exactly that many rows, unless the table does not contain enough rows, in which case the whole table is selected. Like the built-in SYSTEM sampling method, SYSTEM_ROWS performs block-level sampling, so that the sample is not completely random but may be subject to clustering effects, especially if only a small number of rows are requested.First install the extensionThen your query,A variation of the materialized view "Possible alternative" outlined by Erwin Brandstetter is possible.Say, for example, that you don\'t want duplicates in the randomized values that are returned. So you will need to set a boolean value on the primary table containing your (non-randomized) set of values.Assuming this is the input table:Populate the ID_VALUES table as needed. Then, as described by Erwin, create a materialized view that randomizes the ID_VALUES table once:Note that the materialized view does not contain the used column, because this will quickly become out-of-date. Nor does the view need to contain other columns that may be in the id_values table.In order to obtain (and "consume") random values, use an UPDATE-RETURNING on id_values, selecting id_values from id_values_randomized with a join, and applying the desired criteria to obtain only relevant possibilities. For example:Change LIMIT as necessary -- if you only need one random value at a time, change LIMIT to 1.With the proper indexes on id_values, I believe the UPDATE-RETURNING should execute very quickly with little load. It returns randomized values with one database round-trip. The criteria for "eligible" rows can be as complex as required. New rows can be added to the id_values table at any time, and they will become accessible to the application as soon as the materialized view is refreshed (which can likely be run at an off-peak time). Creation and refresh of the materialized view will be slow, but it only needs to be executed when new id\'s are added to the id_values table.After getting kind of caught up in the SQL mode of thought, I realized that, for my use case, the following method should work.For my purpose, I want to select a specified number (about 10-20) of random elements "from the past month" (a subset of maybe 300-400 elements - a number which won\'t increase with increased server activity). Micka\xc3\xabl\'s solution almost fit the bill but it appears that you can\'t use TABLESAMPLE after a WHERE clause.Here\'s the solution I arrived at:Firstly, I ran a simple query: SELECT id FROM table WHERE "timestamp" > now()::Date - 30Once the results were returned to my program, I selected a random sample of the ids. Then I simply ran:SELECT * FROM table WHERE id IN (1,2,3) (where (1,2,3) is my randomly selected sample).I realize this isn\'t a strictly PostgreSQL solution but it\'s nice and simple and, provided you mind the scaling constraints, should work just fine. Hopefully it will be the right fit for someone in a similar position.Here is a decision that works for me. I guess it\'s very simple to understand and execute.Add a column called r with type serial. Index r.Assume we have 200,000 rows, we are going to generate a random number n, where 0 < n <= 200, 000.Select rows with r > n, sort them ASC and select the smallest one.Code:The code is self-explanatory. The subquery in the middle is used to quickly estimate the table row counts from https://stackoverflow.com/a/7945274/1271094 .In application level you need to execute the statement again if n > the number of rows or need to select multiple rows.