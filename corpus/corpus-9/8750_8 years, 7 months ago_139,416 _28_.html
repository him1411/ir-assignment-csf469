I have a very big file 4GB and when I try to read it my computer hangs.\nSo I want to read it piece by piece and after processing each piece store the processed piece into another file and read next piece.Is there any method to yield these pieces ?I would love to have a lazy method.To write a lazy function, just use yield:Another option would be to use iter and a helper function:If the file is line-based, the file object is already a lazy generator of lines:If your computer, OS and python are 64-bit, then you can use the mmap module to map the contents of the file into memory and access it with indices and slices. Here an example from the documentation:If either your computer, OS or python are 32-bit, then mmap-ing large files can reserve large parts of your address space and starve your program of memory.file.readlines() takes in an optional size argument which approximates the number of lines read in the lines returned.Take a look at this post on Neopythonic: "Sorting a million 32-bit integers in 2MB of RAM using Python"UPDATE: The approach is best explained in https://stackoverflow.com/a/4566523/38592There are already many good answers, but I ran into a similar issue recently and the solution I needed is not listed here, so I figured I could complement this thread.80% of the time, I need to read files line by line. Then, as suggested in this answer, you want to use the file object itself as lazy generator:However, I recently ran into a very very big (almost) single line csv, where the row separator was in fact not \'\\n\' but \'|\'.I came up with the following snippet:I have tested it succesfully on large files and with different chunk sizes (I even tried a chunksize of 1 byte, just to make sure the algorithm is not size dependent).I think we can write like this:i am not allowed to comment due to my low reputation, but SilentGhosts solution should be much easier with file.readlines([sizehint])python file methodsedit: SilentGhost is right, but this should be better than:I\'m in a somewhat similar situation. It\'s not clear whether you know chunk size in bytes; I usually don\'t, but the number of records (lines) that is required is known:Update: Thanks nosklo. Here\'s what I meant. It almost works, except that it loses a line \'between\' chunks.Does the trick w/o losing any lines, but it doesn\'t look very nice.To process line by line, this is an elegant solution:As long as there\'re no blank lines.you can use following code.open() returns a file objectthen use os.stat for getting size