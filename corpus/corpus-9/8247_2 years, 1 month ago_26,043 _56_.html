I\'m solving a problem and it involves sorting 10 numbers (int32) very quickly. My application needs to sort 10 numbers millions of times as fast as possible. I\'m sampling a data set of billions of elements and every time I need to pick 10 numbers out of it (simplified) and sort them (and make conclusions from the sorted 10 element list).Currently I\'m using insertion sort but I imagine I could implement a very fast custom sorting algorithm for my specific problem of 10 numbers which would beat insertion sort.Does anyone have any idea about how to approach this problem?(Following up on the suggestion of HelloWorld to look into sorting networks.) It seems that a 29-comparison/swap network is the fastest way to do a 10-input sort. I used the network discovered by Waksman in 1969 for this example in Javascript, which should translate directly into C, as it\'s just a list of if statements, comparisons and swaps.Here\'s a graphical representation of the network, divided into independent phases.\n\nTo take advantage of parallel processing, the 5-4-3-4-4-4-3-2 grouping can be changed into a 4-4-4-4-4-4-3-2 grouping.\n When you deal with this fixed size take a look at Sorting Networks. These algorithms have a fixed runtime and are independent to their input. For your use-case you don\'t have such overhead that some sorting algorithms have.Bitonic sort is an implementation of such network. This one works best with len(n) <= 32 on a CPU. On bigger inputs you could think of moving to a GPU.\nhttps://en.wikipedia.org/wiki/Sorting_networkBtw, a good page to compare sorting algorithms is this one here (though its missing the bitonic sort.http://www.sorting-algorithms.comUse a sorting network that has comparisons in groups of 4, so you can do it in SIMD registers.  A pair of packed min/max instructions implements a packed comparator function.  Sorry I don\'t have time right now to look for a page I remember seeing about this, but hopefully searching on SIMD or SSE sorting networks will turn something up.x86 SSE does have packed-32bit-integer min and max instructions for vectors of four 32bit ints.  AVX2 (Haswell and later) have the same but for 256b vectors of 8 ints.  There are also efficient shuffle instructions.If you have a lot of independent small sorts, it might be possible to do 4 or 8 sorts in parallel using vectors.  Esp. if you\'re choosing elements randomly (so the data to be sorted won\'t be contiguous in memory anyway), you can avoid shuffles and simply compare in the order you need.  10 registers to hold all the data from 4 (AVX2: 8) lists of 10 ints still leaves 6 regs for scratch space.Vector sorting networks are less efficient if you also need to sort associated data.  In that case, the most efficient way seems to be to use a packed-compare to get a mask of which elements changed, and use that mask to blend vectors of (references to) associated data.What about an unrolled, branch-less selection sort?http://coliru.stacked-crooked.com/a/71e18bc4f7fa18c6The only relevant lines are the first two #define.It uses two lists and entirely recheck the first one for ten times which would be a badly implemented selection sort, however it avoids branches and variable length loops, which may compensate with modern processors and such a small data set.I benchmarked against the sorting network, and my code seems to be slower. However I tried to remove the unrolling and the copy. Running this code:I am consistently getting better result for the branch-less selection sort compared to the sorting network.The question doesn\'t say that this is some kind of a web-based application.  The one thing that caught my eye was:I\'m sampling a data set of billions of elements and every time I need to pick 10 numbers out of it (simplified) and sort them (and make conclusions from the sorted 10 element list).As a software and hardware engineer this absolutely screams "FPGA" to me.  I don\'t know what kind of conclusions you need to draw from the sorted set of numbers or where the data comes from but I know it would be almost trivial to process somewhere between one hundred million and a billion of these "sort-and-analyze" operations per second.  I\'ve done FPGA-assisted DNA sequencing work in the past.  It is nearly impossible to beat the massive processing power of FPGA\'s when the problem is well suited for that type of a solution.At some level the only limiting factor becomes how quickly you can shovel data into an FPGA and how quickly you can get it out.As a point of reference, I designed a high performance real-time image processor that received 32 bit RGB image data at a rate of about 300 million pixels per second.  The data streamed through FIR filters, matrix multipliers, lookup tables, spatial edge detection blocks and a number of other operations before coming out the other end.  All of this on a relatively small Xilinx Virtex2 FPGA with internal clocking spanning from about 33MHz to, if I remember correctly, 400MHz.  Oh, yes, it also had a DDR2 controller implementation and ran two banks of DDR2 memory.An FPGA can output a sort of ten 32 bit number on every clock transition while operating at hundreds of MHz.  There would be short delay at the start of the operation as the data fills the processing pipeline/s.  After that you should be able to get one result per clock.  Or more if the processing can be parallelized through replicating the sort-and-analyze pipeline.  The solution, in principle, is almost trivial.The point is:  If the application isn\'t PC-bound and the data stream and processing is "compatible" with an FPGA solution (either stand-alone or as a co-processor card in the machine) there is no way you are going to be able to beat the attainable level of performance with software written in any language, regardless of the algorithm.EDIT:Just ran quick search and found a paper that might be of use to you.  It looks like it dates back to 2012.  You can do a LOT better in performance today (and  even back then).  Here it is:Sorting Networks on FPGAsI recently wrote a little class that uses the Bose-Nelson algorithm to generate a sorting network on compile time. It can be used to create a very fast sort for 10 numbers.    Note that instead of an if (compare) swap statement, we explicitly code out ternary operators for min and max. This is to help nudge the compiler into using branchless code.The following benchmarks are compiled with clang -O3 and ran on my mid-2012 macbook air.Comparing it with DarioP\'s code, here are the number of milliseconds taken to sort 1 million 32-bit int arrays of size 10:  Hardcoded Sort Net 10         : 88.774 ms\nTemplated Bose-Nelson sort 10 : 27.815 msUsing this templated approach, we can also generate sorting networks upon compile time for other number of elements. Time (in milliseconds) to sort 1 million arrays of various sizes.\nThe number of milliseconds for arrays of size 2, 4, 8 are 1.943, 8.655, 20.246 respectively.\nCredits to Glenn Teitelbaum for the unrolled insertion sort.Here are the average clocks per sort for small arrays of 6 elements. The benchmark code and examples can be found at this question:\nFastest sort of fixed length 6 int array It performs as fast as the fastest example in the question for 6 elements. Often, the input arrays may be already sorted or mostly sorted.\nIn such cases, insertion sort can be better choice.You may want to choose an appropriate sorting algorithm depending on the data.The code used for the benchmarks can be found here.Although a network sort has good odds of being fast on small arrays, sometimes you can\'t beat insertion sort if properly optimized. For example batch insert with 2 elements:You can fully unroll insertion sortTo make that easier, recursive templates can be used with no function overhead.  Since it already is a template, int can be a template parameter as well.  This also makes coding array sizes other than 10 trivial to create.Note that to sort int x[10] the call is insert_sort<int, 9>::sort(x); since the class uses the index of the last item.   This could be wrapped, but that would be more code to read through. In my testing this was faster than the sorting network examples.An insertion sort requires on average 29,6 comparisons to sort 10 inputs with a best case of 9 and a worst of 45 (given input that is in reverse order).A {9,6,1} shellsort will require on average 25.5 comparisons to sort 10 inputs. Best case is 14 comparisons, worst is 34 and sorting a reversed input requires 22.Since you seem to be familiar with insertion sort you can implement the algorithm as a sorting network for {9,6} and then tack on the insertion sort ({1}) after that:For reasons similar to those that I described here, the following sorting functions, sort6_iterator() and sort10_iterator_local(), should perform well, where the sorting network was taken from here:To call this function I passed it a std::vector iterator. 