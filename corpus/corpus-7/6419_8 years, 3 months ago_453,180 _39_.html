In Python, how do I read in a binary file and loop over each byte of that file?By suggestion of chrispy:Note that the with statement is not available in versions of Python below 2.5. To use it in v 2.5 you\'ll need to import it:In 2.6 this is not needed.In Python 3, it\'s a bit different. We will no longer get raw characters from the stream in byte mode but byte objects, thus we need to alter the condition:Or as benhoyt says, skip the not equal and take advantage of the fact that b"" evaluates to false. This makes the code compatible between 2.6 and 3.x without any changes. It would also save you from changing the condition if you go from byte mode to text or the reverse.This generator yields bytes from a file, reading the file in chunks:See the Python documentation for information on iterators and generators.If the file is not too big that holding it in memory is a problem:where process_byte represents some operation you want to perform on the passed-in byte.If you want to process a chunk at a time:To read a file \xe2\x80\x94 one byte at a time (ignoring the buffering)  \xe2\x80\x94 you could use the two-argument iter(callable, sentinel) built-in function:It calls file.read(1) until it returns nothing b\'\' (empty bytestring).  The memory doesn\'t grow unlimited for large files. You could pass buffering=0  to open(), to disable the buffering \xe2\x80\x94 it guarantees that only one byte is read per iteration (slow).with-statement closes the file automatically \xe2\x80\x94 including the case when the code underneath raises an exception.Despite the presence of internal buffering by default, it is still inefficient to process one byte at a time. For example, here\'s the blackhole.py utility that eats everything it is given:Example:It processes ~1.5 GB/s when chunksize == 32768 on my machine and only ~7.5 MB/s when chunksize == 1. That is, it is 200 times slower to read one byte at a time. Take it into account if you can rewrite your processing to use more than one byte at a time and if you need performance.mmap allows you to treat a file as a bytearray and a file object simultaneously. It can serve as an alternative to loading the whole file in memory if you need access both interfaces. In particular, you can iterate one byte at a time over a memory-mapped file just using a plain for-loop:mmap supports the slice notation. For example, mm[i:i+len] returns len bytes from the file starting at position i. The context manager protocol is not supported before Python 3.2; you need to call mm.close() explicitly in this case. Iterating over each byte using mmap consumes more memory than file.read(1), but mmap is an order of magnitude faster.To sum up all the brilliant points of chrispy, Skurmedel, Ben Hoyt and Peter Hansen, this would be the optimal solution for processing a binary file one byte at a time:For python versions 2.6 and above, because:Or use J. F. Sebastians solution for improved speedOr if you want it as a generator function like demonstrated by codeape:Let\'s create a function to do this:Let\'s make a file:Now let\'s iterate over it, using the rb flag (read mode, bytes mode). Note that the multiple for loops do not increase the complexity (which remains O(n)) - this is just how you lazily iterate over a file - line by line.This will loop over each byte in the code, without any hacky .read(1) business. This is far more Pythonic and natural than the while loop and complicatedness I\'ve seen in the other answers here.If you have large files with no newlines, you may want to buffer your reading. Python 2.7 requires io.open to get this:And we now have a buffered reader:Python 3\'s builtin open function is 2\'s io.open.Python 3, read all of the file at once:You can iterate whatever you want using data variable.If you have a lot of binary data to read, you might want to consider the struct module.  It is documented as converting "between C and Python types", but of course, bytes are bytes, and whether those were created as C types does not matter.  For example, if your binary data contains two 2-byte integers and one 4-byte integer, you can read them as follows (example taken from struct documentation):You might find this more convenient, faster, or both, than explicitly looping over the content of a file.my solution actually returns a string:the only problem with this is if you\'re managing an external offset, you\'ll need to explicitly add 1 to make up for the termination character.EDIT:\ntypically though I\'ll do something like array(\'B\',[ord(c) for c in file.read()]) and do the data management manually from there.\n(this is the fastest way to handle file data, file.read(1) is many times slower)