Are lexers and parsers really that different in theory? It seems fashionable to hate regular expressions: coding horror, another blog post. However, popular lexing based tools: pygments, geshi, or prettify, all use regular expressions.  They seem to lex anything... When is lexing enough, when do you need EBNF? Has anyone used the tokens produced by these lexers with bison or antlr parser generators?What parsers and lexers have in common:As you can see, parsers and tokenizers have much in common. One parser can be a tokenizer for other parser, which reads its input tokens as symbols from its own alphabet (tokens are simply symbols of some alphabet) in the same way as sentences from one language can be alphabetic symbols of some other, higher-level language. For example, if * and - are the symbols of the alphabet M (as "Morse code symbols"), then you can build a parser which recognizes strings of these dots and lines as letters encoded in the Morse code. The sentences in the language "Morse Code" could be tokens for some other parser, for which these tokens are atomic symbols of its language (e.g. "English Words" language). And these "English Words" could be tokens (symbols of the alphabet) for some higher-level parser which understands "English Sentences" language. And all these languages differ only in the complexity of the grammar. Nothing more.So what\'s all about these "Chomsky\'s grammar levels"? Well, Noam Chomsky classified grammars into four levels depending on their complexity:Yes, they are very different in theory, and in implementation.Lexers are used to recognize "words" that make up language elements, because the structure of such words is generally simple.   Regular expressions are extremely good at handling this simpler structure, and there are very high-performance regular-expression matching engines used to implement lexers.Parsers are used to recognize "structure" of a language phrases.  Such structure is generally far beyond what "regular expressions" can recognize, so one needs \n"context sensitive" parsers to extract such structure.   Context-sensitive parsers\nare hard to build, so the engineering compromise is to use "context-free" grammars\nand add hacks to the parsers ("symbol tables", etc.) to handle the context-sensitive part.Neither lexing nor parsing technology is likely to go away soon.They may be unified by deciding to use "parsing" technology to recognize "words", as is currently explored by so-called scannerless GLR parsers.   That has a runtime cost, as you are applying more general machinery to what is often a problem that doesn\'t need it, and usually you pay for that in overhead.   Where you have lots of free cycles, that overhead may not matter.  If you process a lot of text, then the overhead does matter and classical regular expression parsers will continue to be used.When is lexing enough, when do you need EBNF?EBNF really doesn\'t add much to the power of grammars. It\'s just a convenience / shortcut notation / "syntactic sugar" over the standard Chomsky\'s Normal Form (CNF) grammar rules. For example, the EBNF alternative:you can achieve in CNF by just listing each alternative production separately:The optional element from EBNF:you can achieve in CNF by using a nullable production, that is, the one which can be replaced by an empty string (denoted by just empty production here; others use epsilon or lambda or crossed circle):A production in a form like the last one B above is called "erasure", because it can erase whatever it stands for in other productions (product an empty string instead of something else).Zero-or-more repetiton from EBNF:you can obtan by using recursive production, that is, one which embeds itself somewhere in it. It can be done in two ways. First one is left recursion (which usually should be avoided, because Top-Down Recursive Descent parsers cannot parse it):Knowing that it generates just an empty string (ultimately) followed by zero or more As, the same string (but not the same language!) can be expressed using right-recursion:And when it comes to + for one-or-more repetition from EBNF:it can be done by factoring out one A and using * as before:which you can express in CNF as such (I use right recursion here; try to figure out the other one yourself as an exercise):Knowing that, you can now probably recognize a grammar for a regular expression (that is, regular grammar) as one which can be expressed in a single EBNF production consisting only from terminal symbols. More generally, you can recognize regular grammars when you see productions similar to these:That is, using only empty strings, terminal symbols, simple non-terminals for substitutions and state changes, and using recursion only to achieve repetition (iteration, which is just linear recursion - the one which doesn\'t branch tree-like). Nothing more advanced above these, then you\'re sure it\'s a regular syntax and you can go with just lexer for that.But when your syntax uses recursion in a non-trivial way, to produce tree-like, self-similar, nested structures, like the following one:then you can easily see that this cannot be done with regular expression, because you cannot resolve it into one single EBNF production in any way; you\'ll end up with substituting for S indefinitely, which will always add another as and bs on both sides. Lexers (more specifically: Finite State Automata used by lexers) cannot count to arbitrary number (they are finite, remember?), so they don\'t know how many as were there to match them evenly with so many bs. Grammars like this are called context-free grammars (at the very least), and they require a parser.Context-free grammars are well-known to parse, so they are widely used for describing programming languages\' syntax. But there\'s more. Sometimes a more general grammar is needed -- when you have more things to count at the same time, independently. For example, when you want to describe a language where one can use round parentheses and square braces interleaved, but they have to be paired up correctly with each other (braces with braces, round with round). This kind of grammar is called context-sensitive. You can recognize it by that it has more than one symbol on the left (before the arrow). For example:You can think of these additional symbols on the left as a "context" for applying the rule. There could be some preconditions, postconditions etc. For example, the above rule will substitute R into S, but only when it\'s in between A and B, leaving those A and B themselves unchanged. This kind of syntax is really hard to parse, because it needs a full-blown Turing machine. It\'s a whole another story, so I\'ll end here.To answer the question as asked (without repeating unduly what appears in\nother answers)Lexers and parsers are not very different, as suggested by the\naccepted answer. Both are based on simple language formalisms: regular\nlanguages for lexers and, almost always, context-free (CF) languages\nfor parsers. They both are associated with fairly simple computational\nmodels, the finite state automaton and the push-down stack automaton.\nRegular languages are a special case of context-free languages, so\nthat lexers could be produced with the somewhat more complex CF\ntechnology. But it is not a good idea for at least two reasons.A fundamental point in programming is that a system component should\nbe buit with the most appropriate technology, so that it is easy to\nproduce, to understand and to maintain. The technology should not be\noverkill (using techniques much more complex and costly than needed),\nnor should it be at the limit of its power, thus requiring technical\ncontortions to achieve the desired goal.That is why "It seems fashionable to hate regular expressions".\nThough they can do a lot, they sometimes require very unreadable\ncoding to achieve it, not to mention the fact that various extensions\nand restrictions in implementation somewhat reduce their theoretical\nsimplicity. Lexers do not usually do that, and are usually a simple,\nefficient, and appropriate technology to parse token. Using CF parsers\nfor token would be overkill, though it is possible.Another reason not to use CF formalism for lexers is that it might\nthen be tempting to use the full CF power. But that might raise\nsructural problems regarding the reading of programs.Fundamentally, most of the structure of program text, from which\nmeaning is extracted, is a tree structure. It expresses how the parse\nsentence (program) is generated from syntax rules. Semantics is\nderived by compositional techniques (homomorphism for the\nmathematically oriented) from the way syntax rules are composed to\nbuild the parse tree. Hence the tree structure is essential.\nThe fact that tokens are identified with a regular set based lexer\ndoes not change the situation, because CF composed with regular still\ngives CF (I am speaking very loosely about regular transducers, that\ntransform a stream of characters into a stream of token).However, CF composed with CF (via CF transducers ... sorry for the\nmath), does not necessarily give CF, and might makes things more\ngeneral, but less tractable in practice. So CF is not the appropriate\ntool for lexers, even though it can be used.One of the major differences between regular and CF is that regular\nlanguages (and transducers) compose very well with almost any\nformalism in various ways, while CF languages (and transducers) do\nnot, not even with themselves (with a few exceptions).(Note that regular transducers may have others uses, such as\nformalization of some syntax error handling techniques.)BNF is just a specific syntax for presenting CF grammars.EBNF is a syntactic sugar for BNF, using the facilities of regular\nnotation to give terser version of BNF grammars. It can always be\ntransformed into an equivalent pure BNF.However, the regular notation is often used in EBNF only to emphasize these\nparts of the syntax that correspond to the structure of lexical\nelements, and should be recognized with the lexer, while the rest with\nbe rather presented in straight BNF. But it is not an absolute rule.To summarize, the simpler structure of token is better analyzed with\nthe simpler technology of regular languages, while the tree oriented\nstructure of the language (of program syntax) is better handled by CF\ngrammars.I would suggest also looking at AHR\'s answer.But this leaves a question open: Why trees?Trees are a  good basis for specifying syntax becausethey give a simple structure to the textthere are very convenient for associating semantics with the text\non the basis of that structure, with a mathematically well\nunderstood technology (compositionality via homomorphisms), as\nindicated above. It is a fundamental algebraic tool to define the\nsemantics of mathematical formalisms.Hence it is a good intermediate representation, as shown by the\nsuccess of Abstract Syntax Trees (AST).  Note that AST are often\ndifferent from parse tree because the parsing technology used by many\nprofessionals (Such as LL or LR) applies only to a subset of CF\ngrammars, thus forcing grammatical distorsions which are later\ncorrected in AST. This can be avoided with more general parsing\ntechnology (based on dynamic programming) that accepts any CF grammar.Statement about the fact that programming languages are\ncontext-sensitive (CS) rather than CF are arbitrary and disputable.The problem is that the separation of syntax and semantics is\narbitrary. Checking declarations or type agreement may be seen as\neither part of syntax, or part of semantics. The same would be true of\ngender and number agreement in natural languages. But there are natural\nlanguages where plural agreement depends on the actual semantic\nmeaning of words, so that it does not fit well with syntax.Many definitions of programming languages in denotational semantics\nplace declarations and type checking in the semantics. So stating as\ndone by Ira Baxter that CF parsers are being hacked to get a context\nsensitivity required by syntax is at best an arbitrary view of the\nsituation. It may be organized as a hack in some compilers, but it\ndoes not have to be.Also it is not just that CS parsers (in the sense used in other answers here) are hard to build, and less\nefficient. They are are also inadequate to express perspicuously the\nkinf of context-sensitivity that might be needed. And they do not\nnaturally produce a syntactic structure (such as parse-trees) that\nis convenient to derive the semantics of the program, i.e. to generate\nthe compiled code.There are  a  number  of reasons why the analysis portion  of a compiler is  normally \nseparated into  lexical  analysis and parsing  ( syntax analysis)  phases.resource___Compilers (2nd Edition)\nwritten by-\nAlfred V.  Abo \nColumbia  University \nMonica  S.  Lam \nStanford  University \nRavi  Sethi \nAvaya \nJeffrey  D.  Ullman \nStanford  University