What is the difference between concurrent programming and parallel programing? I asked google but didn\'t find anything that helped me to understand that difference. Could you give me an example for both?For now I found this explanation: http://www.linux-mag.com/id/7411 - but "concurrency is a property of the program" vs "parallel execution is a property of the machine" isn\'t enough for me - still I can\'t say what is what.If you program using threads (concurrent programming), it\'s not necessarily going to be executed as such (parallel execution), since it depends on whether the machine can handle several threads.Here\'s a visual example. Threads on a non-threaded machine:Threads on a threaded machine:The dashes represent executed code. As you can see, they both split up and execute separately, but the threaded machine can execute several separate pieces at once.Concurrent programming regards operations that appear to overlap and is primarily concerned with the complexity that arises due to non-deterministic control flow. The quantitative costs associated with concurrent programs are typically both throughput and latency. Concurrent programs are often IO bound but not always, e.g. concurrent garbage collectors are entirely on-CPU. The pedagogical example of a concurrent program is a web crawler. This program initiates requests for web pages and accepts the responses concurrently as the results of the downloads become available, accumulating a set of pages that have already been visited. Control flow is non-deterministic because the responses are not necessarily received in the same order each time the program is run. This characteristic can make it very hard to debug concurrent programs. Some applications are fundamentally concurrent, e.g. web servers must handle client connections concurrently. Erlang is perhaps the most promising upcoming language for highly concurrent programming.Parallel programming concerns operations that are overlapped for the specific goal of improving throughput. The difficulties of concurrent programming are evaded by making control flow deterministic. Typically, programs spawn sets of child tasks that run in parallel and the parent task only continues once every subtask has finished. This makes parallel programs much easier to debug. The hard part of parallel programming is performance optimization with respect to issues such as granularity and communication. The latter is still an issue in the context of multicores because there is a considerable cost associated with transferring data from one cache to another. Dense matrix-matrix multiply is a pedagogical example of parallel programming and it can be solved efficiently by using Straasen\'s divide-and-conquer algorithm and attacking the sub-problems in parallel. Cilk is perhaps the most promising language for high-performance parallel programming on shared-memory computers (including multicores).https://joearms.github.io/2013/04/05/concurrent-and-parallel-programming.htmlConcurrent = Two queues and one coffee machine.Parallel = Two queues and two coffee machines.I believe concurrent programming refers to multithreaded programming which is about letting your program run multiple threads, abstarcted from hardware details.Parallel programming refers to specifically designing your program algorithms to take advantage of available parallel execution. For example, you can execute in parallel two branches of some algorithms in expectation that it will hit the result sooner (on average) than it would if you first checked the first then the second branch.Interpreting the original question as parallel/concurrent computation instead of programming. In concurrent computation two computations both advance independently of each other. The second computation doesn\'t have to wait until the first is finished for it to advance. It doesn\'t state however, the mechanism how this is achieved. In single-core setup, suspending and alternating between threads is required (also called pre-emptive multithreading).In parallel computation two computations both advance simultaneously - that is literally at the same time. This is not possible with single CPU and requires multi-core setup instead. versus According to: "Parallel vs Concurrent in Node.js".I found this content in some blog. Thought it is useful and  relevant. Concurrency and parallelism are NOT the same thing. Two tasks T1 and T2 are concurrent if the order in which the two tasks are executed in time is not predetermined,T1 may be executed and finished before T2,\nT2 may be executed and finished before T1,\nT1 and T2 may be executed simultaneously at the same instance of time (parallelism),\nT1 and T2 may be executed alternatively,\n...\nIf two concurrent threads are scheduled by the OS to run on one single-core non-SMT non-CMP processor, you may get concurrency but not parallelism. Parallelism is possible on multi-core, multi-processor or distributed systems.Concurrency is often referred to as a property of a program, and is a concept more general than parallelism.Source: https://blogs.oracle.com/yuanlin/entry/concurrency_vs_parallelism_concurrent_programmingIn view of from processor, It can be described by this pic In view of from processor, It can be described by this pic Classic scheduling of tasks can be SERIAL, PARALLEL or CONCURRENTSERIAL:\nAnalysis shows that tasks MUST BE executed one after the other in a known sequence tricked order OR it will not work. I.e.: Easy enough, we can live with thisPARALLEL: Analysis shows that tasks MUST BE executed at the same time OR it will not work.I.e.: Try to avoid this or we will have tears by tea time.CONCURRENT.  Analysis shows that we NEED NOT CARE. We are not careless, we have analysed it and it does not matter; we can therefore execute any task using any available facility at any time.I.e.: HAPPY DAYSOften the scheduling available changes at known events which I called a state change.People often think this is about software but it is in fact a systems design concept that pre-dates computersSoftware systems were a little slow in the uptake, very few software languages even attempt to address the problem.You might try looking up the TRANSPUTER language occam if you are interested in a good try.( occam has many principally innovative ( if not second to none ) features, incl. explicit language support for PAR and SER code-parts execution constructors that other languages principally suffer from having in the forthcomming era of Massive Parallel Processor Arrays available in recent years, re-inventing the wheel InMOS Transputers used more than 35 years ago (!!!) )Succinctly, systems design addresses the following:THE VERB - What are you doing. ( operation or algorithm )THE NOUN - What are you doing it to. ( Data or interface )WHEN - Initiation, schedule, state changes, SERIAL, PARALLEL, CONCURRENTWHERE - Once you know when things happen then you can say where they can happen and not before.WHY -   Is this a way to do it? Is there any other ways? Is there a best way?.. and last but not least .. WHAT HAPPENS IF YOU DO NOT DO IT ?Recent Parallel architectures available in 2014 in action on arrays of 16-, 64-, 1024- parallel RISC uP-sQuarter of century back - a part of the true parallel history with Inmos Transputer CPU demo video from the early 1990sGood luckparallel programming happens when code is being executed at the same time and each execution is independent of the other. Therefore there is usually not a preocupation about shared variables and such because that won\'t likelly happen.However concurrent programming consists on code being executed by diferent processes/threads that share variables and such, therefore on concurrent programming we must establish some sort of rule to decide wich process/thread executes first, we want this so that we can be sure there will be consistency and that we can know with certainty what will happen. If there is no control and all threads compute at the same times and store things on the same variables how would we know what to expect in the end? Maybe a thread is faster than the other, maybe one of the threads even stopped in the middle of it\'s execution and another continued a different computation with a corrupted(not yet fully computed) variable, the possibilities are endless. It\'s in these situations that we usually use concurrent programming instead of parallel.In programming, concurrency is the composition of independently\n  executing processes, while parallelism is the simultaneous execution\n  of (possibly related) computations. \n  - Andrew Gerrand -And Concurrency is the composition of independently executing\n  computations. Concurrency is a way to structure software, particularly\n  as a way to write clean code that interacts well with the real world.\n  It is not parallelism.Concurrency is not parallelism, although it enables parallelism. If\n  you have only one processor, your program can still be concurrent but\n  it cannot be parallel. On the other hand, a well-written concurrent\n  program might run efficiently in parallel on a multiprocessor. That\n  property could be important... \n  - Rob Pike -To understand the difference, I strongly recommend to see this Rob Pike(one of Golang creators)\'s video. \'Concurrency Is Not Parallelism\'They\'re two phrases that describe the same thing from (very slightly) different viewpoints. Parallel programming is describing the situation from the viewpoint of the hardware -- there are at least two processors (possibly within a single physical package) working on a problem in parallel. Concurrent programming is describing things more from the viewpoint of the software -- two or more actions may happen at exactly the same time (concurrently).The problem here is that people are trying to use the two phrases to draw a clear distinction when none really exists. The reality is that the dividing line they\'re trying to draw has been fuzzy and indistinct for decades, and has grown ever more indistinct over time.What they\'re trying to discuss is the fact that once upon a time, most computers had only a single CPU. When you executed multiple processes (or threads) on that single CPU, the CPU was only really executing one instruction from one of those threads at a time. The appearance of concurrency was an illusion--the CPU switching between executing instructions from different threads quickly enough that to human perception (to which anything less than 100 ms or so looks instantaneous) it looked like it was doing many things at once.The obvious contrast to this is a computer with multiple CPUs, or a CPU with multiple cores, so the machine is executing instructions from multiple threads and/or processes at exactly the same time; code executing one can\'t/doesn\'t have any effect on code executing in the other.Now the problem: such a clean distinction has almost never existed. Computer designers are actually fairly intelligent, so they noticed a long time ago that (for example) when you needed to read some data from an I/O device such as a disk, it took a long time (in terms of CPU cycles) to finish. Instead of leaving the CPU idle while that happened, they figured out various ways of letting one process/thread make an I/O request, and let code from some other process/thread execute on the CPU while the I/O request completed.So, long before multi-core CPUs became the norm, we had operations from multiple threads happening in parallel.That\'s only the tip of the iceberg though. Decades ago, computers started providing another level of parallelism as well. Again, being fairly intelligent people, computer designers noticed that in a lot of cases, they had instructions that didn\'t affect each other, so it was possible to execute more than one instruction from the same stream at the same time. One early example that became pretty well known was the Control Data 6600. This was (by a fairly wide margin) the fastest computer on earth when it was introduced in 1964--and much of the same basic architecture remains in use today. It tracked the resources used by each instruction, and had a set of execution units that executed instructions as soon as the resources on which they depended became available, very similar to the design of most recent Intel/AMD processors.But (as the commercials used to say) wait--that\'s not all. There\'s yet another design element to add still further confusion. It\'s been given quite a few different names (e.g., "Hyperthreading", "SMT", "CMP"), but they all refer to the same basic idea: a CPU that can execute multiple threads simultaneously, using a combination of some resources that are independent for each thread, and some resources that are shared between the threads. In a typical case this is combined with the instruction-level parallelism outlined above. To do that, we have two (or more) sets of architectural registers. Then we have a set of execution units that can execute instructions as soon as the necessary resources become available. These often combine well because the instructions from the separate streams virtually never depend on the same resources.Then, of course, we get to modern systems with multiple cores. Here things are obvious, right? We have N (somewhere between 2 and 256 or so, at the moment) separate cores, that can all execute instructions at the same time, so we have  clear-cut case of real parallelism--executing instructions in one process/thread doesn\'t affect executing instructions in another.Well, sort of. Even here we have some independent resources (registers, execution units, at least one level of cache) and some shared resources (typically at least the lowest level of cache, and definitely the memory controllers and bandwidth to memory).To summarize: the simple scenarios people like to contrast between shared resources and independent resources virtually never happen in real life. With all resources shared, we end up with something like MS-DOS, where we can only run one program at a time, and we have to stop running one before we can run the other at all. With completely independent resources, we have N computers running MS-DOS (without even a network to connect them) with no ability to share anything between them at all (because if we can even share a file, well, that\'s  a shared resource, a violation of the basic premise of nothing being shared).Every interesting case involves some combination of independent resources and shared resources. Every reasonably modern computer (and a lot that aren\'t at all modern) has at least some ability to carry out at least a few independent operations simultaneously, and just about anything more sophisticated than MS-DOS has taken advantage of that to at least some degree. The nice, clean division between "concurrent" and "parallel" that people like to draw just doesn\'t exist, and almost never has. What people like to classify as "concurrent" usually still involves at least one and often more different types of parallel execution. What they like to classify as "parallel" often involves sharing resources and (for example) one process blocking another\'s execution while using a resource that\'s shared between the two.People trying to draw a clean distinction between "parallel" and "concurrent" are living in a fantasy of computers that never actually existed.I understood the difference to be:1) Concurrent - running in tandem using shared resources\n2) Parallel - running side by side using different resourcesSo you can have two things happening at the same time independent of each other, even if they come together at points (2) or two things drawing on the same reserves throughout the operations being executed (1).Although there isn\xe2\x80\x99t complete\nagreement on the distinction between the terms parallel and concurrent,\nmany authors make the following distinctions:So parallel programs are concurrent, but a program such as a multitasking operating system is also concurrent, even when it is run on a machine with\nonly one core, since multiple tasks can be in progress at any instant.Source: An introduction to parallel programming, Peter PachecoConcurrency is a property of the program and parallel execution is a property of the machine. What concurrent parts should and should not be executed in parallel can only be answered when the exact hardware is known. Which I might like to add leads to the most unhappy conclusion when dealing with explicit parallel programming, There is no guarantee of both efficiency and portability with explicit parallel programs. Concurrent programming is in a general sense to refer to environments in which the tasks we define can occur in any order. One\n  task can occur before or after another, and some or all tasks can be\n  performed at the same time. Parallel programming is to specifically refer to the simultaneous  execution of concurrent tasks on different processors. Thus, all\n  parallel programming is concurrent, but not all concurrent programming\n  is parallel.Source: PThreads Programming - A POSIX Standard for Better Multiprocessing, Buttlar, Farrell, Nichols