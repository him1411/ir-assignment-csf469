Is there a "canonical" way of doing that? I\'ve been using head -n | tail -1 which does the trick, but I\'ve been wondering if there\'s a Bash tool that specifically extracts a line (or a range of lines) from a file.By "canonical" I mean a program whose main function is doing that.head and pipe with tail will be slow for a huge file. I would suggest sed like this:Where NUM is the number of the line you want to print; so, for example, sed \'10q;d\' file to print the 10th line of file.Explanation:NUMq will quit immediately when the line number is NUM.d will delete the line instead of printing it; this is inhibited on the last line because the q causes the rest of the script to be skipped when quitting.If you have NUM in a variable, you will want to use double quotes instead of single:will print 2nd line2011th lineline 10 up to line 331st and 3th lineand so on...For adding lines with sed, you can check this:sed: insert a line in a certain positionWith awk it is pretty fast:When this is true, the default behaviour of awk is performed: {print $0}.If your file happens to be huge, you\'d better exit after reading the required line. This way you save CPU time.If you want to give the line number from a bash variable you can use:I have a unique situation where I can benchmark the solutions proposed on this page, and so I\'m writing this answer as a consolidation of the proposed solutions with included run times for each.Set UpI have a 3.261 gigabyte ASCII text data file with one key-value pair per row. The file contains 3,339,550,320 rows in total and defies opening in any editor I have tried, including my go-to Vim. I need to subset this file in order to investigate some of the values that I\'ve discovered only start around row ~500,000,000.Because the file has so many rows:My best-case-scenario is a solution that extracts only a single line from the file without reading any of the other rows in the file, but I can\'t think of how I would accomplish this in Bash.For the purposes of my sanity I\'m not going to be trying to read the full 500,000,000 lines I\'d need for my own problem. Instead I\'ll be trying to extract row 50,000,000 out of 3,339,550,320 (which means reading the full file will take 60x longer than necessary).I will be using the time built-in to benchmark each command.BaselineFirst let\'s see how the head tail solution:The baseline for row 50 million is 00:01:15.321, if I\'d gone straight for row 500 million it\'d probably be ~12.5 minutes.cutI\'m dubious of this one, but it\'s worth a shot:This one took 00:05:12.156 to run, which is much slower than the baseline! I\'m not sure whether it read through the entire file or just up to line 50 million before stopping, but regardless this doesn\'t seem like a viable solution to the problem.AWKI only ran the solution with the exit because I wasn\'t going to wait for the full file to run:This code ran in 00:01:16.583, which is only ~1 second slower, but still not an improvement on the baseline. At this rate if the exit command had been excluded it would have probably taken around ~76 minutes to read the entire file!PerlI ran the existing Perl solution as well:This code ran in 00:01:13.146, which is ~2 seconds faster than the baseline. If I\'d run it on the full 500,000,000 it would probably take ~12 minutes.sedThe top answer on the board, here\'s my result:This code ran in 00:01:12.705, which is 3 seconds faster than the baseline, and ~0.4 seconds faster than Perl. If I\'d run it on the full 500,000,000 rows it would have probably taken ~12 minutes.mapfileI have bash 3.1 and therefore cannot test the mapfile solution.ConclusionIt looks like, for the most part, it\'s difficult to improve upon the head tail solution. At best the sed solution provides a ~3% increase in efficiency.(percentages calculated with the formula % = (runtime/baseline - 1) * 100)Row 50,000,000Row 500,000,000Row 3,338,559,320Wow, all the possibilities!Try this:or one of these depending upon your version of Awk:(You may have to try the nawk or gawk command).Is there a tool that only does the print that particular line? Not one of the standard tools. However, sed is probably the closest and simplest to use.Useful one-line scripts for sedThis question being tagged Bash, here\'s the Bash (\xe2\x89\xa54) way of doing: use mapfile with the -s (skip) and -n (count) option.If you need to get the 42nd line of a file file:At this point, you\'ll have an array ary the fields of which containing the lines of file (including the trailing newline), where we have skipped the first 41 lines (-s 41), and stopped after reading one line (-n 1). So that\'s really the 42nd line. To print it out:If you need a range of lines, say the range 42\xe2\x80\x93666 (inclusive), and say you don\'t want to do the math yourself, and print them on stdout:If you need to process these lines too, it\'s not really convenient to store the trailing newline. In this case use the -t option (trim):You can have a function do that for you:No external commands, only Bash builtins!You may also used sed print and quit:You can also use Perl for this:The fastest solution for big files is always tail|head, provided that the two distances:are known. Then, we could use this:howmany is just the count of lines required.Some more detail in https://unix.stackexchange.com/a/216614/79743One of possible ways:Note that without the q command, if the file is large, sed continues to work, which slows down the computation.I would say that head -n | tail -1 is hard to beat. For me, it is still best solution.It is portable and pretty readable. It is also very fast. The other answers include some benchmark, but it seems to differ very much on the system on which you are testing.In my own (non-representative) tests, head/tails outperformed sed \'NUMq;d\' consistently (it was significantly faster). But even in the other benchmarks that were posted, it is hard to find a case where head/tails was really bad. It is also not surprising, as these are operations that you would expect to be heavily optimized in a modern Unix system.The top-voted sed \'NUMq;d\' is interesting to know, but I would argue that it will be understood by fewer people out of the box than the head/tail solution.If you got multiple lines by delimited by \\n (normally new line). You can use \'cut\' as well:You will get the 2nd line from the file. -f3 gives you the 3rd line.To print nth line using sed with a variable as line number:Here the \'-e\' flag is for adding script to command to be executed.