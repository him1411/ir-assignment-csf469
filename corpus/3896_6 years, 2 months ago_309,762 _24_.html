Suppose I have a file similar to the following:I would like to find how many times \'123\' was duplicated, how many times \'234\' was duplicated, etc.\nSo ideally, the output would be like:Assuming there is one number per line:You can use the more verbose --count flag too with the GNU version, e.g., on Linux:This will print duplicate lines only, with counts:or, with GNU long options (on Linux):on BSD and OSX you have to use grep to filter out unique lines:For the given example, the result would be:If you want to print counts for all lines including those that appear only once:or, with GNU long options (on Linux):For the given input, the output is:In order to sort the output with the most frequent lines on top, you can do the following (to get all results):or, to get only duplicate lines, most frequent first:on OSX and BSD the final one becomes:To find and count duplicate lines in multiple files, you can try the following command:or:Via awk:In awk \'dups[$1]++\' command, the variable $1 holds the entire contents of column1 and square brackets are array access. So, for each 1st column of line in data file, the node of the array named dups is incremented.And at the end, we are looping over dups array with num as variable and print the saved numbers first then their number of duplicated value by dups[num].Note that your input file has spaces on end of some lines, if you clear up those, you can use $0 in place of $1 in command above :)Assuming you\'ve got access to a standard Unix shell and/or cygwin environment: Basically: convert all space characters to linebreaks, then sort the tranlsated output and feed that to uniq and count duplicate lines.In windows using "Windows PowerShell" I used the command mentioned below to achieve thisAlso we can use the where-object Cmdlet to filter the result