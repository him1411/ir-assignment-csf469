I\'m using Python 2 to parse JSON from ASCII encoded text files. When loading these files with either json or  simplejson, all my string values are cast to Unicode objects instead of string objects. The problem is, I have to use the data with some libraries that only accept string objects. I can\'t change the libraries nor update them.Is it possible to get string objects instead of Unicode ones?This question was asked a long time ago when I was stuck with Python 2. One easy and clean solution for today is to just use a recent version of Python \xe2\x80\x94 i.e. Python 3 and forward.Example usage:Mark Amery\'s function is shorter and clearer than these ones, so what\'s the point of them? Why would you want to use them?Purely for performance. Mark\'s answer decodes the JSON text fully first with unicode strings, then recurses through the entire decoded value to convert all strings to byte strings. This has a couple of undesirable effects:This answer mitigates both of those performance issues by using the object_hook parameter of json.load and json.loads. From the docs:object_hook is an optional function that will be called with the result of any object literal decoded (a dict). The return value of object_hook will be used instead of the dict. This feature can be used to implement custom decodersSince dictionaries nested many levels deep in other dictionaries get passed to object_hook as they\'re decoded, we can byteify any strings or lists inside them at that point and avoid the need for deep recursion later.Mark\'s answer isn\'t suitable for use as an object_hook as it stands, because it recurses into nested dictionaries. We prevent that recursion in this answer with the ignore_dicts parameter to _byteify, which gets passed to it at all times except when object_hook passes it a new dict to byteify. The ignore_dicts flag tells _byteify to ignore dicts since they already been byteified.Finally, our implementations of json_load_byteified and json_loads_byteified call _byteify (with ignore_dicts=True) on the result returned from json.load or json.loads to handle the case where the JSON text being decoded doesn\'t have a dict at the top level.While there are some good answers here, I ended up using PyYAML to parse my JSON files, since it gives the keys and values as str type strings instead of unicode type. Because JSON is a subset of YAML it works nicely:Some things to note though:I get string objects because all my entries are ASCII encoded. If I would use unicode encoded entries, I would get them back as unicode objects \xe2\x80\x94 there is no conversion!You should (probably always) use PyYAML\'s safe_load function; if you use it to load JSON files, you don\'t need the "additional power" of the load function anyway.If you want a YAML parser that has more support for the 1.2 version of the spec (and correctly parses very low numbers) try Ruamel YAML: pip install ruamel.yaml and import ruamel.yaml as yaml was all I needed in my tests.As stated, there is no conversion! If you can\'t be sure to only deal with ASCII values (and you can\'t be sure most of the time), better use a conversion function:I used the one from Mark Amery a couple of times now, it works great and is very easy to use. You can also use a similar function as an object_hook instead, as it might gain you a performance boost on big files. See the slightly more involved answer from Mirec Miskuf for that.There\'s no built-in option to make the json module functions return byte strings instead of unicode strings. However, this short and simple recursive function will convert any decoded JSON object from using unicode strings to UTF-8-encoded byte strings:Just call this on the output you get from a json.load or json.loads call.A couple of notes:You can use the object_hook parameter for json.loads to pass in a converter. You don\'t have to do the conversion after the fact. The json module will always pass the object_hook dicts only, and it will recursively pass in nested dicts, so you don\'t have to recurse into nested dicts yourself. I don\'t think I would convert unicode strings to numbers like Wells shows. If it\'s a unicode string, it was quoted as a string in the JSON file, so it is supposed to be a string (or the file is bad).Also, I\'d try to avoid doing something like str(val) on a unicode object. You should use value.encode(encoding) with a valid encoding, depending on what your external lib expects.So, for example:That\'s because json has no difference between string objects and unicode objects. They\'re all strings in javascript.I think JSON is right to return unicode objects. In fact, I wouldn\'t accept anything less, since javascript strings are in fact unicode objects (i.e. JSON (javascript) strings can store any kind of unicode character) so it makes sense to create unicode objects when translating strings from JSON. Plain strings just wouldn\'t fit since the library would have to guess the encoding you want.It\'s better to use unicode string objects everywhere. So your best option is to update your libraries so they can deal with unicode objects.But if you really want bytestrings, just encode the results to the encoding of your choice:There exists an easy work-around.TL;DR - Use ast.literal_eval() instead of json.loads().  Both ast and json are in the standard library.While not a \'perfect\' answer, it gets one pretty far if your plan is to ignore Unicode altogether.  In Python 2.7gives:This gets more hairy when some objects are really Unicode strings.  The full answer gets hairy quickly.I\'m afraid there\'s no way to achieve this automatically within the simplejson library.The scanner and decoder in simplejson are designed to produce unicode text. To do this, the library uses a function called c_scanstring (if it\'s available, for speed), or py_scanstring if the C version is not available. The scanstring function is called several times by nearly every routine that simplejson has for decoding a structure that might contain text. You\'d have to either monkeypatch the scanstring value in simplejson.decoder, or subclass JSONDecoder and provide pretty much your own entire implementation of anything that might contain text.The reason that simplejson outputs unicode, however, is that the json spec specifically mentions that "A string is a collection of zero or more Unicode characters"... support for unicode is assumed as part of the format itself. Simplejson\'s scanstring implementation goes so far as to scan and interpret unicode escapes (even error-checking for malformed multi-byte charset representations), so the only way it can reliably return the value to you is as unicode.If you have an aged library that needs an str, I recommend you either laboriously search the nested data structure after parsing (which I acknowledge is what you explicitly said you wanted to avoid... sorry), or perhaps wrap your libraries in some sort of facade where you can massage the input parameters at a more granular level. The second approach might be more manageable than the first if your data structures are indeed deeply nested.Mike Brennan\'s answer is close, but there is no reason to re-traverse the entire structure. If you use the object_hook_pairs (Python 2.7+) parameter:object_pairs_hook is an optional function that will be called with the result of any object literal decoded with an ordered list of pairs.  The return value of object_pairs_hook will be used instead of the dict. This feature can be used to implement custom decoders that rely on the order that the key and value pairs are decoded (for example, collections.OrderedDict will remember the order of insertion). If object_hook is also defined, the object_pairs_hook takes priority.With it, you get each JSON object handed to you, so you can do the decoding with no need for recursion:Notice that I never have to call the hook recursively since every object will get handed to the hook when you use the object_pairs_hook. You do have to care about lists, but as you can see, an object within a list will be properly converted, and you don\'t have to recurse to make it happen.EDIT: A coworker pointed out that Python2.6 doesn\'t have object_hook_pairs. You can still use this will Python2.6 by making a very small change. In the hook above, change:toThen use object_hook instead of object_pairs_hook:Using object_pairs_hook results in one less dictionary being instantiated for each object in the JSON object, which, if you were parsing a huge document, might be worth while.The gotcha is that simplejson and json are two different modules, at least in the manner they deal with unicode. You have json in py 2.6+, and this gives you unicode values, whereas simplejson returns string objects. Just try easy_install-ing simplejson in your environment and see if that works. It did for me.As Mark (Amery) correctly notes: Using PyYaml\'s deserializer on a json dump works only if you have ASCII only. At least out of the box. Two quick comments on the PyYaml approach:NEVER use yaml.load on data from the field. Its a feature(!) of yaml to execute arbitrary code hidden within the structure. You can make it work also for non ASCII via this:But performance wise its of no comparison to Mark Amery\'s answer:Throwing some deeply nested sample dicts onto the two methods, I get this (with dt[j] = time delta of json.loads(json.dumps(m))):So deserialization including fully walking the tree and encoding, well within the order of magnitude of json\'s C based implementation. I find this remarkably fast and its also more robust than the yaml load at deeply nested structures. And less security error prone, looking at yaml.load.=> While I would appreciate a pointer to a C only based converter the byteify function should be the default answer. This holds especially true if your json structure is from the field, containing user input. Because then you probably need to walk anyway over your structure - independent on your desired internal data structures (\'unicode sandwich\' or byte strings only).Why?Unicode normalisation. For the unaware: Take a painkiller and read this.So using the byteify recursion you kill two birds with one stone: In my tests it turned out that replacing the input.encode(\'utf-8\') with a unicodedata.normalize(\'NFC\', input).encode(\'utf-8\') was even faster than w/o NFC - but thats heavily dependent on the sample data I guess.So, I\'ve run into the same problem. Guess what was the first Google result.Because I need to pass all data to PyGTK, unicode strings aren\'t very useful to me either. So I have another recursive conversion method. It\'s actually also needed for typesafe JSON conversion - json.dump() would bail on any non-literals, like Python objects. Doesn\'t convert dict indexes though.Check out this answer to a similar question like this which states thatThe u- prefix just means that you have a Unicode string.  When you really use the string, it won\'t appear in your data.  Don\'t be thrown by the printed output.For example, try this:You won\'t see a u.Support Python2&3 using hook (from https://stackoverflow.com/a/33571117/558397)Returns:This is late to the game, but I built this recursive caster. It works for my needs and I think it\'s relatively complete. It may help you.Just pass it a JSON object like so:I have it as a private member of a class, but you can repurpose the method as you see fit.I rewrote Wells\'s _parse_json() to handle cases where the json object itself is an array (my use case).Just use pickle instead of json for dump and load, like so:The output it produces is (strings and integers are handled correctly):here is a recursive encoder written in C: \nhttps://github.com/axiros/nested_encodePerformance overhead for "average" structures around 10% compared to json.loads.using this teststructure:I ran into this problem too, and having to deal with JSON, I came up with a small loop that converts the unicode keys to strings.  (simplejson on GAE does not return string keys.)obj is the object decoded from JSON:kwargs is what I pass to the constructor of the GAE application (which does not like unicode keys in **kwargs)Not as robust as the solution from Wells, but much smaller.I\'ve adapted the code from the answer of Mark Amery, particularly in order to get rid of isinstance for the pros of duck-typing.The encoding is done manually and ensure_ascii is disabled. The python docs for json.dump says that If ensure_ascii is True (the default), all non-ASCII characters in the output are escaped with \\uXXXX sequencesDisclaimer: in the doctest I used the Hungarian language. Some notable Hungarian-related character encodings are: cp852 the IBM/OEM encoding used eg. in DOS (sometimes referred as ascii, incorrectly I think, it is dependent on the codepage setting), cp1250 used eg. in Windows (sometimes referred as ansi, dependent on the locale settings), and iso-8859-2, sometimes used on http servers. The test text T\xc3\xbcsk\xc3\xa9sh\xc3\xa1t\xc3\xba k\xc3\xadgy\xc3\xb3b\xc5\xb1v\xc3\xb6l\xc5\x91 is attributed to Koltai L\xc3\xa1szl\xc3\xb3 (native personal name form) and is from wikipedia.I\'d also like to highlight the answer of Jarret Hardie which references the JSON spec, quoting:A string is a collection of zero or more Unicode charactersIn my use-case I had files with json. They are utf-8 encoded files. ensure_ascii results in properly escaped but not very readable json files, that is why I\'ve adapted Mark Amery\'s answer to fit my needs.The doctest is not particularly thoughtful but I share the code in the hope that it will useful for someone.I had a JSON dict as a string. The keys and values were unicode objects like in the following example:I could use the byteify function suggested above by converting the string to a dict object using ast.literal_eval(myStringDict).