I\'m trying to run a find command for all JavaScript files, but how do I exclude a specific directory?Here is the find code we\'re using.Use the prune switch, for example if you want to exclude the misc directory just add a -path ./misc -prune -o to your find command:Here is an example with multiple directories:Here we exclude dir1, dir2 and dir3, since in find expressions it is an action, that acts on the criteria -path dir1 -o -path dir2 -o -path dir3 (if dir1 or dir2 or dir3), ANDed with type -d. Further action is -o print, just print. If -prune doesn\'t work for you, this will:I find the following easier to reason about than other proposed solutions:This comes from an actual use case, where I needed to call yui-compressor on some files generated by wintersmith, but leave out other files that need to be sent as-is.Inside \\( and \\) is an expression that will match exactly build/external, and will, on success, avoid traversing anything below. This is then grouped as a single expression with the escaped parenthesis, and prefixed with -not which will make find skip anything that was matched by that expression.One might ask if adding -not will not make all other files hidden by -prune reappear, and the answer is no. The way -prune works is that anything that, once it is reached, the files below that directory are permanently ignored.That is also easy to expand to add additional exclusions. For example:There is clearly some confusion here as to what the preferred syntax for skipping a directory should be.GNU OpinionFrom the GNU find man pageReasoning-prune stops find from descending into a directory. Just specifying -not -path will still descend into the skipped directory, but -not -path will be false whenever find tests each file.Issues with -prune-prune does what it\'s intended to, but are still some things you have to take care of when using it.find prints the pruned directory.-prune only works with -print and no other actions.PerformanceI set up a simple test of the three top upvoted answers on this question (replaced -print with -exec bash -c \'echo $0\' {} \\; to show another action example). Results are belowConclusionBoth f10bit\'s syntax and Daniel C. Sobral\'s syntax took 10-25ms to run on average. GetFree\'s syntax, which doesn\'t use -prune, took 865ms. So, yes this is a rather extreme example, but if you care about run time and are doing anything remotely intensive you should use -prune. Note Daniel C. Sobral\'s syntax performed the better of the two -prune syntaxes; but, I strongly suspect this is the result of some caching as switching the order in which the two ran resulted in the opposite result, while the non-prune version was always slowest.Test ScriptOne option would be to exclude all results that contain the directory name with grep. For example:I prefer the -not notation ... it\'s more readable:Use the -prune option.  So, something like:The \'-type d -name proc -prune\' only look for directories named proc to exclude.\nThe \'-o\' is an \'OR\' operator.This is the format I used to exclude some paths:I used this to find all files not in ".*" paths:For a working solution (tested on UbuntuÂ 12.04 (Precise Pangolin))...will search for MP3 files in the current folder and subfolders except in dir1 subfolder.Use:...to exclude dir1 AND dir2To exclude multiple directories:To add directories, add -o -path "./dirname/*":But maybe you should use a regular expression, if there are many directories to exclude.You can use the prune option to achieve this. As in for example:Or the inverse grep \xe2\x80\x9cgrep -v\xe2\x80\x9d option:You can find detailed instructions and examples in Linux find command exclude directories from searching.I was using find to provide a list of files for xgettext, and wanted to omit a specific directory and its contents. I tried many permutations of -path combined with -prune but was unable to fully exclude the directory which I wanted gone.Although I was able to ignore the contents of the directory which I wanted ignored, find then returned the directory itself as one of the results, which caused xgettext to crash as a result (doesn\'t accept directories; only files).My solution was to simply use grep -v to skip the directory that I didn\'t want in the results:Whether or not there is an argument for find that will work 100%, I cannot say for certain. Using grep was a quick and easy solution after some headache.seems to work the same asand is easier to remember IMO.None of previous answers is good on Ubuntu.\nTry this:I have found this hereThis is suitable for me on a Mac:It will exclude vendor and app/cache dir for search name which suffixed with php.how-to-use-prune-option-of-find-in-sh is an excellent answer by Laurence Gonsalves on how -prune works.And here is the generic solution:To avoid typing /path/to/seach/ multiple times, wrap the find in a pushd .. popd pair.I found the functions name in C sources files exclude *.o and exclude *.swp and exclude (not regular file) and exclude dir output with this command:Better use the exec action than the for loop:The exec ... \'{}\' ... \'{}\' \\; will be executed once for every matching file, replacing the braces \'{}\' with the current file name.Notice that the braces are enclosed in single quote marks to protect them from interpretation as shell script punctuation*.* From the EXAMPLES section of the find (GNU findutils) 4.4.2 man pageThis works because find TESTS the files for the pattern "*foo*":but it does NOT work if you don\'t use a pattern (find does not TEST the file). So find makes no use of its former evaluated "true" & "false" bools. Example for not working use case with above notation:There is no find TESTING! So if you need to find files without any pattern matching use the -prune. Also, by the use of prune find is always faster while it really skips that directories instead of matching it or better not matching it. So in that case use something like:or:RegardsFor FreeBSD users:If search directories has pattern (in my case most of the times); you can simply do it like below:In above example; it searches in all the sub-directories starting with "n".I have found the suggestions on this page and a lot of other pages just do not work on my Mac OS X system. However, I have found a variation which does work for me.The big idea is to search the Macintosh HD but avoid traversing all the external volumes, which are mostly Time Machine backups, image backups, mounted shares, and archives, but without having to unmount them all, which is often impractical.Here is my working script, which I have named "findit".The various paths have to do with external archive volumes, Time Machine, Virtual Machines, other mounted servers, and so on. Some of the volume names have spaces in them.A good test run is "findit index.php", because that file occurs in many places on my system. With this script, it takes about 10 minutes to search the main hard drive. Without those exclusions, it takes many hours.i wanted to know the number of directories, files an MB of just the current directory - and that code does exactly what i want :-)the sourcethe codenote: the extra format="%s%\'12d\\n" is necessary for awk to format the numbers.the resultNot sure if this would cover all edge cases, but following would be pretty straight forward and simple to try:ls -1|grep -v -e ddl -e docs| xargs rm -rfThis should remove all files/directories from the current directory excpet \'ddls\' and \'docs\'.I tried command above, but none of those using "-prune" works for me.\nEventually I tried this out with command below:The -path -prune approach also works with wildcards in the path. Here is a find statement that will find the directories for a git server serving multiple git repositiories leaving out the git internal directories: