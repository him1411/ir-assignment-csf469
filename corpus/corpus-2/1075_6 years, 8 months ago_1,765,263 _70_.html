It seems there are different ways to read and write data of files in Java.I want to read ASCII data from a file. What are the possible ways and their differences?ASCII is a TEXT file so you would use Readers for reading. Java also supports reading from a binary file using InputStreams. If the files being read are huge then you would want to use a BufferedReader on top of a FileReader to improve read performance.Go through this article on how to use a ReaderI\'d also recommend you download and read this wonderful (yet free) book called Thinking In JavaIn Java 7:new String(Files.readAllBytes(...)) or Files.readAllLines(...)In Java 8:Files.lines(..).forEach(...)My favorite way to read a small file is to use a BufferedReader and a StringBuilder. It is very simple and to the point (though not particularly effective, but good enough for most cases):Some has pointed out that after Java 7 you should use try-with-resources (i.e. auto close) features:When I read strings like this, I usually want to do some string handling per line anyways, so then I go for this implementation.Though if I want to actually just read a file into a String, I always use Apache Commons IO with the class IOUtils.toString() method. You can have a look at the source here:http://www.docjar.com/html/api/org/apache/commons/io/IOUtils.java.htmlAnd even simpler with Java 7:The easiest way is to use the Scanner class in Java and the FileReader object. Simple example:Scanner has several methods for reading in strings, numbers, etc... You can look for more information on this on the Java documentation page.For example reading the whole content into a String:Also if you need a specific encoding you can use this instead of FileReader:Here\'s another way to do it without using external libraries:Here is a simple solution:The methods within org.apache.commons.io.FileUtils may also be very handy, e.g.:What do you want to do with the text?  Is the file small enough to fit into memory?  I would try to find the simplest way to handle the file for your needs.  The FileUtils library is very handle for this.Here are the three working and tested methods:I had to benchmark the different ways. I shall comment on my findings but, in short, the fastest way is to use a plain old BufferedInputStream over a FileInputStream. If many files must be read then three threads will reduce the total execution time to roughly half, but adding more threads will progressively degrade performance until making it take three times longer to complete with twenty threads than with just one thread.The assumption is that you must read a file and do something meaningful with its contents. In the examples here is reading lines from a log and count the ones which contain values that exceed a certain threshold. So I am assuming that the one-liner Java 8 Files.lines(Paths.get("/path/to/file.txt")).map(line -> line.split(";")) is not an option.I tested on Java 1.8, Windows 7 and both SSD and HDD drives.I wrote six different implementations:rawParse: Use BufferedInputStream over a FileInputStream and then cut lines reading byte by byte. This outperformed any other single-thread approach, but it may be very inconvenient for non-ASCII files.lineReaderParse: Use a BufferedReader over a FileReader, read line by line, split lines by calling String.split(). This is approximatedly 20% slower that rawParse.lineReaderParseParallel: This is the same as lineReaderParse, but it uses several threads. This is the fastest option overall in all cases.nioFilesParse: Use java.nio.files.Files.lines()nioAsyncParse: Use an AsynchronousFileChannel with a completion handler and a thread pool.nioMemoryMappedParse: Use a memory-mapped file. This is really a bad idea yielding execution times at least three times longer than any other implementation.These are the average times for reading 204 files of 4Â MB each on an quad-core i7 and SSD drive. The files are generated on the fly to avoid disk caching.I found a difference smaller than I expected between running on an SSD or an HDD drive being the SSD approximately 15% faster. This may be because the files are generated on an unfragmented HDD and they are read sequentially, therefore the spinning drive can perform nearly as an SSD.I was surprised by the low performance of the nioAsyncParse implementation. Either I have implemented something in the wrong way or the multi-thread implementation using NIO and a completion handler performs the same (or even worse) than a single-thread implementation with the java.io API. Moreover the asynchronous parse with a CompletionHandler is much longer in lines of code and tricky to implement correctly than a straight implementation on old streams.Now the six implementations followed by a class containing them all plus a parametrizable main() method that allows to play with the number of files, file size and concurrency degree. Note that the size of the files varies plus minus 20%. This is to avoid any effect due to all the files being of exactly the same size.rawParselineReaderParselineReaderParseParallelnioFilesParsenioAsyncParseFULL RUNNABLE IMPLEMENTATION OF ALL CASEShttps://github.com/sergiomt/javaiobenchmark/blob/master/FileReadBenchmark.javaUsing BufferedReader:This is basically the exact same as Jesus Ramos\' answer, except with File instead of FileReader plus iteration to step through the contents of the file.... throws FileNotFoundExceptionBelow is a one-liner of doing it in the Java 8 way. Assuming text.txt file is in the root of the project directory of the Eclipse.Probably not as fast as with buffered I/O, but quite terse:The \\Z pattern tells the Scanner that the delimiter is EOF.I don\'t see it mentioned yet in the other answers so far.  But if "Best" means speed, then the new Java I/O (NIO) might provide the fastest preformance, but not always the easiest to figure out for someone learning.http://download.oracle.com/javase/tutorial/essential/io/file.htmlThe most simple way to read data from a file in Java is making use of the File class to read the file and the Scanner class to read the content of the file.PS: Don\'t forget to import java.util.*; for Scanner to work.For JSF-based Maven web applications, just use ClassLoader and the Resources folder to read in any file you want:Put the Apache Commons IO dependency into your POM:Use the code below to read it (e.g. below is reading in a .json file):You can do the same for text files, .properties files, XSD schemas, etc.Here is a simple solution:This might not be the exact answer to the question, it\'s just another way of reading a file where you do not explicitly specify the path to your file in your java code and instead, you read it as a  command line argument.with the following code:just go ahead and run it withThis would read the contents of the input.txt and print it to the your console.You can also make your System.out.println() to write to a specific file through the command line as follows:This would read from input.txt and write to output.txtCactoos give you a declarative one-liner:Use Java kiss if this is about simplicity of structure:Guava provides a one-liner for this:This code I programmed is much faster for very large files: