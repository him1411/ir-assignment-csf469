There are plenty of performance questions on this site already, but it occurs to me that almost all are very problem-specific and fairly narrow. And almost all repeat the advice to avoid premature optimization.Let\'s assume:What I am looking for here is strategies and tricks to squeeze out up to the last few percent in a critical algorithm when there is nothing else left to do but whatever it takes.Ideally, try to make answers language agnostic, and indicate any down-sides to the suggested strategies where applicable.I\'ll add a reply with my own initial suggestions, and look forward to whatever else the Stack Overflow community can think of.OK, you\'re defining the problem to where it would seem there is not much room for improvement. That is fairly rare, in my experience. I tried to explain this in a Dr. Dobbs article in November \'93, by starting from a conventionally well-designed non-trivial program with no obvious waste and taking it through a series of optimizations until its wall-clock time was reduced from 48 seconds to 1.1 seconds, and the source code size was reduced by a factor of 4. My diagnostic tool was this. The sequence of changes was this:The first problem found was use of list clusters (now called "iterators" and "container classes") accounting for over half the time. Those were replaced with fairly simple code, bringing the time down to 20 seconds.Now the largest time-taker is more list-building. As a percentage, it was not so big before, but now it is because the bigger problem was removed. I find a way to speed it up, and the time drops to 17 sec.Now it is harder to find obvious culprits, but there are a few smaller ones that I can do something about, and the time drops to 13 sec.Now I seem to have hit a wall. The samples are telling me exactly what it is doing, but I can\'t seem to find anything that I can improve. Then I reflect on the basic design of the program, on its transaction-driven structure, and ask if all the list-searching that it is doing is actually mandated by the requirements of the problem.Then I hit upon a re-design, where the program code is actually generated (via preprocessor macros) from a smaller set of source, and in which the program is not constantly figuring out things that the programmer knows are fairly predictable. In other words, don\'t "interpret" the sequence of things to do, "compile" it.Now, because it\'s getting so quick, it\'s hard to sample, so I give it 10 times as much work to do, but the following times are based on the original workload.More diagnosis reveals that it is spending time in queue-management. In-lining these reduces the time to 7 seconds.Now a big time-taker is the diagnostic printing I had been doing. Flush that - 4 seconds.Now the biggest time-takers are calls to malloc and free. Recycle objects - 2.6 seconds.Continuing to sample, I still find operations that are not strictly necessary - 1.1 seconds.Total speedup factor: 43.6Now no two programs are alike, but in non-toy software I\'ve always seen a progression like this. First you get the easy stuff, and then the more difficult, until you get to a point of diminishing returns. Then the insight you gain may well lead to a redesign, starting a new round of speedups, until you again hit diminishing returns. Now this is the point at which it might make sense to wonder whether ++i or i++ or for(;;) or while(1) are faster: the kinds of questions I see so often on SO.P.S. It may be wondered why I didn\'t use a profiler. The answer is that almost every one of these "problems" was a function call site, which stack samples pinpoint. Profilers, even today, are just barely coming around to the idea that statements and call instructions are more important to locate, and easier to fix, than whole functions.\nI actually built a profiler to do this, but for a real down-and-dirty intimacy with what the code is doing, there\'s no substitute for getting your fingers right in it. It is not an issue that the number of samples is small, because none of the problems being found are so tiny that they are easily missed.ADDED: jerryjvl  requested some examples. Here is the first problem. It consists of a small number of separate lines of code, together taking over half the time:These were using the list cluster ILST (similar to a list class). They are implemented in the usual way, with "information hiding" meaning that the users of the class were not supposed to have to care how they were implemented. When these lines were written (out of roughly 800 lines of code) thought was not given to the idea that these could be a "bottleneck" (I hate that word). They are simply the recommended way to do things. It is easy to say in hindsight that these should have been avoided, but in my experience all performance problems are like that. In general, it is good to try to avoid creating performance problems. It is even better to find and fix the ones that are created, even though they "should have been avoided" (in hindsight). I hope that gives a bit of the flavor.Here is the second problem, in two separate lines:These are building lists by appending items to their ends. (The fix was to collect the items in arrays, and build the lists all at once.) The interesting thing is that these statements only cost (i.e. were on the call stack) 3/48 of the original time, so they were not in fact a big problem at the beginning. However, after removing the first problem, they cost 3/20 of the time and so were now a "bigger fish". In general, that\'s how it goes.I might add that this project was distilled from a real project I helped on. In that project, the performance problems were far more dramatic (as were the speedups), such as calling a database-access routine within an inner loop to see if a task was finished.REFERENCE ADDED:\nThe source code, both original and redesigned, can be found in www.ddj.com, for 1993, in file 9311.zip, files slug.asc and slug.zip .EDIT 2011/11/26:\nThere is now a sourceforge project containing source code in Visual C++ and a blow-by-blow description of how it was tuned. It only goes through the first half of the scenario described above, and it doesn\'t follow exactly the same sequence, but still gets a 2-3 order of magnitude speedup.Suggestions:When you can\'t improve the performance any more - see if you can improve the perceived performance instead.You may not be able to make your fooCalc algorithm faster, but often there are ways to make your application seem more responsive to the user. A few examples:These won\'t make your program faster, but it might make your users happier with the speed you have.I spend most of my life in just this place. The broad strokes are to run your profiler and get it to record:And one more thing I like to do:Throw more hardware at it!More suggestions:Avoid I/O: Any I/O (disk, network, ports, etc.) is\nalways going to be far slower than any code that is\nperforming calculations, so get rid of any I/O that you do\nnot strictly need.Move I/O up-front: Load up all the data you are going\nto need for a calculation up-front, so that you do not\nhave repeated I/O waits within the core of a critical\nalgorithm (and maybe as a result repeated disk seeks, when\nloading all the data in one hit may avoid seeking).Delay I/O: Do not write out your results until the\ncalculation is over, store them in a data structure and\nthen dump that out in one go at the end when the hard work\nis done.Threaded I/O: For those daring enough, combine \'I/O\nup-front\' or \'Delay I/O\' with the actual calculation by\nmoving the loading into a parallel thread, so that while\nyou are loading more data you can work on a calculation on\nthe data you already have, or while you calculate the next\nbatch of data you can simultaneously write out the results\nfrom the last batch.Since many of the performance problems involve database issues, I\'ll give you some specific things to look at when tuning queries and stored procedures.Avoid cursors in most databases. Avoid looping as well. Most of the time, data access should be set-based, not record by record processing. This includes not reusing a single record stored procedure when you want to insert 1,000,000 records at once.Never use select *, only return the fields you actually need. This is especially true if there are any joins as the join fields will be repeated and thus cause unnecesary load on both the server and the network.Avoid the use of correlated subqueries. Use joins (including joins to derived tables where possible) (I know this is true for Microsoft SQL Server, but test the advice when using a differnt backend).Index, index, index. And get those stats updated if applicable to your database.Make the query sargable. Meaning avoid things which make it impossible to use the indexes such as using a wildcard in the first character of a like clause or a function in the join or as the left part of a where statement.Use correct data types. It is faster to do date math on a date field than to have to try to convert a string datatype to a date datatype, then do the calculation.Never put a loop of any kind into a trigger!Most databases have a way to check how the query execution will be done. In Microsoft SQL Server this is called an execution plan. Check those first to see where problem areas lie.Consider how often the query runs as well as how long it takes to run when determining what needs to be optimized. Sometimes you can gain more perfomance from a slight tweak to a query that runs millions of times a day than you can from wiping time off a long_running query that only runs once a month.Use some sort of profiler tool to find out what is really being sent to and from the database. I can remember one time in the past where we couldn\'t figure out why the page was so slow to load when the stored procedure was fast and found out through profiling that the webpage was asking for the query many many times instead of once. The profiler will also help you to find who are blocking who. Some queries that execute quickly while running alone may become really slow due to locks from other queries.The single most important limiting factor today is the limited memory bandwitdh. Multicores are just making this worse, as the bandwidth is shared betwen cores. Also, the limited chip area devoted to implementing caches is also divided among the cores and threads, worsening this problem even more. Finally, the inter-chip signalling needed to keep the different caches coherent also increase with an increased number of cores. This also adds a penalty.These are the effects that you need to manage. Sometimes through micro managing your code, but sometimes through careful consideration and refactoring.A lot of comments already mention cache friendly code. There are at least two distinct flavors of this:The first problem specifically has to do with making your data access patterns more regular, allowing the hardware prefetcher to work efficiently. Avoid dynamic memory allocation which spreads your data objects around in memory. Use linear containers instead of linked lists, hashes and trees.The second problem has to do with improving data reuse. Alter your algorithms to work on subsets of your data that do fit in available cache, and reuse that data as much as possible while it is still in the cache.Packing data tighter and making sure you use all data in cache lines in the hot loops, will help avoid these other effects, and allow fitting more useful data in the cache.You should probably consider the "Google perspective", i.e. determine how your application can become largely parallelized and concurrent, which will inevitably also mean at some point to look into distributing your application across different machines and networks, so that it can ideally scale almost linearly with the hardware that you throw at it.On the other hand, the Google folks are also known for throwing lots of manpower and resources at solving some of the issues in projects, tools and infrastructure they are using, such as for example whole program optimization for gcc by having a dedicated team of engineers hacking gcc internals in order to prepare it for Google-typical use case scenarios.Similarly, profiling an application no longer means to simply profile the program code, but also all its surrounding systems and infrastructure (think networks, switches, server, RAID arrays) in order to identify redundancies and optimization potential from a system\'s point of view.Although I like Mike Dunlavey\'s answer, in fact it is a great answer indeed with supporting example, I think it could be expressed very simply thus:Find out what takes the largest amounts of time first, and understand why.It is the identification process of the time hogs that helps you understand where you must refine your algorithm. This is the only all-encompassing language agnostic answer I can find to a problem that\'s already supposed to be fully optimised. Also presuming you want to be architecture independent in your quest for speed.So while the algorithm may be optimised, the implementation of it may not be. The identification allows you to know which part is which: algorithm or implementation. So whichever hogs the time the most is your prime candidate for review. But since you say you want to squeeze the last few % out, you might want to also examine the lesser parts, the parts that you have not examined that closely at first.Lastly a bit of trial and error with performance figures on different ways to implement the same solution, or potentially different algorithms, can bring insights that help identify time wasters and time savers.HPH,\nasoudmove.Divide and conquerIf the dataset being processed is too large, loop over chunks of it. If you\'ve done your code right, implementation should be easy. If you have a monolithic program, now you know better.First of all, as mentioned in several prior answers, learn what bites your performance - is it memory or processor or network or database or something else. Depending on that......if it\'s memory - find one of the books written long time ago by Knuth, one of "The Art of Computer Programming" series. Most likely it\'s one about sorting and search - if my memory is wrong then you\'ll have to find out in which he talks about how to deal with slow tape data storage. Mentally transform his memory/tape pair into your pair of cache/main memory (or in pair of L1/L2 cache) respectively. Study all the tricks he describes - if you don\'s find something that solves your problem, then hire professional computer scientist to conduct a professional research. If your memory issue is by chance with FFT (cache misses at bit-reversed indexes when doing radix-2 butterflies) then don\'t hire a scientist - instead, manually optimize passes one-by-one until you\'re either win or get to dead end. You mentioned squeeze out up to the last few percent right? If it\'s few indeed you\'ll most likely win....if it\'s processor - switch to assembly language. Study processor specification - what takes ticks, VLIW, SIMD. Function calls are most likely replaceable tick-eaters. Learn loop transformations - pipeline, unroll. Multiplies and divisions might be replaceable / interpolated with bit shifts (multiplies by small integers might be replaceable with additions). Try tricks with shorter data - if you\'re lucky one instruction with 64 bits might turn out replaceable with two on 32 or even 4 on 16 or 8 on 8 bits go figure. Try also longer data - eg your float calculations might turn out slower than double ones at particular processor. If you have trigonometric stuff, fight it with pre-calculated tables; also keep in mind that sine of small value might be replaced with that value if loss of precision is within allowed limits....if it\'s network - think of compressing data you pass over it. Replace XML transfer with binary. Study protocols. Try UDP instead of TCP if you can somehow handle data loss....if it\'s database, well, go to any database forum and ask for advice. In-memory data-grid, optimizing query plan etc etc etc.HTH :)I think this has already been said in a different way.  But when you\'re dealing with a processor intensive algorithm, you should simplify everything inside the most inner loop at the expense of everything else.  That may seem obvious to some, but it\'s something I try to focus on regardless of the language I\'m working with.  If you\'re dealing with nested loops, for example, and you find an opportunity to take some code down a level, you can in some cases drastically speed up your code.  As another example, there are the little things to think about like working with integers instead of floating point variables whenever you can, and using multiplication instead of division whenever you can.  Again, these are things that should be considered for your most inner loop.  Sometimes you may find benefit of performing your math operations on an integer inside the inner loop, and then scaling it down to a floating point variable you can work with afterwards.  That\'s an example of sacrificing speed in one section to improve the speed in another, but in some cases the pay off can be well worth it.Caching! A cheap way (in programmer effort) to make almost anything faster is to add a caching abstraction layer to any data movement area of your program. Be it I/O or just passing/creation of objects or structures. Often it\'s easy to add caches to factory classes and reader/writers.Sometimes the cache will not gain you much, but it\'s an easy method to just add caching all over and then disable it where it doesn\'t help. I\'ve often found this to gain huge performance without having to micro-analyse the code.Very difficult to give a generic answer to this question. It really depends on your problem domain and technical implementation. A general technique that is fairly language neutral: Identify code hotspots that cannot be eliminated, and hand-optimize assembler code. Last few % is a very CPU and application dependent thing....The list goes on.... But these sorts of things really are\nthe last resort...Build for x86, and run Valgrind/Cachegrind against the code\nfor proper performance profiling. Or Texas Instruments\'\nCCStudio has a sweet profiler. Then you\'ll really know where\nto focus...Did you know that a CAT6 cable is capable of 10x better shielding off extrenal inteferences than a default Cat5e UTP cable?For any non-offline projects, while having best software and best hardware, if your throughoutput is weak, then that thin line is going to squeeze data and give you delays, albeit in milliseconds... but if you are talking about the last drops, that\'s a some drops gained, 24/7 for any packge sent or received.I\'ve spent some time working on optimising client/server business systems operating over low-bandwidth and long-latency networks (e.g. satellite, remote, offshore), and been able to achieve some dramatic performance improvements with a fairly repeatable process.Measure: Start by understanding the network\'s underlying capacity and topology. Talking to the relevant networking people in the business, and make use of basic tools such as ping and traceroute to establish (at a minimum) the network latency from each client location, during typical operational periods. Next, take accurate time measurements of specific end user functions that display the problematic symptoms. Record all of these measurements, along with their locations, dates and times. Consider building end-user "network performance testing" functionality into your client application, allowing your power users to participate in the process of improvement; empowering them like this can have a huge psychological impact when you\'re dealing with users frustrated by a poorly performing system.Analyze: Using any and all logging methods available to establish exactly what data is being transmitted and received during the execution of the affected operations. Ideally, your application can capture data transmitted and received by both the client and the server. If these include timestamps as well, even better. If sufficient logging isn\'t available (e.g. closed system, or inability to deploy modifications into a production environment), use a network sniffer and make sure you really understand what\'s going on at the network level.Cache: Look for cases where static or infrequently changed data is being transmitted repetitively and consider an appropriate caching strategy. Typical examples include "pick list" values or other "reference entities", which can be surprisingly large in some business applications. In many cases, users can accept that they must restart or refresh the application to update infrequently updated data, especially if it can shave significant time from the display of commonly used user interface elements. Make sure you understand the real behaviour of the caching elements already deployed - many common caching methods (e.g. HTTP ETag) still require a network round-trip to ensure consistency, and where network latency is expensive, you may be able to avoid it altogether with a different caching approach.Parallelise: Look for sequential transactions that don\'t logically need to be issued strictly sequentially, and rework the system to issue them in parallel. I dealt with one case where an end-to-end request had an inherent network delay of ~2s, which was not a problem for a single transaction, but when 6 sequential 2s round trips were required before the user regained control of the client application, it became a huge source of frustration. Discovering that these transactions were in fact independent allowed them to be executed in parallel, reducing the end-user delay to very close to the cost of a single round trip.Combine: Where sequential requests must be executed sequentially, look for opportunities to combine them into a single more comprehensive request. Typical examples include creation of new entities, followed by requests to relate those entities to other existing entities.Compress: Look for opportunities to leverage compression of the payload, either by replacing a textual form with a binary one, or using actual compression technology. Many modern (i.e. within a decade) technology stacks support this almost transparently, so make sure it\'s configured. I have often been surprised by the significant impact of compression where it seemed clear that the problem was fundamentally latency rather than bandwidth,  discovering after the fact that it allowed the transaction to fit within a single packet or otherwise avoid packet loss and therefore have an outsize impact on performance.Repeat: Go back to the beginning and re-measure your operations (at the same locations and times) with the improvements in place, record and report your results. As with all optimisation, some problems may have been solved exposing others that now dominate.In the steps above, I focus on the application related optimisation process, but of course you must ensure the underlying network itself is configured in the most efficient manner to support your application too. Engage the networking specialists in the business and determine if they\'re able to apply capacity improvements, QoS, network compression, or other techniques to address the problem. Usually, they will not understand your application\'s needs, so it\'s important that you\'re equipped (after the Analyse step) to discuss it with them, and also to make the business case for any costs you\'re going to be asking them to incur. I\'ve encountered cases where erroneous network configuration caused the applications data to be transmitted over a slow satellite link rather than an overland link, simply because it was using a TCP port that was not "well known" by the networking specialists; obviously rectifying a problem like this can have a dramatic impact on performance, with no software code or configuration changes necessary at all.Not nearly as in depth or complex as previous answers, but here goes:\n(these are more beginner/intermediate level)Impossible to say. It depends on what the code looks like. If we can assume that the code already exists, then we can simply look at it and figure out from that, how to optimize it.Better cache locality, loop unrolling, Try to eliminate long dependency chains, to get better instruction-level parallelism. Prefer conditional moves over branches when possible. Exploit SIMD instructions when possible.Understand what your code is doing, and understand the hardware it\'s running on. Then it becomes fairly simple to determine what you need to do to improve performance of your code. That\'s really the only truly general piece of advice I can think of.Well, that, and "Show the code on SO and ask for optimization advice for that specific piece of code".If better hardware is an option then definitely go for that. OtherwiseThe google way is one option "Cache it.. Whenever possible don\'t touch the disk"Here are some quick and dirty optimization techniques I use.  I consider this to be a \'first pass\' optimization.Learn where the time is spent Find out exactly what is taking the time.  Is it file  IO?  Is it CPU time?  Is it the network?  Is it the Database?  It\'s useless to optimize for IO if that\'s not the bottleneck.Know Your Environment  Knowing where to optimize typically depends on the development environment.  In VB6, for example, passing by reference is slower than passing by value, but in C and C++, by reference is vastly faster.  In C, it is reasonable to try something and do something different if a return code indicates a failure, while in Dot Net, catching exceptions are much slower than checking for a valid condition before attempting.  Indexes Build indexes on frequently queried database fields.  You can almost always trade space for speed.Avoid lookups  Inside of the loop to be optimized, I avoid having to do any lookups.  Find the offset and/or index outside of the loop and reuse the data inside.Minimize IO try to design in a manner that reduces the number of times you have to read or write especially over a networked connectionReduce Abstractions The more layers of abstraction the code has to work through, the slower it is.  Inside the critical loop, reduce abstractions (e.g. reveal lower-level methods that avoid extra code)Spawn Threads for projects with a user interface, spawning a new thread to preform slower tasks makes the application feel more responsive, although isn\'t.Pre-process You can generally trade space for speed.  If there are calculations or other intense operations, see if you can precompute some of the information before you\'re in the critical loop.Sometimes changing the layout of your data can help. In C, you might switch from an array or structures to a structure of arrays, or vice versa.Tweak the OS and framework.It may sound an overkill but think about it like this: Operating Systems and Frameworks are designed to do many things. Your application only does very specific things. If you could get the OS do to exactly what your application needs and have your application understand how the the framework (php,.net,java) works, you could get much better out of your hardware.Facebook, for example, changed some kernel level thingys in Linux, changed how memcached works (for example they wrote a memcached proxy, and used udp instead of tcp).Another example for this is Window2008. Win2K8 has a version were you can install just the basic OS needed to run X applicaions (e.g. Web-Apps, Server Apps). This reduces much of the overhead that the OS have on running processes and gives you better performance.Of course, you should always throw in more hardware as the first step...pass by reference instead of by valueReduce variable sizes (in embedded systems)If your variable size is larger than the word size on a specific architecture, it can have a significant effect on both code size and speed. For example, if you have a 16 bit system, and use a long int variable very often, and later realize that it can never get outside the range (\xe2\x88\x9232.768 ... 32.767) consider reducing it to short int.From my personal experience, if a program is ready or almost ready, but we realize it takes up about 110% or 120% of the target hardware\'s program memory, a quick normalization of variables usually solves the problem more often than not. By this time, optimizing the algorithms or parts of the code itself can become frustratingly futile: Many people make the mistake of having variables which exactly store the numerical value of a unit they use the variable for: for example, their variable time stores the exact number of milliseconds, even if only time steps of say 50 ms are relevant. Maybe if your variable represented 50 ms for each increment of one, you would be able to fit into a variable smaller or equal to the word size. On an 8 bit system, for example, even a simple addition of two 32-bit variables generates a fair amount of code, especially if you are low on registers, while 8 bit additions are both small and fast.