Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this:(Thanks to unwind for that code sample.)But I\'d like to avail myself of a built-in or a more Pythonic idiom if possible.Related question: In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique while preserving order?Here you have some alternatives: http://www.peterbe.com/plog/uniqifiers-benchmarkFastest one:Why assign seen.add to seen_add instead of just calling seen.add? Python is a dynamic language, and resolving seen.add each iteration is more costly than resolving a local variable. seen.add could have changed between iterations, and the runtime isn\'t smart enough to rule that out. To play it safe, it has to check the object each time.If you plan on using this function a lot on the same dataset, perhaps you would be better off with an ordered set: http://code.activestate.com/recipes/528878/O(1) insertion, deletion and member-check per operation.Edit 2016As Raymond pointed out, in python 3.5+ where OrderedDict is implemented in C, the list comprehension approach will be slower than OrderedDict (unless you actually need the list at the end - and even then, only if the input is very short). So the best solution for 3.5+ is OrderedDict.Important Edit 2015As @abarnert notes, the more_itertools library (pip install more_itertools) contains a unique_everseen function that is built to solve this problem without any unreadable (not seen.add) mutations in list comprehensions. This is also the fastest solution too:Just one simple library import and no hacks. \nThis comes from an implementation of the itertools recipe unique_everseen which looks like:In Python 2.7+ the accepted common idiom (this works but isn\'t optimized for speed, i would now use unique_everseen) for this uses collections.OrderedDict:Runtime: O(N)This looks much nicer than:and doesn\'t utilize the ugly hack:which relies on the fact that set.add is an in-place method that always returns None so not None evaluates to True. Note however that the hack solution is faster in raw speed though it has the same runtime complexity O(N).unique \xe2\x86\x92 [\'1\', \'2\', \'3\', \'6\', \'4\', \'5\']The list doesn\'t even have to be sorted, the sufficient condition is that equal values are grouped together.Edit: I assumed that "preserving order" implies that the list is actually ordered. If this is not the case, then the solution from MizardX is the right one.Community edit: This is however the most elegant way to "compress duplicate consecutive elements into a single element".As of Python 2.7, the new way for removing duplicates from an iterable while keeping it in the original order is:And as of Python 3.5 when OrderedDict was reimplemented in C, this is now the fastest approach as well as the shortest and cleanest.[Update] As of CPython 3.6, dicts are compact and ordered.  Though the ordering behavior is not guaranteed as of yet, it will not change in Python 3.6.  So, you can use list(dict.fromkeys(\'abracadabra\')).I think if you wanna maintain the order,For another very late answer to another very old question:The itertools recipes have a function that does this, using the seen set technique, but:Is it actually faster than f7? It depends on your data, so you\'ll have to test it and see. If you want a list in the end, f7 uses a listcomp, and there\'s no way to do that here. (You can directly append instead of yielding, or you can feed the generator into the list function, but neither one can be as fast as the LIST_APPEND inside a listcomp.) At any rate, usually, squeezing out a few microseconds is not going to be as important as having an easily-understandable, reusable, already-written function that doesn\'t require DSU when you want to decorate.As with all of the recipes, it\'s also available in more-iterools.If you just want the no-key case, you can simplify it as:For no hashable types (e.g. list of lists), based on MizardX\'s:5 x faster reduce variant but more sophisticatedExplanation:You can reference a list comprehension as it is being built by the symbol \'_[1]\'. For example, the following function unique-ifies a list of elements without changing their order by referencing its list comprehension.Demo:Output:Just to add another (very performant) implementation of such a functionality from an external module1: iteration_utilities.unique_everseen:Superficial timings (Python 3.5) show that it\'s faster than all other alternatives I tested, including OrderedDict.fromkeys, f7 and more_itertools.unique_everseen:This (generator-) function can also handle unhashable values in the input (with an O(n*n) performance instead of the O(n) performance when the values are hashable).1 Disclaimer: I\'m the author of that package.Not to kick a dead horse (this question is very old and already has lots of good answers), but here is a solution using pandas that is quite fast in many circumstances and is dead simple to use.  Borrowing the recursive idea used in definining Haskell\'s nub function for lists, this would be a recursive approach:e.g.:I tried it for growing data sizes and saw sub-linear time-complexity (not definitive, but suggests this should be fine for normal data).I also think it\'s interesting that this could be readily generalized to uniqueness by other operations. Like this:For example, you could pass in a function that uses the notion of rounding to the same integer as if it was "equality" for uniqueness purposes, like this:then unique(some_list, test_round) would provide the unique elements of the list where uniqueness no longer meant traditional equality (which is implied by using any sort of set-based or dict-key-based approach to this problem) but instead meant to take only the first element that rounds to K for each possible integer K that the elements might round to, e.g.:You could do a sort of ugly list comprehension hack.A generator expression that uses the O(1) look up of a set to determine whether or not to include an element in the new list.MizardX\'s answer gives a good collection of multiple approaches.This is what I came up with while thinking aloud:Relatively effective approach with _sorted_ a numpy arrays:Outputs:A simple recursive solution:If you need one liner then maybe this would help:... should work but correct me if i\'m wrongThis is fast but...... it doesn\'t work if your list items aren\'t hashable.A more generic approach is:... it should work for all cases.Pop the duplicate in a list and hold uniques in source list :I use [::-1] for read list in reverse order.If your situation allows, you might consider removing duplicates as you load:Say you have a loop that is pulling in data and uses list1.append(item)...That gives you some duplicates:\n [0, 2, 4, 9, 0, 1, 2, 3, 4, 5, 6]But if you did:You get no duplicates and the order is preserved:\n [0, 2, 4, 9, 1, 3, 5, 6]Because I was looking at a dup and collected some related but different, related, useful information that isn\'t part of the other answers, here are two other possible solutions. .get(True) XOR .setdefault(False)The first is very much like the accepted seen_add soultion but with explicit side effects using dictionary\'s get(x,<default>) and setdefault(x,<default>):get(x,<default>) returns <default> if x is not in the dictionary, but does not add the key to the dictionary. set(x,<default>) returns the value if the key is in the dictionary,  otherwise sets it to and returns <default>.Aside: a != b is how to do an XOR in python__OVERRIDING ___missing_____ (inspired by this answer)The second technique is overriding the __missing__ method that gets called when the key doesn\'t exist in a dictionary, which is only called when using d[k] notation:From the docs:New in version 2.5: If a subclass of dict defines a method\n  _____missing_____(), if the key key is not present, the d[key] operation calls that method with the key key as argument. The d[key] operation\n  then returns or raises whatever is returned or raised by the\n  _____missing_____(key) call if the key is not present. No other operations or methods invoke _____missing_____(). If _____missing_____() is not defined,\n  KeyError is raised. _____missing_____() must be a method; it cannot be an\n  instance variable. For an example, see collections.defaultdict.Here is an O(N2) recursive version for fun:If you routinely use pandas, and aesthetics is preferred over performance, then consider the built-in function pandas.Series.drop_duplicates:Timing: this will preserve order and run in O(n) time. basically the idea is to create a hole wherever there is a duplicate found and sink it down to the bottom. makes use of a read and write pointer. whenever a duplicate is found only the read pointer advances and write pointer stays on the duplicate entry to overwrite it.A solution without using imported modules or sets:Gives output:Here is my 2 cents on this:Regards,\nYuriythis is the smartes way to remove duplicates from a list in Python whilst preserving its order, you can even do it in one line of code:My buddy Wes gave me this sweet answer using list comprehensions.Example Code: