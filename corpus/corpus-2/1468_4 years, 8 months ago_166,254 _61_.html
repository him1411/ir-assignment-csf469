I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it\'s out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.One day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I\'m not talking about "big data" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.My first thought is to use HDFStore to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:What are some best-practice workflows for accomplishing the following:Real-world examples would be much appreciated, especially from anyone who uses pandas on "large data".Edit -- an example of how I would like this to work:I am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.Edit -- Responding to Jeff\'s questions specifically:It is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).I routinely use tens of gigabytes of data in just this fashion\ne.g. I have tables on disk that I read via queries, create data and append back.It\'s worth reading the docs and late in this thread for several suggestions for how to store your data.Details which will affect how you store your data, like:\nGive as much detail as you can; and I can help you develop a structure.Ensure you have pandas at least 0.10.1 installed.Read iterating files chunk-by-chunk and multiple table queries.Since pytables is optimized to operate on row-wise (which is what you query on), we will create a table for each group of fields. This way it\'s easy to select a small group of fields (which will work with a big table, but it\'s more efficient to do it this way... I think I may be able to fix this limitation in the future... this is more intuitive anyhow):\n(The following is pseudocode.)Reading in the files and creating the storage (essentially doing what append_to_multiple does):Now you have all of the tables in the file (actually you could store them in separate files if you wish, you would prob have to add the filename to the group_map, but probably this isn\'t necessary).This is how you get columns and create new ones:When you are ready for post_processing:About data_columns, you don\'t actually need to define ANY data_columns; they allow you to sub-select rows based on the column. E.g. something like:They may be most interesting to you in the final report generation stage (essentially a data column is segregated from other columns, which might impact efficiency somewhat if you define a lot).You also might want to:Let me know when you have questions!I think the answers above are missing a simple approach that I\'ve found very useful. When I have a file that is too large to load in memory, I break up the file into multiple smaller files (either by row or cols)Example: In case of 30 days worth of trading data of ~30GB size, I break it into a file per day of ~1GB size. I subsequently process each file separately and aggregate results at the endOne of the biggest advantages is that it allows parallel processing of the files (either multiple threads or processes)The other advantage is that file manipulation (like adding/removing dates in the example) can be accomplished by regular shell commands, which is not be possible in more advanced/complicated file formatsThis approach doesn\'t cover all scenarios, but is very useful in a lot of themThis is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (dict of attributes).  Many people form a collection and you can have many collections (people, stock market, income).pd.dateframe -> pymongo Note: I use the chunksize in read_csv to keep it to 5 to 10k records(pymongo drops the socket if larger)querying: gt = greater than....find() returns an iterator so I commonly use ichunked to chop into smaller iterators.  How about a join since I normally get 10 data sources to paste together:then (in my case sometimes I have to agg on aJoinDF first before its "mergeable".)And you can then write the new info to your main collection via the update method below. (logical collection vs physical datasources).On smaller lookups, just denormalize.  For example, you have code in the document and you just add the field code text and do a dict lookup as you create documents.Now you have a nice dataset based around a person, you can unleash your logic on each case and make more attributes. Finally you can read into pandas your 3 to memory max key indicators and do pivots/agg/data exploration.  This works for me for 3 million records with numbers/big text/categories/codes/floats/...You can also use the two methods built into MongoDB (MapReduce and aggregate framework). See here for more info about the aggregate framework, as it seems to be easier than MapReduce and looks handy for quick aggregate work.  Notice I didn\'t need to define my fields or relations, and I can add items to a document.  At the current state of the rapidly changing numpy, pandas, python toolset, MongoDB helps me just get to work :)If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you\'re looking for here, but doing scientific computing on a notebook with 4GB of RAM isn\'t reasonable.I know this is an old thread but I think the Blaze library is worth checking out.  It\'s built for these types of situations.From the docs:Blaze extends the usability of NumPy and Pandas to distributed and out-of-core computing. Blaze provides an interface similar to that of the NumPy ND-Array or Pandas DataFrame but maps these familiar interfaces onto a variety of other computational engines like Postgres or Spark.Edit: By the way, it\'s supported by ContinuumIO and Travis Oliphant, author of NumPy.I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.My basic workflow is to first get a CSV file from the database. I gzip it, so it\'s not as huge. Then I convert that to a row-oriented HDF5 file, by iterating over it in python, converting each row to a real data type, and writing it to a HDF5 file. That takes some tens of minutes, but it doesn\'t use any memory, since it\'s only operating row-by-row. Then I "transpose" the row-oriented HDF5 file into a column-oriented HDF5 file.The table transpose looks like:Reading it back in then looks like:Now, I generally run this on a machine with a ton of memory, so I may not be careful enough with my memory usage. For example, by default the load operation reads the whole data set.This generally works for me, but it\'s a bit clunky, and I can\'t use the fancy pytables magic.Edit: The real advantage of this approach, over the array-of-records pytables default, is that I can then load the data into R using h5r, which can\'t handle tables. Or, at least, I\'ve been unable to get it to load heterogeneous tables.There is now, two years after the question, an \'out-of-core\' pandas equivalent: dask. It is excellent! Though it does not support all of pandas functionality, you can get really far with it.One more variationMany of the operations done in pandas can also be done as a db query (sql, mongo)Using a RDBMS or mongodb allows you to perform some of the aggregations in the DB Query (which is optimized for large data, and uses cache and indexes efficiently)Later, you can perform post processing using pandas.The advantage of this method is that you gain the DB optimizations for working with large data, while still defining the logic in a high level declarative syntax - and not having to deal with the details of deciding what to do in memory and what to do out of core.And although the query language and pandas are different, it\'s usually not complicated to translate part of the logic from one to another.Consider Ruffus if you go the simple path of creating a data pipeline which is broken down into multiple smaller files. One trick I found helpful for "large data" use cases is to reduce the volume of the data by reducing float precision to 32-bit. It\'s not applicable in all cases, but in many applications 64-bit precision is overkill and the 2x memory savings are worth it. To make an obvious point even more obvious:I recently came across a similar issue. I found simply reading the data in chunks and appending it as I write it in chunks to the same csv works well. My problem was adding a date column based on information in another table, using the value of certain columns as follows. This may help those confused by dask and hdf5 but more familiar with pandas like myself. 