As the title suggests, I\'d like to select the first row of each set of rows grouped with a GROUP BY.Specifically, if I\'ve got a purchases table that looks like this:I\'d like to query for the id of the largest purchase (total) made by each customer. Something like this:But you need to add logic to break ties:In PostgreSQL this is typically simpler and faster (more performance optimization below):Or shorter (if not as clear) with ordinal numbers of output columns:If total can be NULL (won\'t hurt either way, but you\'ll want to match existing indexes):DISTINCT ON is a PostgreSQL extension of the standard (where only DISTINCT on the whole SELECT list is defined).List any number of expressions in the DISTINCT ON clause, the combined row value defines duplicates. The manual:Obviously, two rows are considered distinct if they differ in at least\n  one column value. Null values are considered equal in this comparison.Bold emphasis mine.DISTINCT ON can be combined with ORDER BY. Leading expressions have to match leading DISTINCT ON expressions in the same order. You can add additional expressions to ORDER BY to pick a particular row from each group of peers. I added id as last item to break ties:"Pick the row with the smallest id from each group sharing the highest total."If total can be NULL, you most probably want the row with the greatest non-null value. Add NULLS LAST like demonstrated. Details:The SELECT list is not constrained by expressions in DISTINCT ON or ORDER BY in any way. (Not needed in the simple case above):You don\'t have to include any of the expressions in DISTINCT ON or ORDER BY.You can include any other expression in the SELECT list. This is instrumental for replacing much more complex queries with subqueries and aggregate / window functions.I tested with versions 8.3 \xe2\x80\x93 10. But the feature has been there at least since version 7.1, so basically always.The perfect index for the above query would be a multi-column index spanning all three columns in matching sequence and with matching sort order:May be too specialized for real world applications. But use it if read performance is crucial. If you have DESC NULLS LAST in the query, use the same in the index so Postgres knows sort order matches.You have to weigh cost and benefit before you create a tailored index for every query. The potential of above index largely depends on data distribution.The index is used because it delivers pre-sorted data, and in Postgres 9.2 or later the query can also benefit from an index only scan if the index is smaller than the underlying table. The index has to be scanned in its entirety, though.For few rows per customer, this is very efficient (even more so if you need sorted output anyway). The benefit shrinks with a growing number of rows per customer.\nIdeally, you have enough work_mem to process the involved sort step in RAM and not spill to disk. Generally setting work_mem too high can have adverse effects. Consider SET LOCAL for singular queries on big sets.\nFind how much you need with EXPLAIN ANALYZE. Mention of "Disk:" in the sort step indicates the need for more:For many rows per customer, a loose index scan would be (much) more efficient, but that\'s not currently implemented in Postgres (up to 9.5).\nThere are faster query techniques to substitute for this. In particular if you have a separate table holding unique customers, which is the typical use case. But also if you don\'t:I had a simple benchmark here for Postgres 9.1, which was outdated by 2016. So I ran a new one with a better, reproducible setup for Postgres 9.4 and 9.5 and added the detailed results in another answer.Testing the most interesting candidates with Postgres 9.4 and 9.5 with a halfway realistic table of 200k rows in purchases and 10k distinct customer_id (avg. 20 rows per customer).For Postgres 9.5 I ran a 2nd test with effectively 86446 distinct customers. See below (avg. 2.3 rows per customer).Main tableI use a serial (PK constraint added below) and an integer customer_id since that\'s a more typical setup. Also added some_column to make up for typically more columns.Dummy data, PK, index - a typical table also has some dead tuples:customer table - for superior queryIn my second test for 9.5 I used the same setup, but with random() * 100000 to generate customer_id to get only few rows per customer_id.Generated with this query.Execution time for above queries with EXPLAIN ANALYZE (and all options off), best of 5 runs.All queries used an Index Only Scan on purchases2_3c_idx (among other steps). Some of them just for the smaller size of the index, others more effectively.I ran three tests with PostgreSQL 9.1 on a real life table of 65579 rows and single-column btree indexes on each of the three columns involved and took the best execution time of 5 runs.\nComparing @OMGPonies\' first query (A) to the above DISTINCT ON solution (B):Select the whole table, results in 5958 rows in this case.Use condition WHERE customer BETWEEN x AND y resulting in 1000 rows.Select a single customer with WHERE customer = x.Same test repeated with the index described in the other answerThis is common greatest-n-per-group problem, which has already well tested and highly optimized solutions. Personally I prefer the left join solution by Bill Karwin (the original post with lots of other solutions).Note that bunch of solutions to this common problem can surprisingly be found in the one of most official sources, MySQL manual! See Examples of Common Queries :: The Rows Holding the Group-wise Maximum of a Certain Column.In Postgres you can use array_agg like this:This will give you the id of each customer\'s largest purchase.Some things to note:The solution is not very efficient as pointed by Erwin, because of presence of SubQsI use this way (postgresql only): https://wiki.postgresql.org/wiki/First/last_%28aggregate%29Then your example should work almost as is:CAVEAT: It ignore\'s NULL rowsNow I use this way: http://pgxn.org/dist/first_last_agg/To install on ubuntu 14.04:It\'s a postgres extension that gives you first and last functions; apparently faster than the above way.If you use aggregate functions (like these), you can order the results, without the need to have the data already ordered:So the equivalent example, with ordering would be something like:Of course you can order and filter as you deem fit within the aggregate; it\'s very powerful syntax.Very fast solutionand really very fast if table is indexed by id:The accepted OMG Ponies\' "Supported by any database" solution has good speed from my test.Here I provide a same-approach, but more complete and clean any-database solution.   Ties are considered (assume desire to get only one row for each customer, even multiple records for max total per customer), and other purchase fields (e.g. purchase_payment_id) will be selected for the real matching rows in the purchase table.Supported by any database:This query is reasonably fast especially when there is a composite index like (customer, total) on the purchase table.Remark:t1, t2 are subquery alias which could be removed depending on database.Caveat: the using (...) clause is currently not supported in MS-SQL and Oracle db as of this edit on Jan 2017. You have to expand it yourself to e.g. on t2.id = purchase.id etc.  The USING syntax works in SQLite, MySQL and PostgreSQL.