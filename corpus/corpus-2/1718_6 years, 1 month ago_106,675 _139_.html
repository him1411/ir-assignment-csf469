I have taken Problem #12 from Project Euler as a programming exercise and to compare my (surely not optimal) implementations in C, Python, Erlang and Haskell. In order to get some higher execution times, I search for the first triangle number with more than 1000 divisors instead of 500 as stated in the original problem.The result is the following:C:Python:Python with PyPy:Erlang:Haskell:Summary:I suppose that C has a big advantage as it uses long for the calculations and not arbitrary length integers as the other three. Also it doesn\'t need to load a runtime first (Do the others?).Question 1:\nDo Erlang, Python and Haskell lose speed due to using arbitrary length integers or don\'t they as long as the values are less than MAXINT?Question 2:\nWhy is Haskell so slow? Is there a compiler flag that turns off the brakes or is it my implementation? (The latter is quite probable as Haskell is a book with seven seals to me.)Question 3:\nCan you offer me some hints how to optimize these implementations without changing the way I determine the factors? Optimization in any way: nicer, faster, more "native" to the language.EDIT:Question 4:\nDo my functional implementations permit LCO (last call optimization, a.k.a tail recursion elimination) and hence avoid adding unnecessary frames onto the call stack?I really tried to implement the same algorithm as similar as possible in the four languages, although I have to admit that my Haskell and Erlang knowledge is very limited.Source codes used:Using GHC 7.0.3, gcc 4.4.6, Linux 2.6.29 on an x86_64 Core2 Duo (2.5GHz) machine, compiling using ghc -O2 -fllvm -fforce-recomp for Haskell and gcc -O3 -lm for C.That\'s right, 7.95 seconds.  Consistently half a second faster than the C solution.  Without the -fllvm flag I\'m still getting 8.182 seconds, so the NCG backend is doing well in this case too.Conclusion: Haskell is awesome.Resulting CodeEDIT: So now that we\'ve explored that, lets address the questionsQuestion 1: Do erlang, python and haskell lose speed due to using\n  arbitrary length integers or don\'t they as long as the values are less\n  than MAXINT?In Haskell, using Integer is slower than Int but how much slower depends on the computations performed.  Luckily (for 64 bit machines) Int is sufficient.  For portability sake you should probably rewrite my code to use Int64 or Word64 (C isn\'t the only language with a long).Question 2: Why is haskell so slow? Is there a compiler flag that\n  turns off the brakes or is it my implementation? (The latter is quite\n  probable as haskell is a book with seven seals to me.)Question 3: Can you offer me some hints how to optimize these\n  implementations without changing the way I determine the factors?\n  Optimization in any way: nicer, faster, more "native" to the language.That was what I answered above.   The answer was Question 4: Do my functional implementations permit LCO and hence\n  avoid adding unnecessary frames onto the call stack?Yes, that wasn\'t the issue.  Good work and glad you considered this.There are some problems with the Erlang implementation. As baseline for the following, my measured execution time for your unmodified Erlang program was 47.6 seconds, compared to 12.7 seconds for the C code.The first thing you should do if you want to run computationally intensive Erlang code is to use native code. Compiling with erlc +native euler12 got the time down to 41.3 seconds. This is however a much lower speedup (just 15%) than expected from native compilation on this kind of code, and the problem is your use of -compile(export_all). This is useful for experimentation, but the fact that all functions are potentially reachable from the outside causes the native compiler to be very conservative. (The normal BEAM emulator is not that much affected.) Replacing this declaration with -export([solve/0]). gives a much better speedup: 31.5 seconds (almost 35% from the baseline).But the code itself has a problem: for each iteration in the factorCount loop, you perform this test:The C code doesn\'t do this. In general, it can be tricky to make a fair comparison between different implementations of the same code, and in particular if the algorithm is numerical, because you need to be sure that they are actually doing the same thing. A slight rounding error in one implementation due to some typecast somewhere may cause it to do many more iterations than the other even though both eventually reach the same result.To eliminate this possible error source (and get rid of the extra test in each iteration), I rewrote the factorCount function as follows, closely modelled on the C code:This rewrite, no export_all, and native compilation, gave me the following run time:which is not too bad compared to the C code:considering that Erlang is not at all geared towards writing numerical code, being only 50% slower than C on a program like this is pretty good.Finally, regarding your questions:Question 1: Do erlang, python and haskell loose speed due to using arbitrary length integers or \ndon\'t they as long as the values are less than MAXINT?Yes, somewhat. In Erlang, there is no way of saying "use 32/64-bit arithmetic with wrap-around", so unless the compiler can prove some bounds on your integers (and it usually can\'t), it must check all computations to see if they can fit in a single tagged word or if it has to turn them into heap-allocated bignums. Even if no bignums are ever used in practice at runtime, these checks will have to be performed. On the other hand, that means you know that the algorithm will never fail because of an unexpected integer wraparound if you suddenly give it larger inputs than before.Question 4: Do my functional implementations permit LCO and hence avoid adding unnecessary frames onto the call stack?Yes, your Erlang code is correct with respect to last call optimization.In regards to Python optimization, in addition to using PyPy (for pretty impressive speed-ups with zero change to your code), you could use PyPy\'s translation toolchain to compile an RPython-compliant version, or Cython to build an extension module, both of which are faster than the C version in my testing, with the Cython module nearly twice as fast. For reference I include C and PyPy benchmark results as well:C (compiled with gcc -O3 -lm)PyPy 1.5RPython (using latest PyPy revision, c2f583445aee)Cython 0.15The RPython version has a couple of key changes. To translate into a standalone program you need to define your target, which in this case is the main function. It\'s expected to accept sys.argv as it\'s only argument, and is required to return an int. You can translate it by using translate.py, % translate.py euler12-rpython.py which translates to C and compiles it for you.The Cython version was rewritten as an extension module _euler12.pyx, which I import and call from a normal python file. The _euler12.pyx is essentially the same as your version, with some additional static type declarations. The setup.py has the normal boilerplate to build the extension, using python setup.py build_ext --inplace.I honestly have very little experience with either RPython or Cython, and was pleasantly surprised at the results. If you are using CPython, writing your CPU-intensive bits of code in a Cython extension module seems like a really easy way to optimize your program.Question 3: Can you offer me some hints how to optimize these implementations\n  without changing the way I determine the factors? Optimization in any\n  way: nicer, faster, more "native" to the language.The C implementation is suboptimal (as hinted at by Thomas M. DuBuisson), the version uses 64-bit integers (i.e. long datatype).  I\'ll investigate the assembly listing later, but with an educated guess, there are some memory accesses going on in the compiled code, which make using 64-bit integers significantly slower. It\'s that or generated code (be it the fact that you can fit less 64-bit ints in a SSE register or round a double to a 64-bit integer is slower).Here is the modified code (simply replace long with int and I explicitly inlined factorCount, although I do not think that this is necessary with gcc -O3):Running + timing it gives:For reference, the haskell implementation by Thomas in the earlier answer gives:Conclusion: Taking nothing away from ghc, its a great compiler, but gcc normally generates faster code.Take a look at this blog. Over the past year or so he\'s done a few of the Project Euler problems in Haskell and Python, and he\'s generally found Haskell to be much faster. I think that between those languages it has more to do with your fluency and coding style.When it comes to Python speed, you\'re using the wrong implementation! Try PyPy, and for things like this you\'ll find it to be much, much faster.Your Haskell implementation could be greatly sped up by using some functions from Haskell packages.\nIn this case I used primes, which is just installed with \'cabal install primes\' ;)Timings:Your original program:Improved implementationAs you can see, this one runs in 38 milliseconds on the same machine where yours ran in 16 seconds :)Compilation commands:Just for fun. The following is a more \'native\' Haskell implementation:Using ghc -O3, this consistently runs in 0.55-0.58 seconds on my machine (1.73GHz Core i7).A more efficient factorCount function for the C version:Changing longs to ints in main, using gcc -O3 -lm, this consistently runs in 0.31-0.35 seconds.Both can be made to run even faster if you take advantage of the fact that the nth triangle number = n*(n+1)/2, and n and (n+1) have completely disparate prime factorizations, so the number of factors of each half can be multiplied to find the number of factors of the whole. The following:will reduce the c code run time to 0.17-0.19 seconds, and it can handle much larger searches -- greater than 10000 factors takes about 43 seconds on my machine. I leave a similar haskell speedup to the interested reader.This is unlikely. I cannot say much about Erlang and Haskell (well, maybe a bit about Haskell below) but I can point a lot of other bottlenecks in Python. Every time the program tries to execute an operation with some values in Python, it should verify whether the values are from the proper type, and it costs a bit of time. Your factorCount function just allocates a list with range (1, isquare + 1) various times, and runtime, malloc-styled memory allocation is way slower than iterating on a range with a counter as you do in C. Notably, the factorCount() is called multiple times and so allocates a lot of lists. Also, let us not forget that Python is interpreted and the CPython interpreter has no great focus on being optimized.EDIT: oh, well, I note that you are using Python 3 so range() does not return a list, but a generator. In this case, my point about allocating lists is half-wrong: the function just allocates range objects, which are inefficient nonetheless but not as inefficient as allocating a list with a lot of items.Are you using Hugs? Hugs is a considerably slow interpreter. If you are using it, maybe you can get a better time with GHC - but I am only cogitating hypotesis, the kind of stuff a good Haskell compiler does under the hood is pretty fascinating and way beyond my comprehension :)I\'d say you are playing an unfunny game. The best part of knowing various languages is to use them the most different way possible :) But I digress, I just do not have any recommendation for this point. Sorry, I hope someone can help you in this case :)As far as I remember, you just need to make sure that your recursive call is the last command before returning a value. In other words, a function like the one below could use such optimization:However, you would not have such optimization if your function were such as the one below, because there is an operation (multiplication) after the recursive call:I separated the operations in some local variables for make it clear which operations are executed. However, the most usual is to see these functions as below, but they are equivalent for the point I am making:Note that it is up to the compiler/interpreter to decide if it will make tail recursion. For example, the Python interpreter does not do it if I remember well (I used Python in my example only because of its fluent syntax). Anyway, if you find strange stuff such as factorial functions with two parameters (and one of the parameters has names such as acc, accumulator etc.) now you know why people do it :) With Haskell, you really don\'t need to think in recursions explicitly.In the above code, I have replaced explicit recursions in @Thomas\' answer with common list operations.  The code still does exactly the same thing without us worrying about tail recursion.  It runs (~ 7.49s) about 6% slower than the version in @Thomas\' answer (~ 7.04s) on my machine with GHC 7.6.2, while the C version from @Raedwulf runs ~ 3.15s.  It seems GHC has improved over the year.PS. I know it is an old question, and I stumble upon it from google searches (I forgot what I was searching, now...).  Just wanted to comment on the question about LCO and express my feelings about Haskell in general.  I wanted to comment on the top answer, but comments do not allow code blocks.Looking at your Erlang implementation. The timing has included the start up of the entire virtual machine, running your program and halting the virtual machine. Am pretty sure that setting up and halting the erlang vm takes some time.If the timing was done within the erlang virtual machine itself, results would be different as in that case we would have the actual time for only the program in question. Otherwise, i believe that the total time taken by the process of starting and loading of the Erlang Vm plus that of halting it (as you put it in your program) are all included in the total time which the method you are using to time the program is outputting. Consider using the erlang timing itself which we use when we want to time our programs within the virtual machine itself\ntimer:tc/1 or timer:tc/2 or timer:tc/3. In this way, the results from erlang will exclude the time taken to start and stop/kill/halt the virtual machine. That is my reasoning there, think about it, and then try your bench mark again.I actually suggest that we try to time the program (for languages that have a runtime), within the runtime of those languages in order to get a precise value. C for example has no overhead of starting and shutting down a runtime system as does Erlang, Python and Haskell (98% sure of this - i stand correction). So (based on this reasoning) i conclude by saying that this benchmark wasnot precise /fair enough for languages running on top of a runtime system. Lets do it again with these changes.EDIT: besides even if all the languages had runtime systems, the overhead of starting each and halting it would differ. so i suggest we time from within the runtime systems (for the languages for which this applies). The Erlang VM is known to have considerable overhead at start up!Question 1: Do Erlang, Python and Haskell lose speed due to using\n  arbitrary length integers or don\'t they as long as the values are less\n  than MAXINT?Question one can be answered in the negative for Erlang. The last question is answered by using Erlang appropriately, as in:http://bredsaal.dk/learning-erlang-using-projecteuler-netSince it\'s faster than your initial C example, I would guess there are numerous problems as others have already covered in detail.This Erlang module executes on a cheap netbook in about 5 seconds ... It uses the network threads model in erlang and, as such demonstrates how to take advantage of the event model. It could be distributed over many nodes. And it\'s fast. Not my code.The test below took place on an: Intel(R) Atom(TM) CPU N270   @ 1.60GHzSome more numbers and explanations for the C version. Apparently noone did it during all those years. Remember to upvote this answer so it can get on top for everyone to see and learn.Laptop Specifications:Commands:.Filenames are: integertype_architecture_compiler.exeVS is 250% faster than gcc. The two compilers should give a similar speed. Obviously, something is wrong with the code or the compiler options. Let\'s investigate!The first point of interest is the integer types. Conversions can be expensive and consistency is important for better code generation & optimizations. All integers should be the same type. It\'s a mixed mess of int and long right now. We\'re going to improve that. What type to use? The fastest. Gotta benchmark them\'all!Integer types are int long int32_t uint32_t int64_t and uint64_t from #include <stdint.h>There are LOTS of integer types in C, plus some signed/unsigned to play with, plus the choice to compile as x86 or x64 (not to be confused with the actual integer size). That is a lot of versions to compile and run ^^Definitive conclusions:Trick question: "What are the sizes of int and long in C?"\nThe right answer is: The size of int and long in C are not well-defined!From the C spec:int is at least 32 bits \n    long is at least an intFrom the gcc man page (-m32 and -m64 flags):The 32-bit environment sets int, long and pointer to 32 bits and generates code that runs on any i386 system. \n    The 64-bit environment sets int to 32 bits and long and pointer to 64 bits and generates code for AMD\xe2\x80\x99s x86-64 architecture.From MSDN documentation (Data Type Ranges) https://msdn.microsoft.com/en-us/library/s3f49ktz%28v=vs.110%29.aspx :int,  4 bytes, also knows as signed \n    long, 4 bytes, also knows as long int and signed long int32 bits integers are faster than 64 bits integers.Standard integers types are not well defined in C nor C++, they vary depending on compilers and architectures. When you need consistency and predictability, use the uint32_t integer family from #include <stdint.h>.Speed issues solved. All other languages are back hundreds percent behind, C & C++ win again ! They always do. The next improvement will be multithreading using OpenMP :DC++11, < 20ms for me - Run it hereI understand that you want tips to help improve your language specific knowledge, but since that has been well covered here, I thought I would add some context for people who may have looked at the mathematica comment on your question, etc, and wondered why this code was so much slower.This answer is mainly to provide context to hopefully help people evaluate the code in your question / other answers more easily.This code uses only a couple of (uglyish) optimisations, unrelated to the language used, based on:That takes around 19ms on average for my desktop and 80ms for my laptop, a far cry from most of the other code I\'ve seen here. And there are, no doubt, many optimisations still available.Trying GO:I get:original c version: 9.1690  100%\ngo: 8.2520  111% But using:I get: original c version: 9.1690  100%\nthaumkid\'s c version:   0.1060  8650%\nfirst  go version:  8.2520  111%\nsecond go   version: 0.0230 39865% I also tried Python3.6 and pypy3.3-5.5-alpha:original c version: 8.629   100%\nthaumkid\'s c version:   0.109   7916%\nPython3.6:         54.795   16%\npypy3.3-5.5-alpha:     13.291   65% and then with following code I got:original c version: 8.629   100%\nthaumkid\'s c version:   0.109   8650%\nPython3.6:          1.489   580%\npypy3.3-5.5-alpha:      0.582   1483% Change: case (divisor(T,round(math:sqrt(T))) > 500) ofTo:     case (divisor(T,round(math:sqrt(T))) > 1000) ofThis will produce the correct answer for the Erlang multi-process example.I made the assumption that the number of factors is only large if the numbers involved have many small factors. So I used thaumkid\'s excellent algorithm, but first used an approximation to the factor count that is never too small. It\'s quite simple: Check for prime factors up to 29, then check the remaining number and calculate an upper bound for the nmber of factors. Use this to calculate an upper bound for the number of factors, and if that number is high enough, calculate the exact number of factors. The code below doesn\'t need this assumption for correctness, but to be fast. It seems to work; only about one in 100,000 numbers gives an estimate that is high enough to require a full check.Here\'s the code: This finds the 14,753,024th triangular with 13824 factors in about 0.7 seconds, the 879,207,615th triangular number with 61,440 factors in 34 seconds, the 12,524,486,975th triangular number with 138,240 factors in 10 minutes 5 seconds, and the 26,467,792,064th triangular number with 172,032 factors in 21 minutes 25 seconds (2.4GHz Core2 Duo), so this code takes only 116 processor cycles per number on average. The last triangular number itself is larger than 2^68, so I modified "Jannich Brendle" version to 1000 instead 500. And list the result of euler12.bin, euler12.erl, p12dist.erl. Both erl codes use \'+native\' to compile.gcc -lm -Ofast euler.ctime ./a.out  2.79s user 0.00s system 99% cpu 2.794 totalThe fastest "high level" programming language is fortran. It is used for benchmark tests on supercomputers and is very fast at matrix multiplication and so on. I would suggest you to use cython it is sometimes even faster than C and much easier to code, cause lets face it python is just the best language to code. Hope I could help you out a little bit.