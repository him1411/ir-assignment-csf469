The QuestionHow to find time complexity of an algorithm?What have I done before posting a question on SO ?I have gone through this, this, this and many other linksBut no where I was able to find a clear and straight forward explanation for how to calculate time complexity.What do I know ?Say for a code as simple as the one below:Say for a loop like the one below: int i=0; This will be executed only once.\nThe time is actually calculated to i=0 and not the declaration.i < N;  This will be executed N+1 timesi++ ;   This will be executed N timesSo the number of operations required by this loop are{1+(N+1)+N} = 2N+2Note: This still may be wrong, as I am not confident about my understanding on calculating time complexityWhat I want to know ?Ok, so these small basic calculations I think I know, but in most cases I have seen the time complexity as O(N), O(n2), O(log n), O(n!).... and many other, Can anyone help me understand how does one calculate time complexity of an algorithm? I am sure there are plenty of newbies like me wanting to know this.How to find time complexity of an algorithmYou add up how many machine instructions it will execute as a function of the size of its input, and then simplify the expression to the largest (when N is very large) term  and can include any simplifying constant factor.For example, lets see how we simplify 2N + 2 machine instructions to describe this as just O(N).Why do we remove the two 2s ?We are interested in the performance of the algorithm as N becomes large.Consider the two terms 2N and 2. What is the relative influence of these two terms as N becomes large? Suppose N is a million.Then the first term is 2 million and the second term is only 2.For this reason, we drop all but the largest terms for large N.So, now we have gone from 2N + 2 to 2N.Traditionally, we are only interested in performance up to constant factors. This means that we don\'t really care if there is some constant multiple of difference in performance when N is large.  The unit of 2N is not well-defined in the first place anyway.  So we can multiply or divide by a constant factor to get to the simplest expression.So 2N becomes just N.This is an excellent article :\nhttp://www.daniweb.com/software-development/computer-science/threads/13488/time-complexity-of-algorithmThe below answer is copied from above (in case the excellent link goes bust)The most common metric for calculating time complexity is Big O notation. This removes all constant factors so that the running time can be estimated in relation to N as N approaches infinity. In general you can think of it like this:Is constant. The running time of the statement will not change in relation to N.Is linear. The running time of the loop is directly proportional to N. When N doubles, so does the running time.Is quadratic. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N * N.Is logarithmic. The running time of the algorithm is proportional to the number of times N can be divided by 2. This is because the algorithm divides the working area in half with each iteration.Is N * log ( N ). The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic. There are other Big O measures such as cubic, exponential, and square root, but they\'re not nearly as common. Big O notation is described as O (  ) where  is the measure. The quicksort algorithm would be described as O ( N * log ( N ) ).Note that none of this has taken into account best, average, and worst case measures. Each would have its own Big O notation. Also note that this is a VERY simplistic explanation. Big O is the most common, but it\'s also more complex that I\'ve shown. There are also other notations such as big omega, little o, and big theta. You probably won\'t encounter them outside of an algorithm analysis course. ;)Taken from here - Introduction to Time Complexity of an AlgorithmIn computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input.The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n, the asymptotic time complexity is O(n3). More on that later.Few more Examples:An algorithm is said to run in constant time if it requires the same amount of time regardless of the input size. Examples:An algorithm is said to run in linear time if its time execution is directly proportional to the input size, i.e. time grows linearly as input size increases.Consider the following examples, below I am linearly searching for an element, this has a time complexity of O(n).More Examples:An algorithm is said to run in logarithmic time if its time execution is proportional to the logarithm of the input size.Example: Binary SearchRecall the "twenty questions" game - the task is to guess the value of a hidden number in an interval. Each time you make a guess, you are told whether your guess is too high or too low. Twenty questions game implies a strategy that uses your guess number to halve the interval size. This is an example of the general problem-solving method known as binary searchAn algorithm is said to run in quadratic time if its time execution is proportional to the square of the input size. Examples:Although there are some good answers for this question. I would like to give another answer here with several examples of loop.O(n): Time Complexity of a loop is considered as O(n) if the loop variables is incremented / decremented by a constant amount. For example following functions have O(n) time complexity.O(n^c): Time complexity of nested loops is equal to the number of times the innermost statement is executed. For example the following sample loops have O(n^2) time complexityFor example Selection sort and Insertion Sort have O(n^2) time complexity.O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount.For example Binary Search has O(Logn) time complexity.O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount.One example of time complexity analysisAnalysis:\nFor i = 1, the inner loop is executed n times.\nFor i = 2, the inner loop is executed approximately n/2 times.\nFor i = 3, the inner loop is executed approximately n/3 times.\nFor i = 4, the inner loop is executed approximately n/4 times.\n\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6\xe2\x80\xa6.\nFor i = n, the inner loop is executed approximately n/n times.\nSo the total time complexity of the above algorithm is (n + n/2 + n/3 + \xe2\x80\xa6 + n/n), Which becomes n * (1/1 + 1/2 + 1/3 + \xe2\x80\xa6 + 1/n)The important thing about series (1/1 + 1/2 + 1/3 + \xe2\x80\xa6 + 1/n) is equal to O(Logn). So the time complexity of the above code is O(nLogn).Ref:\n1\n2\n31 - Basic Operations (arithmetic, comparisons, accessing array\xe2\x80\x99s elements, assignment) : The running time is always constant O(1)Example :2 - If then else statement: Only taking the maximum running time from two or more possible statements.Example:So, the complexity of the above pseudo code is T(n) = 2 + 1 + max(1, 1+2) = 6. Thus, its big oh is still constant T(n) = O(1).3 - Looping (for, while, repeat): Running time for this statement is the number of looping multiplied by the number of operations inside that looping.Example:So, its complexity is T(n) = 1+4n+1 = 4n + 2. Thus, T(n) = O(n).4 - Nested Loop (looping inside looping): Since there is at least one looping inside the main looping, running time of this statement used O(n^2) or O(n^3).Example:There are some common running times when analyzing an algorithm:O(1) \xe2\x80\x93 Constant Time\nConstant time means the running time is constant, it\xe2\x80\x99s not affected by the input size.O(n) \xe2\x80\x93 Linear Time\nWhen an algorithm accepts n input size, it would perform n operations as well.O(log n) \xe2\x80\x93 Logarithmic Time\nAlgorithm that has running time O(log n) is slight faster than O(n). Commonly, algorithm divides the problem into sub problems with the same size. Example: binary search algorithm, binary conversion algorithm.O(n log n) \xe2\x80\x93 Linearithmic Time\nThis running time is often found in "divide & conquer algorithms" which divide the problem into sub problems recursively and then merge them in n time. Example: Merge Sort algorithm.O(n2) \xe2\x80\x93 Quadratic Time\nLook Bubble Sort algorithm!O(n3) \xe2\x80\x93 Cubic Time\nIt has the same principle with O(n2).O(2n) \xe2\x80\x93 Exponential Time\nIt is very slow as input get larger, if n = 1000.000, T(n) would be 21000.000. Brute Force algorithm has this running time.O(n!) \xe2\x80\x93 Factorial Time\nTHE SLOWEST !!! Example : Travel Salesman Problem (TSP)Taken from this article. Very well explained should give a read.Loosely speaking, time complexity is a way of summarising how the number of operations or run-time of an algorithm grows as the input size increases.Like most things in life, a cocktail party can help us understand.O(N)When you arrive at the party, you have to shake everyone\'s hand (do an operation on every item). As the number of attendees N increases, the time/work it will take you to shake everyone\'s hand increases as O(N).Why O(N) and not cN?There\'s variation in the amount of time it takes to shake hands with people. You could average this out and capture it in a constant c. But the fundamental operation here --- shaking hands with everyone --- would always be proportional to O(N), no matter what c was. When debating whether we should go to a cocktail party, we\'re often more interested in the fact that we\'ll have to meet everyone than in the minute details of what those meetings look like.O(N^2)The host of the cocktail party wants you to play a silly game where everyone meets everyone else. Therefore, you must meet N-1 other people and, because the next person has already met you, they must meet N-2 people, and so on. The sum of this series is x^2/2+x/2. As the number of attendees grows, the x^2 term gets big fast, so we just drop everything else.O(N^3)You have to meet everyone else and, during each meeting, you must talk about everyone else in the room.O(1)The host wants to announce something. They ding a wineglass and speak loudly. Everyone hears them. It turns out it doesn\'t matter how many attendees there are, this operation always takes the same amount of time.O(log N)The host has laid everyone out at the table in alphabetical order. Where is Dan? You reason that he must be somewhere between Adam and Mandy (certainly not between Mandy and Zach!). Given that, is he between George and Mandy? No. He must be between Adam and Fred, and between Cindy and Fred. And so on... we can efficiently locate Dan by looking at half the set and then half of that set. Ultimately, we look at O(log_2 N) individuals.O(N log N)You could find where to sit down at the table using the algorithm above. If a large number of people came to the table, one at a time, and all did this, that would take O(N log N) time. This turns out to be how long it takes to sort any collection of items when they must be compared.Best/Worst CaseYou arrive at the party and need to find Inigo - how long will it take? It depends on when you arrive. If everyone is milling around you\'ve hit the worst-case: it will take O(N) time. However, if everyone is sitting down at the table, it will take only O(log N) time. Or maybe you can leverage the host\'s wineglass-shouting power and it will take only O(1) time.Assuming the host is unavailable, we can say that the Inigo-finding algorithm has a lower-bound of O(log N) and an upper-bound of O(N), depending on the state of the party when you arrive.Space & CommunicationThe same ideas can be applied to understanding how algorithms use space or communication.Knuth has written a nice paper about the former entitled "The Complexity of Songs".Theorem 2: There exist arbitrarily long songs of complexity O(1).PROOF: (due to Casey and the Sunshine Band). Consider the songs Sk defined by (15), but withfor all k. When you\'re analyzing  code, you have to analyse it line by line, counting every operation/recognizing time complexity, in the end, you have to sum it to get whole picture.For example, you can have one simple loop with linear complexity, but later in that same program you can have a triple loop that has cubic complexity, so your program will have cubic complexity. Function order of growth comes into play right here. Let\'s look at what are possibilities for time complexity of an algorithm, you can see order of growth I mentioned above:Constant time has an order of growth 1, for example: a = b + c.Logarithmic time has an order of growth LogN, it usually occurs\nwhen you\'re dividing something in half (binary search, trees, even loops), or multiplying something in same way.Linear, order of growth is N, for exampleLinearithmic, order of growth is n*logN, usually occurs in divide and conquer algorithms.Cubic, order of growth N^3, classic example is a triple loop where you check all triplets: Exponential, order of growth 2^N, usually occurs when you do exhaustive search, for example check subsets of some set.1. Time complexityWikipedia defines time complexity as In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input.2. Why do we talk about it? Why is it so important?Time complexity helps us understand how a particular algorithm behaves as the input grows larger. If let\'s say your algorithm takes 1 second to run for a input size of 1K how will it behave when the input size doubles. Will it run in the same time, double the time, four time slower? In real world programming it is very important to predict how your algorithm behaves when the input becomes larger.The time complexity of an algorithm is commonly expressed using big O notation.Which begs the next question...3. What is Big O notation?In Compter Science, Big O is used to describe the performance and complexity of an algorithm. It specifically describes the worst-case scenario and the execution time required by the algorithm.  For a more in-depth read check these questions on SO - What is a plain English explanation of "Big O" notation? and Big-O for Eight Year Olds?Now that we have some base ready, let\'s start talking about some time complexity with sample code - 4. O(1) - Constant timeTime complexity of an algorithm can be O(1) only when your algorithm takes a finite amount of time no matter what input or size of input is. For example your algorithm takes 20 milliseconds, 5 nanoseconds or 3 seconds or even 15 mins, as long as your algorithm takes a finite amount of time its time complexity is O(n).Here is one example of such code.all are examples of time complexity O(1).5. O(n) - Linear timeO(n) - describes an algorithm whose performance grows linearly and in direct proportion to the size of the input.The above code also demonstrates how Big O favors the worst-case performance scenario.Here are some examples of O(n) -6. O(n2)All algorithms whose performance is directly proportional to the square of the size of the input have their time complexity as O(n2).are few examples under this category.O(n) is big O notation used for writing time complexity of an algorithm. When you add up the number of executions in an algoritm you\'ll get an expression in result like 2N+2, in this expression N is the dominating term(the term having largest effect on expression if its value increases or decreases). Now O(N) is the time comlexity while N is dominating term.\nExamplehere total number of executions for inner loop are n+1 and total number of executions for outer loop are n(n+1)/2, so total number of executions for whole algorithm are n+1+n(n+1/2) = (n^2+3n)/2.\nhere n^2 is the dominating term so the time complexity for this algorithm is O(n^2) I know this question goes a way back and there are some excellent answers here, nonetheless I wanted to share another bit for the mathematically-minded people that will stumble in this post. The Master theorem is another usefull thing to know when studying complexity. I didn\'t see it mentioned in the other answers.For getting a feeling for an algorithm, I often do this experimentally. Simply vary the input N and see how long the computation takes. This needs some thought, since big-O describes the worst-case time complexity of the algorithm, and finding the worst case can be tricky.For doing it theoretically, your approach seems right to me: walk through the programm (always choosing the most time-complex path), add the indivial times and get rid of all constants/factors. Nested loops, jumps, etc. can make this fairly complex but I can\'t think of a silver bullet to solve this otherwise.You might also be intersted in http://en.wikipedia.org/wiki/Big_O_notation, despite it is fairly mathematical.I also just found http://en.wikipedia.org/wiki/Analysis_of_algorithms