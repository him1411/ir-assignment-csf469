I\'m aware of the Gradient Descent and the Back-propagation Theorem. What I don\'t get is: When is using a bias important and how do you use it?For example, when mapping the AND function, when I use 2 inputs and 1 output, it does not give the correct weights, however, when I use 3 inputs (1 of which is a bias), it gives the correct weights.I think that biases are almost always helpful.  In effect, a bias value allows you to shift the activation function to the left or right, which may be critical for successful learning.It might help to look at a simple example.  Consider this 1-input, 1-output network that has no bias:The output of the network is computed by multiplying the input (x) by the weight (w0) and passing the result through some kind of activation function (e.g. a sigmoid function.)Here is the function that this network computes, for various values of w0:Changing the weight w0 essentially changes the "steepness" of the sigmoid.  That\'s useful, but what if you wanted the network to output 0 when x is 2?  Just changing the steepness of the sigmoid won\'t really work -- you want to be able to shift the entire curve to the right.That\'s exactly what the bias allows you to do.  If we add a bias to that network, like so:...then the output of the network becomes sig(w0*x + w1*1.0).  Here is what the output of the network looks like for various values of w1:Having a weight of -5 for w1 shifts the curve to the right, which allows us to have a network that outputs 0 when x is 2.Just to add my two cents.A simpler way to understand what the bias is: it is somehow similar to the constant b of a linear functiony = ax + bIt allows you to move the line up and down to fit the prediction with the data better.\nWithout b the line always goes through the origin (0, 0) and you may get a poorer fit.Two different kinds of parameters can\n  be adjusted during the training of an\n  ANN, the weights and the value in the\n  activation functions. This is\n  impractical and it would be easier if\n  only one of the parameters should be\n  adjusted. To cope with this problem a\n  bias neuron is invented. The bias\n  neuron lies in one layer, is connected\n  to all the neurons in the next layer,\n  but none in the previous layer and it\n  always emits 1. Since the bias neuron\n  emits 1 the weights, connected to the\n  bias neuron, are added directly to the\n  combined sum of the other weights\n  (equation 2.1), just like the t value\n  in the activation functions.1The reason it\'s impractical is because you\'re simultaneously adjusting the weight and the value, so any change to the weight can neutralize the change to the value that was useful for a previous data instance... adding a bias neuron without a changing value allows you to control the behavior of the layer.Furthermore the bias allows you to use a single neural net to represent similar cases.  Consider the AND boolean function represented by the following neural network:  ANN http://www.aihorizon.com/images/essays/perceptron.gifA single perceptron can be used to\n  represent many boolean functions. For example, if we assume boolean values\n  of 1 (true) and -1 (false), then one\n  way to use a two-input perceptron to\n  implement the AND function is to set\n  the weights w0 = -3, and w1 = w2 = .5.\n  This perceptron can be made to\n  represent the OR function instead by\n  altering the threshold to w0 = -.3. In\n  fact, AND and OR can be viewed as\n  special cases of m-of-n functions:\n  that is, functions where at least m of\n  the n inputs to the perceptron must be\n  true. The OR function corresponds to\n  m = 1 and the AND function to m = n.\n  Any m-of-n function is easily\n  represented using a perceptron by\n  setting all input weights to the same\n  value (e.g., 0.5) and then setting the\n  threshold w0 accordingly. Perceptrons can represent all of the\n  primitive boolean functions AND, OR,\n  NAND ( 1 AND), and NOR ( 1 OR). Machine Learning- Tom Mitchell)The threshold is the bias and w0 is the weight associated with the bias/threshold neuron.A layer in a neural network without a bias is nothing more than the multiplication of an input vector with a matrix. (The output vector might be passed through a sigmoid function for normalisation and for use in multi-layered ANN afterwards but that\xe2\x80\x99s not important.)This means that you\xe2\x80\x99re using a linear function and thus an input of all zeros will always be mapped to an output of all zeros. This might be a reasonable solution for some systems but in general it is too restrictive.Using a bias, you\xe2\x80\x99re effectively adding another dimension to your input space, which always takes the value one, so you\xe2\x80\x99re avoiding an input vector of all zeros. You don\xe2\x80\x99t lose any generality by this because your trained weight matrix needs not be surjective, so it still can map to all values previously possible.2d ANN:For a ANN mapping two dimensions to one dimension, as in reproducing the AND or the OR (or XOR) functions, you can think of a neuronal network as doing the following:On the 2d plane mark all positions of input vectors. So, for boolean values, you\xe2\x80\x99d want to mark (-1,-1), (1,1), (-1,1), (1,-1). What your ANN now does is drawing a straight line on the 2d plane, separating the positive output from the negative output values.Without bias, this straight line has to go through zero, whereas with bias, you\xe2\x80\x99re free to put it anywhere.\nSo, you\xe2\x80\x99ll see that without bias you\xe2\x80\x99re facing a problem with the AND function, since you can\xe2\x80\x99t put both (1,-1) and (-1,1) to the negative side. (They are not allowed to be on the line.) The problem is equal for the OR function. With a bias, however, it\xe2\x80\x99s easy to draw the line.Note that the XOR function in that situation can\xe2\x80\x99t be solved even with bias.When you use ANNs, you rarely know about the internals of the systems you want to learn. Some things cannot be learned without a bias. E.g., have a look at the following data: (0, 1), (1, 1), (2, 1), basically a function that maps any x to 1. If you have a one layered network (or a linear mapping), you cannot find a solution. However, if you have a bias it\'s trivial!In an ideal setting, a bias could also map all points to the mean of the target points and let the hidden neurons model the differences from that point.Just to add to all of this something that is very much missing and which the rest, most likely, didn\'t know.If you\'re working with images, you might actually prefer to not use a bias at all. In theory, that way your network will be more independent of data magnitude, as in whether the picture is dark, or bright and vivid. And the net is going to learn to do it\'s job through studying relativity inside your data. Lots of modern neural networks utilize this.For other data having biases might be critical. It depends on what type of data you\'re dealing with. If your information is magnitude-invariant --- if inputting [1,0,0.1] should lead to the same result as if inputting [100,0,10], you might be better off without a bias.The bias is not an NN term, it\'s a generic algebra term to consider.Y = M*X + C (straight line equation)Now if C(Bias) = 0 then,\nthe line will always pass through the origin, i.e. (0,0), and depends on only one parameter, i.e. M, which is the slope so we have less things to play with.C, which is the bias takes any number and has the activity to shift the graph, and hence able to represent more complex situations.In a logistic regression, the expected value of the target is transformed by a link function to restrict its value to the unit interval. In this way, model predictions can be viewed as primary outcome probabilities as shown: Sigmoid function on WikipediaThis is the final activation layer in the NN map that turns on and off the neuron. Here also bias has a role to play and it shifts the curve flexibly to help us map the model.Modification of neuron WEIGHTS alone only serves to manipulate the shape/curvature of your transfer function, and not its equilibrium/zero crossing point.The introduction of BIAS neurons allows you to shift the transfer function curve horizontally (left/right) along the input axis while leaving the shape/curvature unaltered.\nThis will allow the network to produce arbitrary outputs different from the defaults and hence you can customize/shift the input-to-output mapping to suit your particular needs.See here for graphical explanation:\nhttp://www.heatonresearch.com/wiki/BiasExpanding on @zfy explanation... \nThe equation for one input, one neuron, one output should look:where x is the value from the input node and 1 is the value of the bias node;\ny can be directly your output or be passed into a function, often a sigmoid function. Also note that the bias could be any constant, but to make everything simpler we always pick 1 (and probably that\'s so common that @zfy did it without showing & explaining it).Your network is trying to learn coefficients a and b to adapt to your data.\nSo you can see why adding the element b * 1 allows it to fit better to more data: now you can change both slope and intercept.If you have more than one input your equation will look like:Note that the equation still describes a one neuron, one output network; if you have more neurons you just add one dimension to the coefficient matrix, to multiplex the inputs to all nodes and sum back each node contribution.That you can write in vectorized format as i.e. putting coefficients in one array and (inputs + bias) in another you have your desired solution as the dot product of the two vectors (you need to transpose X for the shape to be correct, I wrote XT a \'X transposed\')So in the end you can also see your bias as is just one more input to represent the part of the output that is actually independent of your input.To think in simple way,if you have y=w1*x where y is your output and w1 is the weight imagine a condition where x=0 then y=w1*x equals to 0,If you want to update your weight you have to compute how much change by delw=target-y where target is your target output,in this case \'delw\' will not change since y is computed as 0.So,suppose if you can add some extra value it will help y=w1*x+w0*1,where bias=1 and weight can be adjusted to get a correct bias.Consider the example below.  In terms of line Slope-intercept is a specific form of linear equations. y=mx+bcheck the imageimagehere b is (0,2)if you want to increase it to (0,3) how will you do it by changing the value of b which will be your biasIn a couple of experiments in my masters thesis (e.g. page 59), I found that the bias might be important for the first layer(s), but especially at the fully connected layers at the end it seems not to play a big role.This might be highly dependent on the network architecture / dataset.For all the ML books I studied, the W is always defined as the connectivity index between two neurons , which means the higher connectivity between two neurons , the stronger the signals will be transmitted from the firing neuron to the target neuron or Y= w * X as  a result to maintain the biological character of neurons, \nwe need to keep the 1 >=W >= -1 , but in the real regression, the W will end up with |W| >=1 which contradict with how the Neurons are working, as a result I propose W= cos(theta) , while 1 >=| cos( theta)| , and Y= a * X = W * X + b\n  while a = b + W = b + cos( theta) , b is an integer 