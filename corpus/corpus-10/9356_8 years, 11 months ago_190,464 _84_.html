I believe there\'s a way to find the kth largest element in an unsorted array of length n in O(n).  Or perhaps it\'s "expected" O(n) or something.  How can we do this?This is called finding the k-th order statistic. There\'s a very simple randomized algorithm (called quickselect) taking O(n) average time, O(n^2) worst case time, and a pretty complicated non-randomized algorithm (called introselect) taking O(n) worst case time. There\'s some info on Wikipedia, but it\'s not very good.Everything you need is in these powerpoint slides. Just to extract the basic algorithm of the O(n) worst-case algorithm (introselect):It\'s also very nicely detailed in the Introduction to Algorithms book by Cormen et al.If you want a true O(n) algorithm, as opposed to O(kn) or something like that, then you should use quickselect (it\'s basically quicksort where you throw out the partition that you\'re not interested in). My prof has a great writeup, with the runtime analysis: (reference)The QuickSelect algorithm quickly finds the k-th smallest element of an unsorted array of n elements.  It is a RandomizedAlgorithm, so we compute the worst-case expected running time.Here is the algorithm.What is the running time of this algorithm?  If the adversary flips coins for us, we may find that the pivot is always the largest element and k is always 1, giving a running time of But if the choices are indeed random, the expected running time is given bywhere we are making the not entirely reasonable assumption that the recursion always lands in the larger of A1 or A2.Let\'s guess that T(n) <= an for some a.  Then we getand now somehow we have to get the horrendous sum on the right of the plus sign to absorb the cn on the left.  If we just bound it as 2(1/n) \xe2\x88\x91i=n/2 to n an, we get roughly 2(1/n)(n/2)an = an.  But this is too big - there\'s no room to squeeze in an extra cn.  So let\'s expand the sum using the arithmetic series formula:where we take advantage of n being "sufficiently large" to replace the ugly floor(n/2) factors with the much cleaner (and smaller) n/4.  Now we can continue withprovided a > 16c.This gives T(n) = O(n).  It\'s clearly Omega(n), so we get T(n) = Theta(n).The keywords you are looking for are selection algorithm: Wikipedia lists a number of different ways of doing this.A quick Google on that (\'kth largest element array\') returned this: http://discuss.joelonsoftware.com/default.asp?interview.11.509587.17(it was specifically for 3d largest)and this answer:You do like quicksort.  Pick an element at random and shove everything either higher or lower.  At this point you\'ll know which element you actually picked, and if it is the kth element you\'re done, otherwise you repeat with the bin (higher or lower), that the kth element would fall in. Statistically speaking, the time it takes to find the kth element grows with n, O(n). A Programmer\'s Companion to Algorithm Analysis gives a version that is O(n), although the author states that the constant factor is so high, you\'d probably prefer the naive sort-the-list-then-select method.I answered the letter of your question :)The C++ standard library has almost exactly that function call nth_element, although it does modify your data.  It has expected linear run-time, O(N), and it also does a partial sort.Although not very sure about O(n) complexity, but it will be sure to be between O(n) and nLog(n). Also sure to be closer to O(n) than nLog(n). Function is written in JavaI implemented finding kth minimimum in n unsorted elements using dynamic programming, specifically tournament method. The execution time is O(n + klog(n)). The mechanism used is listed as one of methods on Wikipedia page about Selection Algorithm (as indicated in one of the posting above). You can read about the algorithm and also find code (java) on my blog page Finding Kth Minimum. In addition the logic can do partial ordering of the list - return first K min (or max) in O(klog(n)) time.Though the code provided result kth minimum, similar logic can be employed to find kth maximum in O(klog(n)), ignoring the pre-work done to create tournament tree.You can do it in O(n + kn) = O(n) (for constant k) for time and O(k) for space, by keeping track of the k largest elements you\'ve seen.  For each element in the array you can scan the list of k largest and replace the smallest element with the new one if it is bigger.Warren\'s priority heap solution is neater though.Read Chapter 9, Medians and Other statistics from Cormen\'s "Introduction to Algorithms", 2nd Ed. It has an expected linear time algorithm for selection. It\'s not something that people would randomly come up with in a few minutes..\nA heap sort, btw, won\'t work in O(n), it\'s O(nlgn).Find the median of the array in linear time, then use partition procedure exactly as in quicksort to divide the array in two parts, values to the left of the median lesser( < ) than than median and to the right greater than ( > ) median, that too can be done in lineat time, now, go to that part of the array where kth element lies, \nNow recurrence becomes:\nT(n) = T(n/2) + cn \nwhich gives me O (n) overal.Sexy quickselect in PythonBelow is the link to full implementation with quite an extensive explanation how the algorithm for finding Kth element in an unsorted algorithm works. Basic idea is to partition the array like in QuickSort. But in order to avoid extreme cases (e.g. when smallest element is chosen as pivot in every step, so that algorithm degenerates into O(n^2) running time), special pivot selection is applied, called median-of-medians algorithm. The whole solution runs in O(n) time in worst and in average case.Here is link to the full article (it is about finding Kth smallest element, but the principle is the same for finding Kth largest):Finding Kth Smallest Element in an Unsorted ArrayAs per this paper Finding the Kth largest item in a list of n items the following algorithm will take O(n) time in worst case.Analysis: As suggested in the original paper:We use the median to partition the list into two halves(the first half,\n  if k <= n/2 , and the second half otherwise). This algorithm takes\n  time cn at the first level of recursion for some constant c, cn/2 at\n  the next level (since we recurse in a list of size n/2), cn/4 at the\n  third level, and so on. The total time taken is cn + cn/2 + cn/4 +\n  .... = 2cn = o(n).Why partition size is taken 5 and not 3?As mentioned in original paper: Dividing the list by 5 assures a worst-case split of 70 \xe2\x88\x92 30. Atleast\n  half of the medians greater than the median-of-medians, hence atleast\n  half of the n/5 blocks have atleast 3 elements and this gives a\n  3n/10 split, which means the other partition is 7n/10 in worst case.\n  That gives T(n) = T(n/5)+T(7n/10)+O(n). Since n/5+7n/10 < 1, the\n  worst-case running time isO(n).Now I have tried to implement the above algorithm as:Just for sake of completion, another algorithm makes use of Priority Queue and takes time O(nlogn).Both of these algorithms can be tested as:As expected output is:\n18\n18 iterate through the list.  if the current value is larger than the stored largest value, store it as the largest value and bump the 1-4 down and 5 drops off the list. If not,compare it to number 2 and do the same thing.  Repeat, checking it against all 5 stored values. this should do it in O(n)i would like to suggest one answerif we take the first k elements and sort them into a linked list of k valuesnow for every other value even for the worst case if we do insertion sort for rest n-k values even in the worst case number of comparisons will be k*(n-k) and for prev k values to be sorted let it be k*(k-1) so it comes out to be (nk-k) which is o(n)cheers Explanation of the median - of - medians algorithm to find the k-th largest integer out of n can be found here:\nhttp://cs.indstate.edu/~spitla/presentation.pdfImplementation in c++ is below:There is also Wirth\'s selection algorithm, which has a simpler implementation than QuickSelect. Wirth\'s selection algorithm is slower than QuickSelect, but with some improvements it becomes faster. In more detail. Using Vladimir Zabrodsky\'s MODIFIND optimization and the median-of-3 pivot selection and paying some attention to the final steps of the partitioning part of the algorithm, i\'ve came up with the following algorithm (imaginably named "LefSelect"):In benchmarks that i did here, LefSelect is 20-30% faster than QuickSelect.Haskell Solution:This implements the median of median solutions by using the withShape method to discover the size of a partition without actually computing it. Here is a C++ implementation of Randomized QuickSelect. The idea is to randomly pick a pivot element. To implement randomized partition, we use a random function, rand() to generate index between l and r, swap the element at randomly generated index with the last element, and finally call the standard partition process which uses last element as pivot. The worst case time complexity of the above solution is still O(n2).In worst case, the randomized function may always pick a corner element. The expected time complexity of above randomized QuickSelect is \xce\x98(n)This is an implementation in Javascript.If you release the constraint that you cannot modify the array, you can prevent the use of extra memory using two indexes to identify the "current partition" (in classic quicksort style - http://www.nczonline.net/blog/2012/11/27/computer-science-in-javascript-quicksort/).If you want to test how it perform, you can use this variation:The rest of the code is just to create some playground:  Now, run you tests a few time.\nBecause of the Math.random() it will produce every time different results:If you test it a few times you can see even empirically that the number of iterations is, on average, O(n) ~= constant * n and the value of k does not affect the algorithm.I came up with this algorithm and seems to be O(n):Let\'s say k=3 and we want to find the 3rd largest item in the array. I would create three variables and compare each item of the array with the minimum of these three variables. If array item is greater than our minimum, we would replace the min variable with the item value. We continue the same thing until end of the array. The minimum of our three variables is the 3rd largest item in the array.And, to find Kth largest item we need K variables.Example: (k=3)Can someone please review this and let me know what I am missing?Here is the implementation of the algorithm eladv suggested(I also put here the implementation with random pivot):How about this kinda approach Maintain a buffer of length k and a tmp_max, getting tmp_max is O(k) and is done n times so something like O(kn) Is it right or am i missing something ?Although it doesn\'t beat average case of quickselect and worst case of median statistics method but its pretty easy to understand and implement. it is similar to the quickSort strategy, where we pick an arbitrary pivot, and bring the smaller elements to its left, and the larger to the rightGo to the End of this link : ...........http://www.geeksforgeeks.org/kth-smallestlargest-element-unsorted-array-set-3-worst-case-linear-time/Call poll() k times.What I would do is this:You can simply store pointers to the first and last element in the linked list. They only change when updates to the list are made.Update:First we can build a BST from unsorted array which takes O(n) time and from the BST we can find the kth smallest element in O(log(n)) which over all counts to an order of O(n).