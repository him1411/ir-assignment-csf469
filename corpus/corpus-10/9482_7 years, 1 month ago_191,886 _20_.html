Is there any simple way of generating (and checking) MD5 checksums of a list of files in Python? (I have a small program I\'m working on, and I\'d like to confirm the checksums of the files).You can use hashlib.md5()Note that sometimes you won\'t be able to fit the whole file in memory. In that case, you\'ll have to read chunks of 4096 bytes sequentially and feed them to the Md5 function:There is a way that\'s pretty memory inefficient.single file:list of files:But, MD5 is known broken and (IMHO) should come with scary deprecation warnings and removed from the library, so here\'s how you should actually do it:If you only want 128 bits worth of digest you can do .digest()[:16].This will give you a list of tuples, each tuple containing the name of its file and its hash.Again I strongly question your use of MD5. You should be at least using SHA1. Some people think that as long as you\'re not using MD5 for \'cryptographic\' purposes, you\'re fine. But stuff has a tendency to end up being broader in scope than you initially expect, and your casual vulnerability analysis may prove completely flawed. It\'s best to just get in the habit of using the right algorithm out of the gate. It\'s just typing a different bunch of letters is all. It\'s not that hard.Here is a way that is more complex, but memory efficient:And, again, since MD5 is broken and should not really ever be used anymore:Again, you can put [:16] after the call to hash_bytestr_iter(...) if you only want 128 bits worth of digest.I\'m clearly not adding anything fundamentally new, but added this answer before I was up to commenting status :-), plus the code regions make things more clear -- anyway, specifically to answer @Nemo\'s question from Omnifarious\'s answer:I happened to be thinking about checksums a bit (came here looking for suggestions on block sizes, specifically), and have found that this method may be faster than you\'d expect. Taking the fastest (but pretty typical) timeit.timeit or /usr/bin/time result from each of several methods of checksumming a file of approx. 11MB:So, looks like both Python and /usr/bin/md5sum take about 30ms for an 11MB file.  The relevant md5sum function (md5sum_read in the above listing) is pretty similar to Omnifarious\'s:Granted, these are from single runs (the mmap ones are always a smidge faster when at least a few dozen runs are made), and mine\'s usually got an extra f.read(blocksize) after the buffer is exhausted, but it\'s reasonably repeatable and shows that md5sum on the command line is not necessarily faster than a Python implementation...EDIT: Sorry for the long delay, haven\'t looked at this in some time, but to answer @EdRandall\'s question, I\'ll write down an Adler32 implementation.  However, I haven\'t run the benchmarks for it.  It\'s basically the same as the CRC32 would have been: instead of the init, update, and digest calls, everything is a zlib.adler32() call:Note that this must start off with the empty string, as Adler sums do indeed differ when starting from zero versus their sum for "", which is 1 -- CRC can start with 0 instead.  The AND-ing is needed to make it a 32-bit unsigned integer, which ensures it returns the same value across Python versions.I don\'t know if people have ever tried to md5sum a gzipped file, but its certainly not deterministic. I modified @Omnifaurious answer to handle this situation: