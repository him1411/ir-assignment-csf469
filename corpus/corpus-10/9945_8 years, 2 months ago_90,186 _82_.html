What is dynamic programming? How\'s it different from recursion, memoization, etc? I\'ve read the wikipedia article on it, but I still don\'t really understand it.Dynamic programming is when you use past knowledge to make solving a future problem easier.A good example is solving the Fibonacci sequence for n=1,000,002.This will be a very long process, but what if I give you the results for n=1,000,000 and n=1,000,001? Suddenly the problem just became more manageable.Dynamic programming is used a lot in string problems, such as the string edit problem. You solve a subset(s) of the problem and then use that information to solve the more difficult original problem.With dynamic programming, you store your results in some sort of table generally. When you need the answer to a problem, you reference the table and see if you already know what it is. If not, you use the data in your table to give yourself a stepping stone towards the answer.The Cormen Algorithms book has a great chapter about dynamic programming. AND it\'s free on Google Books! Check it out here.Dynamic programming is a technique used to avoid computing multiple time the same subproblem in a recursive algorithm.Let\'s take the simple example of the fibonacci numbers: finding the n th fibonacci number defined by F0 = 0,\nF1 = 1 and\nFn = Fn-1 + Fn-2The obvious way to do this is recursive:The recursion does a lot of unnecessary calculation because a given fibonacci number will be calculated multiple times. An easy way to improve this is to cache the results:A better way to do this is to get rid of the recursion all-together by evaluating the results in the right order:We can even use constant space and store only the necessary partial results along the way:How apply dynamic programming?Dynamic programming generally works for problems that have an inherent left to right order such as strings, trees or integer sequences. If the naive recursive algorithm does not compute the same subproblem multiple time, dynamic programming won\'t help.I made a collection of problems to help understand the logic: https://github.com/tristanguigue/dynamic-programingHere is my answer in similar topicStart with If you want to test yourself my choices about online judges areand of course You can also checks good universities algorithms coursesAfter all, if you can\'t solve problems ask SO that many algorithms addict exist hereMemoization is the when you store previous results of a function call (a real function always returns the same thing, given the same inputs). It doesn\'t make a difference for algorithmic complexity before the results are stored.Recursion is the method of a function calling itself, usually with a smaller dataset. Since most recursive functions can be converted to similar iterative functions, this doesn\'t make a difference for algorithmic complexity either.Dynamic programming is the process of solving easier-to-solve sub-problems and building up the answer from that. Most DP algorithms will be in the running times between a Greedy algorithm (if one exists) and an exponential (enumerate all possibilities and find the best one) algorithm.It\'s an optimization of your algorithm that cuts running time.While a Greedy Algorithm is usually called naive, because it may run multiple times over the same set of data, Dynamic Programming avoids this pitfall through a deeper understanding of the partial results that must be stored to help build the final solution.A simple example is traversing a tree or a graph only through the nodes that would contribute with the solution, or putting into a table the solutions that you\'ve found so far so you can avoid traversing the same nodes over and over. Here\'s an example of a problem that\'s suited for dynamic programming, from UVA\'s online judge: Edit Steps Ladder.I\'m going to make quick briefing of the important part of this problem\'s analysis, taken from the book Programming Challenges, I suggest you check it out.Take a good look at that problem, if\n  we define a cost function telling us\n  how far appart two strings are, we\n  have two consider the three natural\n  types of changes:Substitution - change a single\n  character from pattern "s" to a\n  different character in text "t", such\n  as changing "shot" to "spot". Insertion - insert a single character\n  into pattern "s" to help it match text\n  "t", such as changing "ago" to "agog".Deletion - delete a single character\n  from pattern "s" to help it match text\n  "t", such as changing "hour" to "our".When we set each of this operations to\n  cost one step we define the edit\n  distance between two strings. So how\n  do we compute it?We can define a recursive algorithm\n  using the observation that the last\n  character in the string must be either\n  matched, substituted, inserted or\n  deleted. Chopping off the characters\n  in the last edit operation leaves a\n  pair operation leaves a pair of\n  smaller strings. Let i and j be the\n  last character of the relevant prefix\n  of and t, respectively. there are\n  three pairs of shorter strings after\n  the last operation, corresponding to\n  the string after a match/substitution,\n  insertion or deletion. If we knew the\n  cost of editing the three pairs of\n  smaller strings, we could decide which\n  option leads to the best solution and\n  choose that option accordingly. We can\n  learn this cost, through the awesome\n  thing that\'s recursion:This algorithm is correct, but is also\n  impossibly slow. Running on our computer, it takes\n  several seconds to compare two\n  11-character strings, and the\n  computation disappears into\n  never-never land on anything longer. Why is the algorithm so slow? It takes\n  exponential time because it recomputes\n  values again and again and again. At\n  every position in the string, the\n  recursion branches three ways, meaning\n  it grows at a rate of at least 3^n \xe2\x80\x93\n  indeed, even faster since most of the\n  calls reduce only one of the two\n  indices, not both of them. So how can we make the algorithm\n  practical? The important observation\n  is that most of these recursive calls\n  are computing things that have already\n  been computed before. How do we\n  know? Well, there can only be |s| \xc2\xb7\n  |t| possible unique recursive calls,\n  since there are only that many\n  distinct (i, j) pairs to serve as the\n  parameters of recursive calls.By storing the values for each of these (i, j) pairs in a table, we can\n  avoid recomputing them and just look\n  them up as needed.The table is a two-dimensional matrix\n  m where each of the |s|\xc2\xb7|t| cells\n  contains the cost of the optimal\n  solution of this subproblem, as well\n  as a parent pointer explaining how we\n  got to this location:The dynamic programming version has\n  three differences from the recursive\n  version.First, it gets its intermediate values using table lookup instead of\n  recursive calls.**Second,**it updates the parent field of each cell, which will enable us to\n  reconstruct the edit sequence later.**Third,**Third, it is instrumented using a more general goal cell()\n  function instead of just returning\n  m[|s|][|t|].cost. This will enable us\n  to apply this routine to a wider class\n  of problems.Here, a very particular analysis of what it takes to gather the most optimal partial results, is what makes the solution a "dynamic" one.  Here\'s an alternate, full solution to the same problem. It\'s also a "dynamic" one even though its execution is different. I suggest you check out how efficient the solution is by submitting it to UVA\'s online judge. I find amazing how such a heavy problem was tackled so efficiently.The key bits of dynamic programming are "overlapping sub-problems" and "optimal substructure". These properties of a problem mean that an optimal solution is composed of the optimal solutions to its sub-problems. For instance, shortest path problems exhibit optimal substructure. The shortest path from A to C is the shortest path from A to some node B followed by the shortest path from that node B to C.In greater detail, to solve a shortest-path problem you will: Because we are working bottom-up, we already have solutions to the sub-problems when it comes time to use them, by memoizing them.Remember, dynamic programming problems must have both overlapping sub-problems, and optimal substructure. Generating the Fibonacci sequence is not a dynamic programming problem; it utilizes memoization because it has overlapping sub-problems, but it does not have optimal substructure (because there is no optimization problem involved).Dynamic ProgrammingDefinitionDynamic programming (DP) is a general algorithm design technique for solving\nproblems with overlapping sub-problems. This technique was invented by American\nmathematician \xe2\x80\x9cRichard Bellman\xe2\x80\x9d in 1950s.Key IdeaThe key idea is to save answers of overlapping smaller sub-problems to avoid recomputation.Dynamic Programming PropertiesHere is one tutorial by Michael A. Trick from CMU that I found particularly helpful:http://mat.gsia.cmu.edu/classes/dynamic/dynamic.htmlIt is certainly in addition to all resources others have recommended (all other resources, specially CLR and Kleinberg,Tardos are very good!).The reason why I like this tutorial is because it introduces advanced concepts fairly gradually. It is bit oldish material but it is a good addition to the list of resources presented here. Also check out Steven Skiena\'s page and lectures on Dynamic Programming:\nhttp://www.cs.sunysb.edu/~algorith/video-lectures/http://www.cs.sunysb.edu/~algorith/video-lectures/1997/lecture12.pdfI am also very much new to Dynamic Programming (a powerful algorithm for particular type of problems)In most simple words, just think dynamic programming as a recursive approach with using the previous knowledgePrevious knowledge is what matters here the most, Keep track of the solution of the sub-problems you already have.Consider this, most basic example for dp from WikipediaFinding the fibonacci sequenceLets break down the function call with say n = 5In particular, fib(2) was calculated three times from scratch. In larger examples, many more values of fib, or sub-problems, are recalculated, leading to an exponential time algorithm.Now, lets try it by storing the value we already found out in a data-structure say a MapHere we are saving the solution of sub-problems in the map, if we don\'t have it already. This technique of saving values which we already had calculated is termed as Memoization.At last, For a problem, first try to find the states (possible sub-problems and try to think of the better recursion approach so that you can use the solution of previous sub-problem into further ones).in short  the difference between recursion memoization and Dynamic programming Dynamic programming as name suggest is using the previous calculated value to dynamically construct the next new solution Where to apply dynamic programming : If you solution is  based on optimal substructure and overlapping sub problem then in that case using the earlier calculated value will be useful so you do not have to recompute it. It is bottom up approach. Suppose you need to calculate fib(n) in that case all you need to do is add the previous calculated value of fib(n-1) and fib(n-2)Recursion : Basically subdividing you problem into smaller part to solve it with ease but keep it in mind it does not avoid re computation if we have same value calculated previously in other recursion call. Memoization : Basically storing the old calculated recursion value in table is known as memoization which will avoid re-computation if its already been calculated by some previous call so any value will be calculated once. So before calculating we check whether this value has already been calculated or not if already calculated then we return the same from table instead of recomputing. It is also top down approach