I\'m currently configuring hadoop on a server running CentOs. When I run start-dfs.sh or stop-dfs.sh, I get the following error:WARN util.NativeCodeLoader: Unable to load native-hadoop library for\n  your platform... using builtin-java classes where applicableI\'m running Hadoop 2.2.0. Doing a search online brought up this link: http://balanceandbreath.blogspot.ca/2013/01/utilnativecodeloader-unable-to-load.htmlHowever, the contents of /native/ directory on hadoop 2.x appear to be different so I am not sure what to do.I\'ve also added these two environment variables in hadoop-env.sh:export HADOOP_OPTS="$HADOOP_OPTS\n  -Djava.library.path=/usr/local/hadoop/lib/"export HADOOP_COMMON_LIB_NATIVE_DIR="/usr/local/hadoop/lib/native/"Any ideas?I assume you\'re running Hadoop on 64bit CentOS. The reason you saw that warning is the native Hadoop library $HADOOP_HOME/lib/native/libhadoop.so.1.0.0 was actually compiled on 32 bit. Anyway, it\'s just a warning, and won\'t impact Hadoop\'s functionalities.Here is the way if you do want to eliminate this warning, download the source code of Hadoop  and recompile libhadoop.so.1.0.0 on 64bit system, then replace the 32bit one. Steps on how to recompile source code are included here for Ubuntu:Good luck.Just append word native to your HADOOP_OPTS like this:export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"PS: Thank SeareneThe answer depends... I just installed Hadoop 2.6 from tarball on 64-bit CentOS 6.6. The Hadoop install did indeed come with a prebuilt 64-bit native library. For my install, it is here:And I know it is 64-bit:Unfortunately, I stupidly overlooked the answer right there staring me in the face as I was focuses on, "Is this library 32 pr 64 bit?":So, lesson learned. Anyway, the rest at least led me to being able to suppress the warning. So I continued and did everything recommended in the other answers to provide the library path using the HADOOP_OPTS environment variable to no avail. So I looked at the source code. The module that generates the error tells you the hint (util.NativeCodeLoader):So, off to here to see what it does:http://grepcode.com/file/repo1.maven.org/maven2/com.ning/metrics.action/0.2.6/org/apache/hadoop/util/NativeCodeLoader.java/Ah, there is some debug level logging - let\'s turn that on a see if we get some additional help. This is done by adding the following line to $HADOOP_CONF_DIR/log4j.properties file:Then I ran a command that generates the original warning, like stop-dfs.sh, and got this goodie:And the answer is revealed in this snippet of the debug message (the same thing that the previous ldd command \'tried\' to tell me:What version of GLIBC do I have? Here\'s simple trick to find out:So, can\'t update my OS to 2.14. Only solution is to build the native libraries from sources on my OS or suppress the warning and just ignore it for now. I opted to just suppress the annoying warning for now (but do plan to build from sources in the future) buy using the same logging options we used to get the debug message, except now, just make it ERROR level.I hope this helps others see that a big benefit of open source software is that you can figure this stuff out if you take some simple logical steps.In my case , after I build hadoop on my 64 bit Linux mint OS, I replaced the native library in hadoop/lib. Still the problem persist. Then I figured out the hadoop pointing to hadoop/lib not to the hadoop/lib/native. So I just moved all content from native library to its parent. And the warning just gone.I had the same issue. It\'s solved by adding following lines in .bashrc:After a continuous research as suggested by KotiI got resolved the issue. CheersFor those on OSX with Hadoop installed via Homebrew, follow these steps replacing the path and Hadoop version where appropriatethen update hadoop-env.sh with@zhutoulala -- FWIW your links worked for me with Hadoop 2.4.0 with one exception I had to tell maven not to build the javadocs.  I also used the patch in the first link for 2.4.0 and it worked fine.  Here\'s the maven command I had to issueAfter building this and moving the libraries, don\'t forget to update hadoop-env.sh :)Thought this might help someone who ran into the same roadblocks as meThis also would work:Move your compiled native library files to $HADOOP_HOME/lib folder.Then set your environment variables by editing .bashrc fileMake sure your compiled native library files are in $HADOOP_HOME/lib folder.it should work.This line right here:From KunBetter\'s answer is where the money isThis line right here:From KunBetter\'s answer, worked for me.\nJust append it to .bashrc file and reload .bashrc contentsI had the same problem with JDK6,I changed the JDK to JDK8,the problem solved.\nTry to use JDK8!!!In addition to @zhutoulala accepted answer, here is an update to make it work with latest stable version to date (2.8) on ARMHF platforms (Raspberry Pi 3 model B).\nFirst I can confirm that you must recompile native libraries to 64 bit ARM, other answers here based on setting some environment variables won\'t work. As indicated in Hadoop documentation, the pre-built native libraries are 32 bit.High level steps given in the fist link (http://www.ercoppa.org/posts/how-to-compile-apache-hadoop-on-ubuntu-linux.html) are correct.\nOn this url http://www.instructables.com/id/Native-Hadoop-260-Build-on-Pi/ you get more details specific to Raspberry Pi, but not for Hadoop version 2.8.Here are my indications pour Hadoop 2.8  :CMake file patching method must be changed. Moreovere, files to patch are not the same. Unfortunately, there is no accepted patch on JIRA specific to 2.8. On this URL (https://issues.apache.org/jira/browse/HADOOP-9320) you must copy and paste Andreas Muttscheller proposed patch on your namenode :Once build is successful :And replace the content of the lib/native directory of your Hadoop install with the content of this archive. Warning message when running Hadoop should disappear. For installing Hadoop it is soooooo much easier installing the free version from Cloudera. It comes with a nice GUI that makes it simple to add nodes, there is no compiling or stuffing around with dependencies, it comes with stuff like hive, pig etc.http://www.cloudera.com/content/support/en/downloads.htmlSteps are:\n1) Download\n2) Run it\n3) Go to web GUI (1.2.3.4:7180)\n4) Add extra nodes in the web gui (do NOT install the cloudera software on other nodes, it does it all for you)\n5) Within the web GUI go to Home, click Hue and Hue Web UI. This gives you access to Hive, Pig, Sqoop etc.Verified remedy from earlier postings:1) Checked that the libhadoop.so.1.0.0 shipped with the Hadoop distribution was compiled for my machine architecture, which is x86_64:2) Added -Djava.library.path=<path> to HADOOP_OPT in hadoop-env.sh:This indeed made the annoying warning disappear.Firstly: You can modify the glibc version.CentOS provides safe softwares tranditionally,it also means the version is old such as glibc,protobuf ...You can compare the version of current glibc with needed glibc.Secondly: If the version of current glibc is old,you can update the glibc. \nDownLoad GlibcIf the version of current glibc id right,you can append word native to your HADOOP_OPTSI\'m not using CentOS. Here is what I have in Ubuntu 16.04.2, hadoop-2.7.3, jdk1.8.0_121. Run start-dfs.sh or stop-dfs.sh successfully w/o error:Replace /j01/sys/jdk, /j01/srv/hadoop with your installation pathI also did the following for one time setup on Ubuntu, which eliminates the need to enter passwords for multiple times when running start-dfs.sh:Replace user with your username