Requests is a really nice library. I\'d like to use it for download big files (>1GB).\nThe problem is it\'s not possible to keep whole file in memory I need to read it in chunks. And this is a problem with the following codeBy some reason it doesn\'t work this way. It still loads response into memory before save it to a file.I figured out what should be changed. The trick was to set stream = True in the get() method. After this python process stopped to suck memory (stays around 30kb regardless size of the download file). Thank you @danodonovan for you syntax I use it here:See http://docs.python-requests.org/en/latest/user/advanced/#body-content-workflow for further reference.Your chunk size could be too large, have you tried dropping that - maybe 1024 bytes at a time? (also, you could use with to tidy up the syntax)Incidentally, how are you deducing that the response has been loaded into memory?It sounds as if python isn\'t flushing the data to file, from other SO questions you could try f.flush() and os.fsync() to force the file write and free memory;It\'s much easier if you use Response.raw and shutil.copyfileobj():This streams the file to disk without using excessive memory, and the code is simple.Not exactly what OP was asking, but... it\'s ridiculously easy to do that with urllib:Or this way, if you want to save it to a temporary file:I watched the process:And I saw the file growing, but memory usage stayed at 17 MB. Am I missing something?