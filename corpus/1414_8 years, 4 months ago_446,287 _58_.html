I need to get a line count of a large file (hundreds of thousands of lines) in python. What is the most efficient way both memory- and time-wise?At the moment I do:is it possible to do any better?You can\'t get any better than that.After all, any solution will have to read the entire file, figure out how many \\n you have, and return that result.Do you have a better way of doing that without reading the entire file? Not sure... The best solution will always be I/O-bound, best you can do is make sure you don\'t use unnecessary memory, but it looks like you have that covered.One line, probably pretty fast:I believe that a memory mapped file will be the fastest solution. I tried four functions: the function posted by the OP (opcount); a simple iteration over the lines in the file (simplecount); readline with a memory-mapped filed (mmap) (mapcount); and the buffer read solution offered by Mykola Kharechko (bufcount).I ran each function five times, and calculated the average run-time for a 1.2 million-line text file.Windows XP, Python 2.5, 2GB RAM, 2 GHz AMD processorHere are my results:Edit: numbers for Python 2.6:So the buffer read strategy seems to be the fastest for Windows/Python 2.6Here is the code:You could execute a subprocess and run wc -l filenameI had to post this on a similar question until my reputation score jumped a bit (thanks to whoever bumped me!). All of these solutions ignore one way to make this run considerably faster, namely by using the unbuffered (raw) interface, using bytearrays, and doing your own buffering. (This only applies in Python 3.  In Python 2, the raw interface may or may not be used by default, but in Python 3, you\'ll default into Unicode.)Using a modified version of the timing tool, I believe the following code is faster (and marginally more pythonic) than any of the solutions offered:Using a separate generator function, this runs a smidge faster:This can be done completely with generators expressions in-line using itertools, but it gets pretty weird looking:Here are my timings:Here is a python program to use the multiprocessing library to distribute the line counting across machines/cores.  My test improves counting a 20million line file from 26 seconds to 7 seconds using an 8 core windows 64 server.  Note: not using memory mapping makes things much slower.I would use Python\'s file object method readlines, as follows:This opens the file, creates a list of lines in the file, counts the length of the list, saves that to a variable and closes the file again.I got a small (4-8%) improvement with this version which re-uses a constant buffer so it should avoid any memory or GC overhead:You can play around with the buffer size and maybe see a little improvement.Kyle\'s answer is probably best, an alternative for this isHere is the comparision of performance of both This code is shorter and clearer. It\'s probably the best way:the result of opening a file is an iterator, which can be converted to a sequence, which has a length:this is more concise than your explicit loop, and avoids the enumerate.Just to complete the above methods I tried a variant with the fileinput module:And passed a 60mil lines file to all the above stated methods:It\'s a little surprise to me that fileinput is that bad and scales far worse than all the other methods...count = max(enumerate(open(filename)))[0]This is the fastest thing I have found using pure python.\nYou can use whatever amount of memory you want by setting buffer, though 2**16 appears to be a sweet spot on my computer.  I found the answer here Why is reading lines from stdin much slower in C++ than Python? and tweaked it just a tiny bit.  Its a very good read to understand how to count lines quickly, though wc -l is still about 75% faster than anything else.I have modified the buffer case like this:Now also empty files and the last line (without \\n) are counted.What about thisone line solutionmy snippetos.system(\'wc -l *.txt\')A one-line bash solution similar to this answer, using the modern subprocess.check_output function:Why not read the first 100 and the last 100 lines and estimate the average line length, then divide the total file size through that numbers? If you don\'t need a exact value this could work.As for me this variant will be the fastest:reasons: buffering faster than reading line by line and string.count is also very fastAnother possibility: Here is what I use, seems pretty clean:UPDATE: This is marginally slower than using pure python at the cost of memory usage. Subprocess will fork a new process with the same memory footprint as the parent process while it executes your command.Why wouldn\'t the following work?In this case, the len function uses the input lines as a means of determining the length.How about this?what about this?Similarly:How about this one-liner:Takes 0.003 sec using this method to time it on a 3900 line file