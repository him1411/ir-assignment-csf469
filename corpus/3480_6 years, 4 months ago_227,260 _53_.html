I am trying to read a large file one line at a time. I found a question on Quora that dealt with the subject but I\'m missing some connections to make the whole thing fit together.The bit that I\'d like to figure out is how I might read one line at a time from a file instead of STDIN as in this sample.I tried: but it\'s not working.  I know that in a pinch I could fall back to using something like PHP, but I would like to figure this out.  I don\'t think the other answer would work as the file is much larger than the server I\'m running it on has memory for. Since Node.js v0.12 and as of Node.js v4.0.0, there is a stable readline core module. Here\'s the easiest way to read lines from a file, without any external modules:The last line is read correctly (as of Node v0.12 or later), even if there is no final \\n.UPDATE: this example has been added to Node\'s API official documentation.For such a simple operation there shouldn\'t be any dependency on third-party modules. Go easy.You don\'t have to open the file, but instead, you have to create a ReadStream.fs.createReadStreamThen pass that stream to Lazythere is a very nice module for reading a file line by line, it\'s called line-readerhttps://github.com/nickewing/line-readerwith it you simply just write:you can even iterate the file with a "java-style" interface, if you need more control:Old topic, but this works:Simple. No need for an external module.You can always roll your own line reader. I have\'nt benchmarked this snippet yet, but it correctly splits the incoming stream of chunks into lines without the trailing \'\\n\'I did come up with this when working on a quick log parsing script that needed to accumulate data during the log parsing and I felt that it would nice to try doing this using js and node instead of using perl or bash.Anyway, I do feel that small nodejs scripts should be self contained and not rely on third party modules so after reading all the answers to this question, each using various modules to handle line parsing, a 13 SLOC native nodejs solution might be of interest .With the carrier module:I ended up with a massive, massive memory leak using Lazy to read line by line when trying to then process those lines and write them to another stream due to the way drain/pause/resume in node works (see: http://elegantcode.com/2011/04/06/taking-baby-steps-with-node-js-pumping-data-between-streams/ (i love this guy btw)). I haven\'t looked closely enough at Lazy to understand exactly why, but I couldn\'t pause my read stream to allow for a drain without Lazy exiting.I wrote the code to process massive csv files into xml docs, you can see the code here: https://github.com/j03m/node-csv2xmlIf you run the previous revisions with Lazy line it leaks. The latest revision doesn\'t leak at all and you can probably use it as the basis for a reader/processor. Though I have some custom stuff in there. Edit: I guess I should also note that my code with Lazy worked fine until I found myself writing large enough xml fragments that drain/pause/resume because a necessity. For smaller chunks it was fine.Edit:Use a transform stream.With a BufferedReader you can read lines.I was frustrated by the lack of a comprehensive solution for this, so I put together my own attempt (git / npm). Copy-pasted list of features:NIH? You decide :-)Since posting my original answer, I found that split is a very easy to use node module for line reading in a file; Which also accepts optional parameters. Haven\'t tested on very large files. Let us know if you do.I wanted to tackle this same problem, basically what in Perl would be:My use case was just a standalone script, not a server, so synchronous was fine. These were my criteria:This is a project for me to get a feel for low-level scripting type code in node.js and decide how viable it is as a replacement for other scripting languages like Perl.After a surprising amount of effort and a couple of false starts this is the code I came up with. It\'s pretty fast but less trivial than I would\'ve expected: (fork it on GitHub)It could probably be cleaned up further, it was the result of trial and error.Generator based line reader: https://github.com/neurosnap/gen-readlinesIf you want to read a file line by line and writing this in another: I had the same problem and came up with above solution\nlooks simular to others but is aSync and can read large files very quicklyHopes this helpsIn most cases this should be enough:I have a little module which does this well and is used by quite a few other projects npm readline Note thay in node v10 there is a native readline module so I republished my module as linebyline https://www.npmjs.com/package/linebylineif you dont want to use the module the function is very simple:Another solution is to run logic via sequential executor nsynjs. It reads file line-by-line using node readline module, and it doesn\'t use promises or recursion, therefore not going to fail on large files. Here is how the code will looks like:Code above is based on this exampe: https://github.com/amaksr/nsynjs/blob/master/examples/node-readline/index.jsi use this:use this function on a stream and listen to the line events that is will emit.gr-While you should probably use the readline module as the top answer suggests, readline appears to be oriented toward command line interfaces rather than line reading. It\'s also a little bit more opaque regarding buffering. (Anyone who needs a streaming line oriented reader probably will want to tweak buffer sizes). The readline module is ~1000 lines while this, with stats and tests, is 34.Here\'s an even shorter version, without the stats, at 19 lines:I use below code the read lines after verify that its not a directory and its not included in the list of files need not to be check.