I\'m currently using GCC, but I discovered Clang recently and I\'m pondering switching. There is one deciding factor though - quality (speed, memory footprint, reliability) of binaries it produces - if gcc -O3can produce a binary that runs 1% faster or takes 1% less memory, it\'s a deal-breaker.Clang boasts better compile speeds and lower compile-time memory footprint than GCC, but I\'m really interested in benchmarks/comparisons of resulting compiled software - could you point me to some or describe your experiences?Here are some up-to-date albeit narrow findings of mine with GCC 4.7.2 \nand Clang 3.2 for C++.UPDATE: GCC 4.8.1 v clang 3.3 comparison appended below.UPDATE: GCC 4.8.2 v clang 3.4 comparison is appended to that.I maintain an OSS tool that is built for Linux with both GCC and Clang,\nand with Microsoft\'s compiler for Windows. The tool, coan, is a preprocessor \nand analyser of C/C++ source files and codelines of such: its \ncomputational profile majors on recursive-descent parsing and file-handling.\nThe development branch (to which these results pertain)\ncomprises at present around 11K LOC in about 90 files. It is coded,\nnow, in C++ that is rich in polymorphism and templates and but is still\nmired in many patches by its not-so-distant past in hacked-together C.\nMove semantics are not expressly exploited. It is single-threaded. I\nhave devoted no serious effort to optimizing it, while the "architecture"\nremains so largely ToDo.I employed Clang prior to 3.2 only as an experimental compiler\nbecause, despite its superior compilation speed and diagnostics, its\nC++11 standard support lagged the contemporary GCC version in the\nrespects exercised by coan. With 3.2, this gap has been closed.My Linux test harness for current coan development processes roughly \n70K sources files in a mixture of one-file parser test-cases, stress\ntests consuming 1000s of files and scenario tests consuming < 1K files.\nAs well as reporting the test results, the harness accumulates and \ndisplays the totals of files consumed and the run time consumed in coan \n(it just passes each coan command line to the Linux time command and \ncaptures and adds up the reported numbers). The timings are flattered \nby the fact that any number of tests which take 0 measurable time will \nall add up to 0, but the contribution of such tests is negligible. The \ntiming stats are displayed at the end of make check like this:I compared the test harness performance as between GCC 4.7.2 and\nClang 3.2, all things being equal except the compilers. As of Clang 3.2,\nI no longer require any preprocessor differentiation between code\ntracts that GCC will compile and Clang alternatives. I built to the \nsame C++ library (GCC\'s) in each case and ran all the comparisons\nconsecutively in the same terminal session.The default optimization level for my release build is -O2. I also\nsuccessfully tested builds at -O3. I tested each configuration 3\ntimes back-to-back and averaged the 3 outcomes, with the following\nresults. The number in a data-cell is the average number of \nmicroseconds consumed by the coan executable to process each of\nthe ~70K input files (read, parse and write output and diagnostics).Any particular application is very likely to have traits that play \nunfairly to a compiler\'s strengths or weaknesses. Rigorous benchmarking\nemploys diverse applications. With that well in mind, the noteworthy \nfeatures of these data are:A further interesting comparison of the two compilers emerged by accident\nshortly after those findings. Coan liberally employs smart pointers and\none such is heavily exercised in the file handling. This particular \nsmart-pointer type had been typedef\'d in prior releases for the sake of \ncompiler-differentiation, to be an std::unique_ptr<X> if the \nconfigured compiler had sufficiently mature support for its usage as \nthat, and otherwise an std::shared_ptr<X>. The bias to std::unique_ptr was\nfoolish, since these pointers were in fact transferred around,\nbut std::unique_ptr looked like the fitter option for replacing\nstd::auto_ptr at a point when the C++11 variants were novel to me.  In the course of experimental builds to gauge Clang 3.2\'s continued need\nfor this and similar differentiation, I inadvertently built \nstd::shared_ptr<X> when I had intended to build std::unique_ptr<X>, \nand was surprised to observe that the resulting executable, with default -O2\noptimization, was the fastest I had seen, sometimes achieving 184\nmsecs. per input file. With this one change to the source code,\nthe corresponding results were these;The points of note here are:Before and after the smart-pointer type change, Clang is able to build a \nsubstantially faster coan executable at -O3 optimisation, and it can\nbuild an equally faster executable at -O2 and -O3 when that \npointer-type is the best one - std::shared_ptr<X> - for the job.An obvious question that I am not competent to comment upon is why\nClang should be able to find a 25% -O2 speed-up in my application when\na heavily used smart-pointer-type is changed from unique to shared,\nwhile GCC is indifferent to the same change. Nor do I know whether I should\ncheer or boo the discovery that Clang\'s -O2 optimization harbours\nsuch huge sensitivity to the wisdom of my smart-pointer choices.UPDATE: GCC 4.8.1 v clang 3.3The corresponding results now are: The fact that all four executables now take a much greater average time than previously to process \n1 file does not reflect on the latest compilers\' performance. It is due to the\nfact that the later development branch of the test application has taken on lot of\nparsing sophistication in the meantime and pays for it in speed. Only the ratios are\nsignificant.The points of note now are not arrestingly novel:Comparing these results with those for GCC 4.7.2 and clang 3.2, it stands out that\nGCC has clawed back about a quarter of clang\'s lead at each optimization level. But\nsince the test application has been heavily developed in the meantime one cannot\nconfidently attribute this to a catch-up in GCC\'s code-generation. \n(This time, I have noted the application snapshot from which the timings were obtained\nand can use it again.)UPDATE: GCC 4.8.2 v clang 3.4I finished the update for GCC 4.8.1 v Clang 3.3 saying that I would\nstick to the same coan snaphot for further updates. But I decided\ninstead to test on that snapshot (rev. 301) and on the latest development \nsnapshot I have that passes its test suite (rev. 619). This gives the results a \nbit of longitude, and I had another motive:My original posting noted that I had devoted no effort to optimizing coan for\nspeed. This was still the case as of rev. 301. However, after I had built\nthe timing apparatus into the coan test harness, every time I ran the test suite\nthe performance impact of the latest changes stared me in the face. I saw that \nit was often surprisingly big and that the trend was more steeply negative than \nI felt to be merited by gains in functionality.By rev. 308 the average processing time per input file in the test suite had \nwell more than doubled since the first posting here. At that point I made a \nU-turn on my 10 year policy of not bothering about performance. In the intensive\nspate of revisions up to 619 performance was always a consideration and a\nlarge number of them went purely to rewriting key load-bearers on fundamentally\nfaster lines (though without using any non-standard compiler features to do so). It would be interesting to see each compiler\'s reaction to this\nU-turn,Here is the now familiar timings matrix for the latest two compilers\' builds of rev.301:coan - rev.301 resultsThe story here is only marginally changed from GCC-4.8.1 and Clang-3.3. GCC\'s showing\nis a trifle better. Clang\'s is a trifle worse. Noise could well account for this.\nClang still comes out ahead by -O2 and -O3 margins that wouldn\'t matter in most\napplications but would matter to quite a few.And here is the matrix for rev. 619.coan - rev.619 resultsTaking the 301 and the 619 figures side by side, several points speak out.I was aiming to write faster code, and both compilers emphatically vindicate\nmy efforts. But:GCC repays those efforts far more generously than Clang. At -O2 \noptimization Clang\'s 619 build is 46% faster than its 301 build: at -O3 Clang\'s\nimprovement is 31%. Good, but at each optimization level GCC\'s 619 build is\nmore than twice as fast as its 301.GCC more than reverses Clang\'s former superiority. And at each optimization \nlevel GCC now beats Clang by 17%.Clang\'s ability in the 301 build to get more leverage than GCC from -O3 optimization\nis gone in the 619 build. Neither compiler gains meaningfully from -O3.I was sufficiently surprised by this reversal of fortunes that I suspected I\nmight have accidentally made a sluggish build of clang 3.4 itself (since I built\nit from source). So I re-ran the 619 test with my distro\'s stock Clang 3.3. The\nresults were practically the same as for 3.4.So as regards reaction to the U-turn: On the numbers here, Clang has done much \nbetter than GCC at at wringing speed out of my C++ code when I was giving it no \nhelp. When I put my mind to helping, GCC did a much better job than Clang.I don\'t elevate that observation into a principle, but I take\nthe lesson that "Which compiler produces the better binaries?" is a question\nthat, even if you specify the test suite to which the answer shall be relative,\nstill is not a clear-cut matter of just timing the binaries. Is your better binary the fastest binary, or is it the one that best \ncompensates for cheaply crafted code? Or best compensates for expensively \ncrafted code that prioritizes maintainability and reuse over speed? It depends on the \nnature and relative weights of your motives for producing the binary, and of\nthe constraints under which you do so. And in any case, if you deeply care about building "the best" binaries then you \nhad better keep checking how successive iterations of compilers deliver on your \nidea of "the best" over successive iterations of your code.Phoronix did some benchmarks about this, but it is about a snapshot version of Clang/LLVM from a few months back. The results being that things were more-or-less a push; neither GCC nor Clang is definitively better in all cases.Since you\'d use the latest Clang, it\'s maybe a little less relevant. Then again, GCC 4.6 is slated to have some major optimizations for Core 2 and i7, apparently.I figure Clang\'s faster compilation speed will be nicer for original developers, and then when you push the code out into the world, Linux distro/BSD/etc. end-users will  use GCC for the faster binaries.The fact that Clang compiles code faster may not be as important as the speed of the resulting binary. However, here is a series of benchmarks.There is very little overall difference between GCC 4.8 and clang 3.3 in terms of speed of the resulting binary. In most cases code generated by both compilers performs similarly. Neither of these two compilers dominates the other one.Benchmarks telling that there is a significant performance gap between GCC and clang are coincidental.Program performance is affected by the choice of the compiler. If a developer or a group of developers is exclusively using GCC then the program can be expected to run slightly faster with GCC than with clang, and vice versa.From developer viewpoint, a notable difference between GCC 4.8+ and clang 3.3 is that GCC has the -Og command line option. This option enables optimizations that do not interfere with debugging, so for example it is always possible to get accurate stack traces. The absence of this option in clang makes clang harder to use as an optimizing compiler for some developers.The only way to determine this is to try it. FWIW I have seen some really good improvements using Apple\'s LLVM gcc 4.2 compared to the regular gcc 4.2 (for x86-64 code with quite a lot of SSE), but YMMV for different code bases. Assuming you\'re working with x86/x86-64 and that you really do care about the last few percent then you ought to try Intel\'s ICC too, as this can often beat gcc - you can get a 30 day evaluation license from intel.com and try it.Basically speaking, the answer is: it depends.\nThere are many many benchmarks focusing on different kinds of application.My benchmark on my app is:  gcc > icc > clang.There are rare IO, but many CPU float and data structure operations.compile flags is -Wall -g -DNDEBUG -O3.https://github.com/zhangyafeikimi/ml-pack/blob/master/gbdt/profile/benchmarkA peculiar difference I have noted on gcc 5.2.1 and clang 3.6.2 is\nthat if you have a critical loop like:Then gcc will, when compiling with -O3 or -O2, speculatively\nunroll the loop eight times. Clang will not unroll it at all. Through\ntrial and error I found that in my specific case with my program data,\nthe right amount of unrolling is five so gcc overshot and clang\nundershot. However, overshotting was more detrimental to peformance so\ngcc performed much worse here.I have no idea if the unrolling difference is a general trend or\njust something that was specific to my scenario.A while back I wrote a few garbage\ncollectors to teach myself more\nabout performance optimization in C. And the results I got is in my\nmind enough to slightly favor clang. Especially since garbage\ncollection is mostly about pointer chasing and copying memory.The results are (numbers in seconds):This is all pure C code and I make no claim about either compilers\nperformance when compiling C++ code.On Ubuntu 15.10, x86.64, and an AMD Phenom(tm) II X6 1090T processor.