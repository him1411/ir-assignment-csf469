We\'ve got a PHP application and want to count all the lines of code under a specific directory and its subdirectories.  We don\'t need to ignore comments, as we\'re just trying to get a rough idea. That command works great within a given directory, but ignores subdirectories.  I was thinking this might work, but it is returning 74, which is definitely not the case...What\'s the correct syntax to feed in all the files?  Try:The SLOCCount tool may help as well.It\'ll give an accurate source lines of code count for whatever\nhierarchy you point it at, as well as some additional stats.For another one-liner:works on names with spaces, only outputs one number.If using a decently recent version of Bash (or ZSH), it\'s much simpler:In the Bash shell this requires the globstar option to be set, otherwise the ** glob-operator is not recursive. To enable this setting, issueTo make this permanent, add it to one of the initialization files (~/.bashrc, ~/.bash_profile etc.).For everyone stuck with windows:After I run into some problems counting lines of code under Windows, I found cloc.Serves the same purpose of sloccount but works flawlessly on Windows.Usage and output example:On UNIX-like systems, there is a tool called cloc which provides code statistics. I ran in on a random directory in our code base it says:You didn\'t specify how many files are there or what is the desired output.\nIs this what You are looking for:?Yet another variation :)Edit: this will give the total sum, instead of file-by-file.More common and simple as for me, suppose you need to count files of different name extensions (say, also natives)There is a little tool called sloccount to count the lines of code in directory. It should be noted that it does more than you want as it ignores empty lines/comments, groups the results per programming language and calculates some statistics.Surprisingly there\'s no answer based on find\'s -exec and awk. Here we go:This snippet finds for all files (-type f). To find by file extension, use -name:POSIXLines in each file:Lines in each file, sorted by file pathLines in each file, sorted by number of lines, descendingTotal lines in all filesA straightforward one that will be fast, will use all the search/filtering power of find, not fail when there are too many files (number arguments overflow), work fine with files with funny symbols in their name, without using xargs, will not launch a uselessly high number of external commands (thanks to + for find\'s -exec). Here you go:for sources only:to filter, just use grepYou can also try CLOC (requires Perl)what you want is a simple for loop:I know the question is tagged as bash, but it seems that the problem you\'re trying to solve is also PHP related.Sebastian Bergmann wrote a tool called PHPLOC that does what you want and on top of that provides you with an overview of a project\'s complexity. This is an example of its report:As you can see, the information provided is a lot more useful from the perspective of a developer, because it can roughly tell you how complex a project is before you start working with it.Guessing no one will ever see this buried at the back... Yet none of the answers so far gets at the problem of filenames with spaces. Additionally, all that use xargs are subject to fail if total length of paths in the tree exceeds the shell environment size limit (defaults to a few megabytes in Linux).  Here is one that fixes these problems in a pretty direct manner. The subshell takes care of files with spaces. The awk totals the stream of individual file wc outputs, so ought never to run out of space. It also restricts the exec to files only (skipping directories):WC -L  ?  better use GREP -C ^wc -l ? Wrong!\nwc command counts new lines codes, not lines ! When last line in the file does not end with new line code, this will not counted!if you still want count lines, use  grep -c ^ , full example:finally, watch out for the wc -l trap    (counts enters, not lines !!!)Something different:This works out fine, but  you need to have at least one *.php file in the current folder or one of its subfolders, or else wc stallsIf you need just the total number of lines in let\'s say your PHP files you can use very simple one line command even under Windows if you have GnuWin32 installed. Like this:You need to specify where exactly is the find.exe otherwise the Windows provided FIND.EXE (from the old DOS-like commands) will be executed, since it is probably before the GnuWin32 in the environment PATH, and has different parameters and results.Please note that in the command above you should use back-quotes, not single quotes.If you want your results sorted by number of lines, you can just add | sort or | sort -r (-r for descending order) to the first answer, like so:Giving out the longest files first (ie. maybe these long files need some refactoring love?), and excluding some vendor directories:On OS X at least, the find+xarg+wc commands listed in some of the other answers prints "total" several times on large listings, and there is no complete total given. I was able to get a single total for .c files using the following command:find . -name \'*.c\' -print0 |xargs -0 wc -l|grep -v total|awk \'{ sum += $1; } END { print "SUM: " sum; }\'while I like the scripts I prefer this one as it also shows a per-file summary as long as a totalFor Windows, easy and quick tool is LocMetrics.very simplyI have busy box installed on my windows system. So here is what I did.Yet another command to get the sum of all files (Linux of course)Main difference from other answers: If you want to keep it simple, cut out the middleman and just call wc with all the filenames:Or in the modern syntax:Works as long as there are no spaces in any of the directory names or filenames. And as long as you don\'t have tens of thousands of files (modern shells support really long command lines). Your project has 74 files, so you\'ve got plenty of room to grow.