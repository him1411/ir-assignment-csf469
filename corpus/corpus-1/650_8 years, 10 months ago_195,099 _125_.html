I\'m looking for the fastest way to determine if a long value is a perfect square (i.e. its square root is another integer).  I\'ve done it the easy way, by using the built-in Math.sqrt() function, but I\'m wondering if there is a way to do it faster by restricting yourself to integer-only domain.  Maintaining a lookup table is impratical (since there are about 231.5 integers whose square is less than 263).Here is the very simple and straightforward way I\'m doing it now:Notes: I\'m using this function in many Project Euler problems.  So no one else will ever have to maintain this code.  And this kind of micro-optimization could actually make a difference, since part of the challenge is to do every algorithm in less than a minute, and this function will need to be called millions of times in some problems.Update 2:  A new solution posted by A. Rex has proven to be even faster.  In a run over the first 1 billion integers, the solution only required 34% of the time that the original solution used.  While the John Carmack hack is a little better for small values of n, the benefit compared to this solution is pretty small.Here is the A. Rex solution, converted to Java:Update:  I\'ve tried the different solutions presented below.The one suggestion which did show improvements was made by John D. Cook.  You can observe that the last hex digit (i.e. the last 4 bits) of a perfect square must be 0, 1, 4, or 9.  This means that 75% of numbers can be immediately eliminated as possible squares.  Implementing this solution resulted in about a 50% reduction in runtime.Working from John\'s suggestion, I investigated properties of the last n bits of a perfect square.  By analyzing the last 6 bits, I found that only 12 out of 64 values are possible for the last 6 bits.  This means 81% of values can be eliminated without using any math.  Implementing this solution gave an additional 8% reduction in runtime (compared to my original algorithm).  Analyzing more than 6 bits results in a list of possible ending bits which is too large to be practical.Here is the code that I have used, which runs in 42% of the time required by the original algorithm (based on a run over the first 100 million integers).  For values of n less than 410881, it runs in only 29% of the time required by the original algorithm.Notes:I figured out a method that works ~35% faster than your 6bits+Carmack+sqrt code, at least with my CPU (x86) and programming language (C/C++).  Your results may vary, especially because I don\'t know how the Java factor will play out.My approach is threefold:Even if this code doesn\'t work faster for you, I hope you enjoy some of the ideas it contains.  Complete, tested code follows, including the precomputed tables.I\'m pretty late to the party, but I hope to provide a better answer; shorter and (assuming my benchmark is correct) also much faster.The first test catches most non-squares quickly. It uses a 64-item table packed in a long, so there\'s no array access cost (indirection and bounds checks). For a uniformly random long, there\'s a 81.25% probability of ending here.The second test catches all numbers having an odd number of twos in their factorization. The method Long.numberOfTrailingZeros is very fast as it gets JIT-ed into a single i86 instruction.After dropping the trailing zeros, the third test handles numbers ending with 011, 101, or 111 in binary, which are no perfect squares. It also cares about negative numbers and also handles 0.The final test falls back to double arithmetic. As double has only 53 bits mantissa, \nthe conversion from long to double includes rounding for big values. Nonetheless, the test is  correct (unless the proof is wrong).Trying to incorporate the mod255 idea wasn\'t successful.You\'ll have to do some benchmarking.  The best algorithm will depend on the distribution of your inputs.Your algorithm may be nearly optimal, but you might want to do a quick check to rule out some possibilities before calling your square root routine.  For example, look at the last digit of your number in hex by doing a bit-wise "and."  Perfect squares can only end in 0, 1, 4, or 9 in base 16,  So for 75% of your inputs (assuming they are uniformly distributed) you can avoid a call to the square root in exchange for some very fast bit twiddling.Kip benchmarked the following code implementing the hex trick.  When testing numbers 1 through 100,000,000, this code ran twice as fast as the original.When I tested the analogous code in C++, it actually ran slower than the original. However, when I eliminated the switch statement, the hex trick once again make the code twice as fast.Eliminating the switch statement had little effect on the C# code.I was thinking about the horrible times I\'ve spent in Numerical Analysis course.And then I remember, there was this function circling around the \'net from the Quake Source code:Which basically calculates a square root, using Newton\'s approximation function (cant remember the exact name).It should be usable and might even be faster, it\'s from one of the phenomenal id software\'s game!It\'s written in C++ but it should not be too hard to reuse the same technique in Java once you get the idea:I originally found it at: http://www.codemaestro.com/reviews/9Newton\'s method explained at wikipedia: http://en.wikipedia.org/wiki/Newton%27s_methodYou can follow the link for more explanation of how it works, but if you don\'t care much, then this is roughly what I remember from reading the blog and from taking the Numerical Analysis course:The approximation function gives more precise values the more you iterate the function over the result. In Quake\'s case, one iteration is "good enough", but if it wasn\'t for you... then you could add as much iteration as you need.This should be faster because it reduces the number of division operations done in naive square rooting down to a simple divide by 2 (actually a * 0.5F multiply operation) and replace it with a few fixed number of multiplication operations instead.I\'m not sure if it would be faster, or even accurate, but you could use John Carmack\'s Magical Square Root, algorithm to solve the square root faster.  You could probably easily test this for all possible 32 bit integers, and validate that you actually got correct results, as it\'s only an appoximation.  However, now that I think about it, using doubles is approximating also, so I\'m not sure how that would come into play.If you do a binary chop to try to find the "right" square root, you can fairly easily detect if the value you\'ve got is close enough to tell:So having calculated n^2, the options are:(Sorry, this uses n as your current guess, and target for the parameter. Apologise for the confusion!)I don\'t know whether this will be faster or not, but it\'s worth a try.EDIT: The binary chop doesn\'t have to take in the whole range of integers, either (2^x)^2 = 2^(2x), so once you\'ve found the top set bit in your target (which can be done with a bit-twiddling trick; I forget exactly how) you can quickly get a range of potential answers. Mind you, a naive binary chop is still only going to take up to 31 or 32 iterations.I ran my own analysis of several of the algorithms in this thread and came up with some new results. You can see those old results in the edit history of this answer, but they\'re not accurate, as I made a mistake, and wasted time analyzing several algorithms which aren\'t close. However, pulling lessons from several different answers, I now have two algorithms that crush the "winner" of this thread. Here\'s the core thing I do differently than everyone else:However, this simple line, which most of the time adds one or two very fast instructions, greatly simplifies the switch-case statement into one if statement. However, it can add to the runtime if many of the tested numbers have significant power-of-two factors.The algorithms below are as follows:Here is a sample runtime if the numbers are generated using Math.abs(java.util.Random.nextLong())And here is a sample runtime if it\'s run on the first million longs only:As you can see, DurronTwo does better for large inputs, because it gets to use the magic trick very very often, but gets clobbered compared to the first algorithm and Math.sqrt because the numbers are so much smaller. Meanwhile, the simpler Durron is a huge winner because it never has to divide by 4 many many times in the first million numbers.Here\'s Durron:And DurronTwoAnd my benchmark harness: (Requires Google caliper 0.1-rc5)UPDATE: I\'ve made a new algorithm that is faster in some scenarios, slower in others, I\'ve gotten different benchmarks based on different inputs. If we calculate modulo 0xFFFFFF = 3 x 3 x 5 x 7 x 13 x 17 x 241, we can eliminate 97.82% of numbers that cannot be squares. This can be (sort of) done in one line, with 5 bitwise operations:The resulting index is either 1) the residue, 2) the residue + 0xFFFFFF, or 3) the residue + 0x1FFFFFE. Of course, we need to have a lookup table for residues modulo 0xFFFFFF, which is about a 3mb file (in this case stored as ascii text decimal numbers, not optimal but clearly improvable with a ByteBuffer and so forth. But since that is precalculation it doesn\'t matter so much. You can find the file here (or generate it yourself): I load it into a boolean array like this:Example runtime. It beat Durron (version one) in every trial I ran.It should be much faster to use Newton\'s method to calculate the Integer Square Root, then square this number and check, as you do in your current solution.  Newton\'s method is the basis for the Carmack solution mentioned in some other answers.  You should be able to get a faster answer since you\'re only interested in the integer part of the root, allowing you to stop the approximation algorithm sooner.Another optimization that you can try:  If the Digital Root of a number doesn\'t end in \n1, 4, 7, or 9 the number is not a perfect square.  This can be used as a quick way to eliminate 60% of your inputs before applying the slower square root algorithm.I want this function to work with all\n  positive 64-bit signed integersMath.sqrt() works with doubles as input parameters, so you won\'t get accurate results for integers bigger than 2^53.It\'s been pointed out that the last d digits of a perfect square can only take on certain values. The last d digits (in base b) of a number n is the same as the remainder when n is divided by bd, ie. in C notation n % pow(b, d).This can be generalized to any modulus m, ie. n % m can be used to rule out some percentage of numbers from being perfect squares. The modulus you are currently using is 64, which allows 12, ie. 19% of remainders, as possible squares. With a little coding I found the modulus 110880, which allows only 2016, ie. 1.8% of remainders as possible squares. So depending on the cost of a modulus operation (ie. division) and a table lookup versus a square root on your machine, using this modulus might be faster.By the way if Java has a way to store a packed array of bits for the lookup table, don\'t use it. 110880 32-bit words is not much RAM these days and fetching a machine word is going to be faster than fetching a single bit.Just for the record, another approach is to use the prime decomposition. If every factor of the decomposition is even, then the number is a perfect square. So what you want is to see if a number can be decomposed as a product of squares of prime numbers. Of course, you don\'t need to obtain such a decomposition, just to see if it exists.First build a table of squares of prime numbers which are lower than 2^32. This is far smaller than a table of all integers up to this limit.A solution would then be like this:I guess it\'s a bit cryptic. What it does is checking in every step that the square of a prime number divide the input number. If it does then it divides the number by the square as long as it is possible, to remove this square from the prime decomposition.\nIf by this process, we came to 1, then the input number was a decomposition of square of prime numbers. If the square becomes larger than the number itself, then there is no way this square, or any larger squares, can divide it, so the number can not be a decomposition of squares of prime numbers.Given nowadays\' sqrt done in hardware and the need to compute prime numbers here, I guess this solution is way slower. But it should give better results than solution with sqrt which won\'t work over 2^54, as says mrzl in his answer.For performance, you very often have to do some compromsies. Others have expressed various methods, however, you noted Carmack\'s hack was faster up to certain values of N. Then, you should check the "n" and if it is less than that number N, use Carmack\'s hack, else use some other method described in the answers here.An integer problem deserves an integer solution. ThusDo binary search on the (non-negative) integers to find the greatest integer t such that t**2 <= n. Then test whether r**2 = n exactly. This takes time O(log n). If you don\'t know how to binary search the positive integers because the set is unbounded, it\'s easy. You starting by computing your increasing function f (above f(t) = t**2 - n) on powers of two. When you see it turn positive, you\'ve found an upper bound. Then you can do standard binary search.You should get rid of the 2-power part of N right from the start.2nd Edit\nThe magical expression for m below should beand not as writtenEnd of 2nd edit1st Edit:Minor improvement:End of 1st editNow continue as usual. This way, by the time you get to the floating point part, you already got rid of all the numbers whose 2-power part is odd (about half), and then you only consider 1/8 of whats left. I.e. you run the floating point part on 6% of the numbers.This is the fastest Java implementation I could come up with, using a combination of techniques suggested by others in this thread.I also experimented with these modifications but they did not help performance:The following simplification of maaartinus\'s solution appears to shave a few percentage points off the runtime, but I\'m not good enough at benchmarking to produce a benchmark I can trust:It would be worth checking how omitting the first test,would affect performance.This a rework from decimal to binary of the old Marchant calculator algorithm (sorry, I don\'t have a reference), in Ruby, adapted specifically for this question:Here\'s a workup of something similar (please don\'t vote me down for coding style/smells or clunky O/O - it\'s the algorithm that counts, and C++ is not my home language). In this case, we\'re looking for residue == 0:The sqrt call is not perfectly accurate, as has been mentioned, but it\'s interesting and instructive that it doesn\'t blow away the other answers in terms of speed. After all, the sequence of assembly language instructions for a sqrt is tiny. Intel has a hardware instruction, which isn\'t used by Java I believe because it doesn\'t conform to IEEE.So why is it slow? Because Java is actually calling a C routine through JNI, and it\'s actually slower to do so than to call a Java subroutine, which itself is slower than doing it inline. This is very annoying, and Java should have come up with a better solution, ie building in floating point library calls if necessary. Oh well.In C++, I suspect all the complex alternatives would lose on speed, but I haven\'t checked them all.\nWhat I did, and what Java people will find usefull, is a simple hack, an extension of the special case testing suggested by A. Rex. Use a single long value as a bit array, which isn\'t bounds checked. That way, you have 64 bit boolean lookup.The routine isPerfectSquare5 runs in about 1/3 the time on my core2 duo machine. I suspect that further tweaks along the same lines could reduce the time further on average, but every time you check, you are trading off more testing for more eliminating, so you can\'t go too much farther on that road.Certainly, rather than having a separate test for negative, you could check the high 6 bits the same way.Note that all I\'m doing is eliminating possible squares, but when I have a potential case I have to call the original, inlined isPerfectSquare.The init2 routine is called once to initialize the static values of pp1 and pp2.\nNote that in my implementation in C++, I\'m using unsigned long long, so since you\'re signed, you\'d have to use the >>> operator.There is no intrinsic need to bounds check the array, but Java\'s optimizer has to figure this stuff out pretty quickly, so I don\'t blame them for that.Project Euler is mentioned in the tags and many of the problems in it require checking numbers >> 2^64.  Most of the optimizations mentioned above don\'t work easily when you are working with an 80 byte buffer.I used java BigInteger and a slightly modified version of Newton\'s method, one that works better with integers.  The problem was that exact squares n^2 converged to (n-1) instead of n because n^2-1 = (n-1)(n+1) and the final error was just one step below the final divisor and the algorithm terminated.  It was easy to fix by adding one to the original argument before computing the error.  (Add two for cube roots, etc.)One nice attribute of this algorithm is that you can immediately tell if the number is a perfect square - the final error (not correction) in Newton\'s method will be zero.  A simple modification also lets you quickly calculate floor(sqrt(x)) instead of the closest integer.  This is handy with several Euler problems.I like the idea to use an almost correct method on some of the input. Here is a version with a higher "offset". The code seems to work and passes my simple test case.Just replace your:code with this one:I checked all of the possible results when the last n bits of a square is observed. By successively examining more bits, up to 5/6th of inputs can be eliminated. I actually designed this to implement Fermat\'s Factorization algorithm, and it is very fast there.The last bit of pseudocode can be used to extend the tests to eliminate more values. The tests above are for k = 0, 1, 2, 3It first tests whether it has a  square residual with moduli of power of two, then it tests based on a final modulus, then it uses the Math.sqrt to do a final test. I came up with the idea from the top post, and attempted to extend upon it. I appreciate any comments or suggestions.Update: Using the test by a modulus, (modSq) and a modulus base of 44352, my test runs in 96% of the time of the one in the OP\'s update for numbers up to 1,000,000,000.Considering for general bit length (though I have used specific type here), I tried to design simplistic algo as below. Simple and obvious check for 0,1,2 or <0 is required initially.\nFollowing is simple in sense that it doesn\'t try to use any existing maths functions. Most of the operator can be replaced with bit-wise operators. I haven\'t tested with any bench mark data though. I\'m neither expert at maths or computer algorithm design in particular, I would love to see you pointing out problem. I know there is lots of improvement chances there.If you want speed, given that your integers are of finite size, I suspect that the quickest way would involve (a) partitioning the parameters by size (e.g. into categories by largest bit set), then checking the value against an array of perfect squares within that range. Don\'t know about fastest, but the simplest is to take the square root in the normal fashion, multiply the result by itself, and see if it matches your original value.Since we\'re talking integers here, the fasted would probably involve a collection where you can just make a lookup.If speed is a concern, why not partition off the most commonly used set of inputs and their values to a lookup table and then do whatever optimized magic algorithm you have come up with for the exceptional cases?Regarding the Carmac method, it seems like it would be quite easy just to iterate once more, which should double the number of digits of accuracy. It is, after all, an extremely truncated iterative method -- Newton\'s, with a very good first guess.Regarding your current best, I see two micro-optimizations:I.e:Even better might be a simpleObviously, it would be interesting to know how many numbers get culled at each checkpoint -- I rather doubt the checks are truly independent, which makes things tricky."I\'m looking for the fastest way to determine if a long value is a perfect square (i.e. its square root is another integer)." The answers are impressive, but I failed to see a simple check :check whether the first number on the right of the long it a member of the set (0,1,4,5,6,9) . If it is not, then it cannot possibly be a \'perfect square\' .eg.4567 - cannot be a perfect square.It ought to be possible to pack the \'cannot be a perfect square if the last X digits are N\' much more efficiently than that! I\'ll use java 32 bit ints, and produce enough data to check the last 16 bits of the number - that\'s 2048 hexadecimal int values....Ok. Either I have run into some number theory that is a little beyond me, or there is a bug in my code. In any case, here is the code:and here are the results:(ed: elided for poor performance in prettify.js; view revision history to see.)