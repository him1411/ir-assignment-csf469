Why does this bit of code,run more than 10 times faster than the following bit (identical except where noted)?when compiling with Visual Studio 2010 SP1. (I haven\'t tested with other compilers.)Welcome to the world of denormalized floating-point! They can wreak havoc on performance!!!Denormal (or subnormal) numbers are kind of a hack to get some extra values very close to zero out of the floating point representation. Operations on denormalized floating-point can be tens to hundreds of times slower than on normalized floating-point. This is because many processors can\'t handle them directly and must trap and resolve them using microcode.If you print out the numbers after 10,000 iterations, you will see that they have converged to different values depending on whether 0 or 0.1 is used.Here\'s the test code compiled on x64:Output:Note how in the second run the numbers are very close to zero.Denormalized numbers are generally rare and thus most processors don\'t try to handle them efficiently.To demonstrate that this has everything to do with denormalized numbers, if we flush denormals to zero by adding this to the start of the code:Then the version with 0 is no longer 10x slower and actually becomes faster. (This requires that the code be compiled with SSE enabled.)This means that rather than using these weird lower precision almost-zero values, we just round to zero instead.Timings: Core i7 920 @ 3.5 GHz:In the end, this really has nothing to do with whether it\'s an integer or floating-point. The 0 or 0.1f is converted/stored into a register outside of both loops. So that has no effect on performance.Using gcc and applying a diff to the generated assembly yields only this difference:The cvtsi2ssq one being 10 times slower indeed.Apparently, the float version uses an XMM register loaded from memory, while the int version converts a real int value 0 to float using the cvtsi2ssq instruction, taking a lot of time. Passing -O3 to gcc doesn\'t help. (gcc version 4.2.1.)(Using double instead of float doesn\'t matter, except that it changes the cvtsi2ssq into a cvtsi2sdq.)Update Some extra tests show that it is not necessarily the cvtsi2ssq instruction. Once eliminated (using a int ai=0;float a=ai; and using a instead of 0), the speed difference remains. So @Mysticial is right, the denormalized floats make the difference. This can be seen by testing values between 0 and 0.1f. The turning point in the above code is approximately at 0.00000000000000000000000000000001, when the loops suddenly takes 10 times as long.Update<<1 A small visualisation of this interesting phenomenon:You can clearly see the exponent (the last 9 bits) change to its lowest value, when denormalization sets in. At that point, simple addition becomes 20 times slower.An equivalent discussion about ARM can be found in StackÂ Overflow question Denormalized floating point in Objective-C?.It\'s due to denormalized floating-point use. How to get rid of both it and the performance penalty? Having scoured the Internet for ways of killing denormal numbers, it seems there is no "best" way to do this yet. I have found these three methods that may work best in different environments:Might not work in some GCC environments:Might not work in some Visual Studio environments: 1Appears to work in both GCC and Visual Studio:The Intel compiler has options to disable denormals by default on modern Intel CPUs. More details hereCompiler switches. -ffast-math, -msse or -mfpmath=sse will disable denormals and make a few other things faster, but unfortunately also do lots of other approximations that might break your code. Test carefully! The equivalent of fast-math for the Visual Studio compiler is /fp:fast but I haven\'t been able to confirm whether this also disables denormals.1In gcc you can enable FTZ and DAZ with this:also use gcc switches: -msse -mfpmath=sse(corresponding credits to Carl Hetherington [1])[1] http://carlh.net/plugins/denormals.php