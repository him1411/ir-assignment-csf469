I\'m reading a book where the author says that if( a < 901 ) is faster than if( a <= 900 ). Not exactly as in this simple example, but there are slight performance changes on loop complex code. I suppose this has to do something with generated machine code in case it\'s even true.No, it will not be faster on most architectures. You didn\'t specify, but on x86, all of the integral comparisons will be typically implemented in two machine instructions:Example (Edited for brevity) Compiled with $ gcc -m32 -S -masm=intel test.cCompiles to:AndCompiles to:So the only difference between the two is a jg versus a jge instruction. The two will take the same amount of time.I\'d like to address the comment that nothing indicates that the different jump instructions take the same amount of time.  This one is a little tricky to answer, but here\'s what I can give: In the Intel Instruction Set Reference, they are all grouped together under one common instruction, Jcc (Jump if condition is met). The same grouping is made together under the Optimization Reference Manual, in Appendix C. Latency and Throughput.Latency \xe2\x80\x94 The number of clock cycles that are required for the\n  execution core to  complete the execution of all of the \xce\xbcops that form\n  an instruction.Throughput \xe2\x80\x94 The number of clock cycles required to\n  wait before the issue  ports are free to accept the same instruction\n  again. For many instructions, the  throughput of an instruction can be\n  significantly less than its latencyThe values for Jcc are:with the following footnote on Jcc:7) Selection of conditional jump instructions should be based on the recommendation of section Section 3.4.1, \xe2\x80\x9cBranch Prediction Optimization,\xe2\x80\x9d to improve the  predictability of branches. When branches are predicted successfully, the latency of jcc is effectively zero.So, nothing in the Intel docs ever treats one Jcc instruction any differently from the others.If one thinks about the actual circuitry used to implement the instructions, one can assume that there would be simple AND/OR gates on the different bits in EFLAGS, to determine whether the conditions are met. There is then, no reason that an instruction testing two bits should take any more or less time than one testing only one (Ignoring gate propagation delay, which is much less than the clock period.)Edit: Floating PointThis holds true for x87 floating point as well:  (Pretty much same code as above, but with double instead of int.)Historically (we\'re talking the 1980s and early 1990s), there were some architectures in which this was true. The root issue is that integer comparison is inherently implemented via integer subtractions. This gives rise to the following cases.Now, when A < B the subtraction has to borrow a high-bit for the subtraction to be correct, just like you carry and borrow when adding and subtracting by hand. This "borrowed" bit was usually referred to as the carry bit and would be testable by a branch instruction. A second bit called the zero bit would be set if the subtraction were identically zero which implied equality.There were usually at least two conditional branch instructions, one to branch on the carry bit and one on the zero bit.Now, to get at the heart of the matter, let\'s expand the previous table to include the carry and zero bit results.So, implementing a branch for A < B can be done in one instruction, because the carry bit is clear only in this case, , that is,But, if we want to do a less-than-or-equal comparison, we need to do an additional check of the zero flag to catch the case of equality.So, on some machines, using a "less than" comparison might save one machine instruction.  This was relevant in the era of sub-megahertz processor speed and 1:1 CPU-to-memory speed ratios, but it is almost totally irrelevant today.Assuming we\'re talking about internal integer types, there\'s no possible way one could be faster than the other. They\'re obviously semantically identical. They both ask the compiler to do precisely the same thing. Only a horribly broken compiler would generate inferior code for one of these.If there was some platform where < was faster than <= for simple integer types, the compiler should always convert <= to < for constants. Any compiler that didn\'t would just be a bad compiler (for that platform).I see that neither is faster. The compiler generates the same machine code in each condition with a different value.My example if is from GCC on x86_64 platform on Linux.Compiler writers are pretty smart people, and they think of these things and many others most of us take for granted.I noticed that if it is not a constant, then the same machine code is generated in either case.For floating point code, the <= comparison may indeed be slower (by one instruction) even on modern architectures. Here\'s the first function:On PowerPC, first this performs a floating point comparison (which updates cr, the condition register), then moves the condition register to a GPR, shifts the "compared less than" bit into place, and then returns. It takes four instructions.Now consider this function instead:This requires the same work as compare_strict above, but now there\'s two bits of interest: "was less than" and "was equal to." This requires an extra instruction (cror - condition register bitwise OR) to combine these two bits into one. So compare_loose requires five instructions, while compare_strict requires four.You might think that the compiler could optimize the second function like so:However this will incorrectly handle NaNs. NaN1 <= NaN2 and NaN1 > NaN2 need to both evaluate to false.Maybe the author of that unnamed book has read that a > 0 runs faster than a >= 1 and thinks that is true universally.But it is because a 0 is involved (because CMP can, depending on the architecture, replaced e.g. with OR) and not because of the <.At the very least, if this were true a compiler could trivially optimise a <= b to !(a > b), and so even if the comparison itself were actually slower, with all but the most naive compiler you would not notice a difference.They have the same speed. Maybe in some special architecture what he/she said is right, but in the x86 family at least I know they are the same. Because for doing this the CPU will do a  substraction (a - b) and then check the flags of the flag register. Two bits of that register are called ZF (zero Flag) and SF (sign flag), and it is done in one cycle, because it will do it with one mask operation.This would be highly dependent on the underlying architecture that the C is compiled to.  Some processors and architectures might have explicit instructions for equal to, or less than and equal to, which execute in different numbers of cycles.That would be pretty unusual though, as the compiler could work around it, making it irrelevant.Other answers have concentrated on x86 architecture, and I don\'t know the ARM architecture (which your example assembler seems to be) well enough to comment specifically on the code generated, but this is an example of a micro-optimisation which is very architecture specific, and is as likely to be an anti-optimisation as it is to be an optimisation.As such, I would suggest that this sort of micro-optimisation is an example of cargo cult programming rather than best software engineering practice.There are probably some architectures where this is an optimisation, but I know of at least one architecture where the opposite may be true. The venerable Transputer architecture only had machine code instructions for equal to and greater than or equal to, so all comparisons had to be built from these primitives.Even then, in almost all cases, the compiler could order the evaluation instructions in such a way that in practice, no comparison had any advantage over any other. Worst case though, it might need to add a reverse instruction (REV) to swap the top two items on the operand stack. This was a single byte instruction which took a single cycle to run, so had the smallest overhead possible.Whether or not a micro-optimisation like this is an optimisation or an anti-optimisation depends on the specific architecture you are using, so it is usually a bad idea to get into the habit of using architecture specific micro-optimisations, otherwise you might instinctively use one when it is inappropriate to do so, and it looks like this is exactly what the book you are reading is advocating.You should not be able to notice the difference even if there is any.  Besides, in practice, you\'ll have to do an additional a + 1 or a - 1 to make the condition stand unless you\'re going to use some magic constants, which is a very bad practice by all means.You could say that line is correct in most scripting languages, since the extra character results in slightly slower code processing.\n However, as the top answer pointed out, it should have no effect in C++, and anything being done with a scripting language probably isn\'t that concerned about optimization.Actually, they will be exactly the same speed, because on the assembly level they both take one line. Such as:So no, neither is faster. But if you want to get really technical I think if you were to check it on an electron current level, it would be slightly faster but not anywhere near a speed you would notice.