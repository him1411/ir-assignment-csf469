I wanted to compare reading lines of string input from stdin using Python and C++ and was shocked to see my C++ code run an order of magnitude slower than the equivalent Python code. Since my C++ is rusty and I\'m not yet an expert Pythonista, please tell me if I\'m doing something wrong or if I\'m misunderstanding something.(TLDR answer: include the statement: cin.sync_with_stdio(false) or just use fgets instead.TLDR results: scroll all the way down to the bottom of my question and look at the table.)C++ code:Python Equivalent:Here are my results:Edit: I should note that I tried this both under Mac OS X v10.6.8 (Snow Leopard) and Linux 2.6.32 (Red Hat Linux 6.2). The former is a MacBook Pro, and the latter is a very beefy server, not that this is too pertinent.Edit 2: (Removed this edit, as no longer applicable)Edit 3:Okay, I tried J.N.\'s suggestion of trying having Python store the line read: but it made no difference to python\'s speed.I also tried J.N.\'s suggestion of using scanf into a char array instead of getline into a std::string. Bingo! This resulted in equivalent performance for both Python and C++. (3,333,333 LPS with my input data, which by the way are just short lines of three fields each, usually about 20 characters wide, though sometimes more).Code:Speed:(Yes, I ran it several times.) So, I guess I will now use scanf instead of getline. But, I\'m still curious if people think this performance hit from std::string/getline is typical and reasonable.Edit 4 (was: Final Edit / Solution):Adding:Immediately above my original while loop above results in code that runs faster than Python.New performance comparison (this is on my 2011 MacBook Pro), using the original code, the original with the sync disabled, and the original Python code, respectively, on a file with 20M lines of text. Yes, I ran it several times to eliminate disk caching confound.Thanks to @Vaughn Cato for his answer! Any elaboration people can make or good references people can point to as to why this synchronisation happens, what it means, when it\'s useful, and when it\'s okay to disable would be greatly appreciated by posterity. :-)Edit 5 / Better Solution:As suggested by Gandalf The Gray below, gets is even faster than scanf or the unsynchronized cin approach. I also learned that scanf and gets are both UNSAFE and should NOT BE USED due to potential of buffer overflow. So, I wrote this iteration using fgets, the safer alternative to gets. Here are the pertinent lines for my fellow noobs:Now, here are the results using an even larger file (100M lines; ~3.4 GB) on a fast server with very fast disk, comparing the Python code, the unsynchronised cin, and the fgets approaches, as well as comparing with the wc utility. [The scanf version segmentation faulted and I don\'t feel like troubleshooting it.]:As you can see, fgets is better, but still pretty far from wc performance; I\'m pretty sure this is due to the fact that wc examines each character without any memory copying. I suspect that, at this point, other parts of the code will become the bottleneck, so I don\'t think optimizing to that level would even be worthwhile, even if possible (since, after all, I actually need to store the read lines in memory).Also note that a small tradeoff with using a char * buffer and fgets vs. unsynchronised cin to string is that the latter can read lines of any length, while the former requires limiting input to some finite number.  In practice, this is probably a non-issue for reading most line-based input files, as the buffer can be set to a very large value that would not be exceeded by valid input.This has been educational. Thanks to all for your comments and suggestions.Edit 6:As suggested by J.F. Sebastian in the comments below, the GNU wc utility uses plain C read() (within the safe-read.c wrapper) to read chunks (of 16k bytes) at a time and count new lines. Here\'s a Python equivalent based on J.F.\'s code (just showing the relevant snippet that replaces the Python for loop:The performance of this version is quite fast (though still a bit slower than the raw C wc utility, of course):Again, it\'s a bit silly for me to compare C++ fgets/cin and the first python code on the one hand to wc -l and this last Python snippet on the other, as the latter two don\'t actually store the read lines, but merely count newlines. Still, it\'s interesting to explore all the different implementations and think about the performance implications. Thanks again!Edit 7: Tiny benchmark addendum and recapFor completeness, I thought I\'d update the read speed for the same file on the same box with the original (synced) C++ code. Again, this is for a 100M line file on a fast disk. Here\'s the complete table now:By default, cin is synchronized with stdio, which causes it to avoid any input buffering.  If you add this to the top of your main, you should see much better performance:Normally, when an input stream is buffered, instead of reading one character at a time, the stream will be read in larger chunks.  This reduces the number of system calls, which are typically relatively expensive.  However, since the FILE* based stdio and iostreams often have separate implementations and therefore separate buffers, this could lead to a problem if both were used together.  For example:If more input was read by cin than it actually needed, then the second integer value wouldn\'t be available for the scanf function, which has its own independent buffer.  This would lead to unexpected results.To avoid this, by default, streams are synchronized with stdio.  One common way to achieve this is to have cin read each character one at a time as needed using stdio functions.  Unfortunately, this introduces a lot of overhead.  For small amounts of input, this isn\'t a big problem, but when you are reading millions of lines, the performance penalty is significant.Fortunately, the library designers decided that you should also be able to disable this feature to get improved performance if you knew what you were doing, so they provided the sync_with_stdio method.Just out of curiosity I\'ve taken a look at what happens under the hood, and I\'ve used dtruss/strace on each test.C++syscalls sudo dtruss -c ./a.out < inPythonsyscalls sudo dtruss -c ./a.py < inI reproduced the original result on my computer using g++ on a Mac.Adding the following statements to the C++ version just before the while loop brings it inline with the Python version:sync_with_stdio improved speed to 2 seconds, and setting a larger buffer brought it down to 1 second.I\'m a few years behind here, but:In \'Edit 4/5/6\' of the original post, you are using the construction:This is wrong in a couple of different ways:You\'re actually timing the execution of `cat`, not your benchmark.  The \'user\' and \'sys\' CPU usage displayed by `time` are those of `cat`, not your benchmarked program.  Even worse, the \'real\' time is also not necessarily accurate.  Depending on the implementation of `cat` and of pipelines in your local OS, it is possible that `cat` writes a final giant buffer and exits long before the reader process finishes its work.Use of `cat` is unnecessary and in fact counterproductive; you\'re adding moving parts.  If you were on a sufficiently old system (i.e.  with a single CPU and -- in certain generations of computers -- I/O faster than CPU) -- the mere fact that `cat` was running could substantially color the results.  You are also subject to whatever input and output buffering and other processing `cat` may do.  (This would likely earn you a \'Useless Use Of Cat\' award if I were Randal Schwartz: https://en.wikipedia.org/wiki/Cat_(Unix)#Useless_use_of_cat)A better construction would be:In this statement it is the shell which opens big_file, passing it to your program (well, actually to `time` which then executes your program as a subprocess) as an already-open file descriptor.  100% of the file reading is strictly the responsibility of the program you\'re trying to benchmark.  This gets you a real reading of its performance without spurious complications.I will mention two possible, but actually wrong, \'fixes\' which could also be considered (but I \'number\' them differently as these are not things which were wrong in the original post):A. You could \'fix\' this by timing only your program:B. or by timing the entire pipeline:These are wrong for the same reasons as #2: they\'re still using `cat` unnecessarily.  I mention them for a few reasons:they\'re more \'natural\' for people who aren\'t entirely comfortable with the I/O redirection facilities of the POSIX shellthere may be cases where `cat` is needed (e.g.: the file to be read requires some sort of privilege to access, and you do not want to grant that privilege to the program to be benchmarked: `sudo cat /dev/sda | /usr/bin/time my_compression_test --no-output`)in practice, on modern machines, the added `cat` in the pipeline is probably of no real consequenceBut I say that last thing with some hesitation.  If we examine the last result in \'Edit 5\' ---- this claims that `cat` consumed 74% of the CPU during the test; and indeed 1.34/1.83 is approximately 74%.  Perhaps a run of:would have taken only the remaining .49 seconds!  Probably not: `cat` here had to pay for the read() system calls (or equivalent) which transferred the file from \'disk\' (actually buffer cache), as well as the pipe writes to deliver them to `wc`.  The correct test would still have had to do those read() calls; only the write-to-pipe and read-from-pipe calls would have been saved, and those should be pretty cheap.Still, I predict you would be able to measure the difference between `cat file | wc -l` and `wc -l < file` and find a noticeable (2-digit percentage) difference.  Each of the slower tests will have paid a similar penalty in absolute time; which would however amount to a smaller fraction of its larger total time.In fact I did some quick tests with a 1.5 gigabyte file of garbage, on a Linux 3.13 (Ubuntu 14.04) system, obtaining these results (these are actually \'best of 3\' results; after priming the cache, of course): Notice that the two pipeline results claim to have taken more CPU time (user+sys) than realtime.  This is because I\'m using the shell (Bash)\'s built-in \'time\' command, which is cognizant of the pipeline; and I\'m on a multi-core machine where separate processes in a pipeline can use separate cores, accumulating CPU time faster than realtime.  Using /usr/bin/time I see smaller CPU time than realtime -- showing that it can only time the single pipeline element passed to it on its command line.  Also, the shell\'s output gives milliseconds while /usr/bin/time only gives hundreths of a second.So at the efficiency level of `wc -l`, the `cat` makes a huge difference: 409 / 283 = 1.453 or 45.3% more realtime, and 775 / 280 = 2.768, or a whopping 177% more CPU used!  On my random it-was-there-at-the-time test box.I should add that there is at least one other significant difference between these styles of testing, and I can\'t say whether it is a benefit or fault; you have to decide this yourself:When you run `cat big_file | /usr/bin/time my_program`, your program is receiving input from a pipe, at precisely the pace sent by `cat`, and in chunks no larger than written by `cat`.When you run `/usr/bin/time my_program < big_file`, your program receives an open file descriptor to the actual file.  Your program -- or in many cases the I/O libraries of the language in which it was written -- may take different actions when presented with a file descriptor referencing a regular file.  It may use mmap(2) to map the input file into its address space, instead of using explicit read(2) system calls.  These differences could have a far larger effect on your benchmark results than the small cost of running the `cat` binary.Of course it is an interesting benchmark result if the same program performs significantly differently between the two cases.  It shows that, indeed, the program or its I/O libraries are doing something interesting, like using mmap().  So in practice it might be good to run the benchmarks both ways; perhaps discounting the `cat` result by some small factor to "forgive" the cost of running `cat` itself.Getline, stream operatoes, scanf, can be convenient if you don\'t care about file loading time or if you are loading small text files...but if performance is something you care about, you should really just buffer the entire file into memory (assuming it will fit).  Here\'s an example:If you want, you can wrap a stream around that buffer for more convenient access like this:Also, if you are in control of the file, consider using a flat binary data format instead of text.  It\'s more reliable to read and write because you don\'t have to deal with all the ambiguities of whitespace.  It\'s also smaller and much faster to parse.By the way, the reason the line count for the C++ version is one greater than the count for the Python version is that the eof flag only gets set when an attempt is made to read beyond eof. So the correct loop would be:In your second example (with scanf()) reason why this is still slower might be because scanf("%s") parses string and looks for any space char (space, tab, newline).Also, yes, CPython does some caching to avoid harddisk reads.A first element of an answer: <iostream> is slow. Damn slow. I get a huge performance boost with scanf as in the below, but it is still two times slower than Python.The following code was faster for me than the other code posted here so far:\n(Visual Studio 2013, 64-bit, 500 MB file with line length uniformly in [0, 1000)).It beats all my Python attempts by more than a factor 2.Well, I see that in your second solution you switched from cin to scanf, which was the first suggestion I was going to make you (cin is sloooooooooooow). Now, if you switch from scanf to fgets, you would see another boost in performance: fgets is the fastest C++ function for string input.BTW, didn\'t know about that sync thing, nice. But you should still try fgets.