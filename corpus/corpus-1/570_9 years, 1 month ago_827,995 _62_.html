What is the best way to remove duplicate rows from a fairly large table (i.e. 300,000+ rows)?The rows of course will not be perfect duplicates because of the existence of the RowID identity field.Assuming no nulls, you GROUP BY the unique columns, and SELECT the MIN (or MAX) RowId as the row to keep. Then, just delete everything that didn\'t have a row id:In case you have a GUID instead of an integer, you can replacewithAnother possible way of doing this isI am using ORDER BY (SELECT 0) above as it is arbitrary which row to preserve in the event of a tie. To preserve the latest one in RowID order for example you could use ORDER BY RowID DESC Execution PlansThe execution plan for this is often simpler and more efficient than that in the accepted answer as it does not require the self join.This is not always the case however. One place where the GROUP BY solution might be preferred is situations where a hash aggregate would be chosen in preference to a stream aggregate. The ROW_NUMBER solution will always give pretty much the same plan whereas the GROUP BY strategy is more flexible.Factors which might favour the hash aggregate approach would be In extreme versions of this second case (if there are very few groups with many duplicates in each) one could also consider simply inserting the rows to keep into a new table then TRUNCATE-ing the original and copying them back to minimise logging compared to deleting a very high proportion of the rows.There\'s a good article on removing duplicates on the Microsoft Support site. It\'s pretty conservative - they have you do everything in separate steps - but it should work well against large tables.I\'ve used self-joins to do this in the past, although it could probably be prettied up with a HAVING clause:The following query is useful to delete duplicate rows. The table in this example has ID as an identity column and the columns which have duplicate data are Column1, Column2 and Column3.The following script shows usage of GROUP BY, HAVING, ORDER BY in one query, and returns the results with duplicate column and its count.Postgres: This will delete duplicate rows, except the first rowRefer (http://www.codeproject.com/Articles/157977/Remove-Duplicate-Rows-from-a-Table-in-SQL-Server)Quick and Dirty to delete exact duplicated rows (for small tables):I would prefer CTE for deleting duplicate rows from sql server tablestrongly recommend to follow this article ::http://dotnetmob.com/sql-server-article/delete-duplicate-rows-in-sql-server/by keeping originalwithout keeping originalI prefer the subquery\\having count(*) > 1 solution to the inner join because I found it easier to read and it was very easy to turn into a SELECT statement to verify what would be deleted before you run it. Yet another easy solution can be found at the link pasted here. This one easy to grasp and seems to be effective for most of the similar problems. It is for SQL Server though but the concept used is more than acceptable.Here are the relevant portions from the linked page:Consider this data:So how can we delete those duplicate data?First, insert an identity column in that table by using the following code:Use the following code to resolve it:I thought I\'d share my solution since it works under special circumstances.\nI my case the table with duplicate values did not have a foreign key (because the values were duplicated from another db).PS: when working on things like this I always use a transaction, this not only ensures everything is executed as a whole, but also allows me to test without risking anything. But off course you should take a backup anyway just to be sure...Using CTE:This query showed very good performance for me:it deleted 1M rows in little more than 30sec from a table of 2M (50% duplicates)Oh sure. Use a temp table. If you want a single, not-very-performant statement that "works" you can go with:Basically, for each row in the table, the sub-select finds the top RowID of all rows that are exactly like the row under consideration. So you end up with a list of RowIDs that represent the "original" non-duplicated rows.Here is another good article on removing duplicates.It discusses why its hard: "SQL is based on relational algebra, and duplicates cannot occur in relational algebra, because duplicates are not allowed in a set."The temp table solution, and two mysql examples.In the future are you going to prevent it at a database level, or from an application perspective.  I would suggest the database level because your database should be responsible for maintaining referential integrity, developers just will cause problems ;)I had a table where I needed to preserve non-duplicate rows.\nI\'m not sure on the speed or efficiency.The other way is Create a new table with same fields and with Unique Index. Then move all data from old table to new table. Automatically SQL SERVER ignore (there is also an option about what to do if there will be a duplicate value: ignore, interrupt or sth) duplicate values. So we have the same table without duplicate rows. If you don\'t want Unique Index, after the transfer data you can drop it.Especially for larger tables you may use DTS (SSIS package to import/export data) in order to transfer all data rapidly to your new uniquely indexed table. For 7 million row it takes just a few minute.Use thisBy useing below query we can able to delete duplicate records based on the single column or multiple column. below query is deleting based on two columns. table name is: testing and column names empno,empnameI would mention this approach as well as it can be helpful, and works in all SQL servers:\nPretty often there is only one - two duplicates, and  Ids and count of duplicates are known. In this case:Create new blank table with the same structureExecute query like thisThen execute this queryFrom the application level (unfortunately). I agree that the proper way to prevent duplication is at the database level through the use of a unique index, but in SQL Server 2005, an index is allowed to be only 900 bytes, and my varchar(2048) field blows that away.I dunno how well it would perform, but I think you could write a trigger to enforce this, even if you couldn\'t do it directly with an index.  Something like:Also, varchar(2048) sounds fishy to me (some things in life are 2048 bytes, but it\'s pretty uncommon); should it really not be varchar(max)?Another way of doing this :--To Fetch Duplicate Rows:To Delete the Duplicate Rows:I you want to preview the rows you are about to remove and keep control over which of the duplicate rows to keep. See http://developer.azurewebsites.net/2014/09/better-sql-group-by-find-duplicate-data/