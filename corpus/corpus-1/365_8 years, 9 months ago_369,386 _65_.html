I have a C++ application, running on Linux, which I\'m in the process of optimizing. How can I pinpoint which areas of my code are running slowly?If your goal is to use a profiler, use one of the suggested ones.However, if you\'re in a hurry and you can manually interrupt your program under the debugger while it\'s being subjectively slow, there\'s a simple way to find performance problems.Just halt it several times, and each time look at the call stack. If there is some code that is wasting some percentage of the time, 20% or 50% or whatever, that is the probability that you will catch it in the act on each sample. So that is roughly the percentage of samples on which you will see it. There is no educated guesswork required.\nIf you do have a guess as to what the problem is, this will prove or disprove it.You may have multiple performance problems of different sizes. If you clean out any one of them, the remaining ones will take a larger percentage, and be easier to spot, on subsequent passes.\nThis magnification effect, when compounded over multiple problems, can lead to truly massive speedup factors.Caveat: Programmers tend to be skeptical of this technique unless they\'ve used it themselves. They will say that profilers give you this information, but that is only true if they sample the entire call stack, and then let you examine a random set of samples. (The summaries are where the insight is lost.) Call graphs don\'t give you the same information, because They will also say it only works on toy programs, when actually it works on any program, and it seems to work better on bigger programs, because they tend to have more problems to find.\nThey will say it sometimes finds things that aren\'t problems, but that is only true if you see something once. If you see a problem on more than one sample, it is real.P.S. This can also be done on multi-thread programs if there is a way to collect call-stack samples of the thread pool at a point in time, as there is in Java.P.P.S As a rough generality, the more layers of abstraction you have in your software, the more likely you are to find that that is the cause of performance problems (and the opportunity to get speedup).Added: It might not be obvious, but the stack sampling technique works equally well in the presence of recursion. The reason is that the time that would be saved by removal of an instruction is approximated by the fraction of samples containing it, regardless of the number of times it may occur within a sample.Another objection I often hear is: "It will stop someplace random, and it will miss the real problem".\nThis comes from having a prior concept of what the real problem is.\nA key property of performance problems is that they defy expectations.\nSampling tells you something is a problem, and your first reaction is disbelief.\nThat is natural, but you can be sure if it finds a problem it is real, and vice-versa.ADDED: Let me make a Bayesian explanation of how it works.  Suppose there is some instruction I (call or otherwise) which is on the call stack some fraction f of the time (and thus costs that much). For simplicity, suppose we don\'t know what f is, but assume it is either 0.1, 0.2, 0.3, ... 0.9, 1.0, and the prior probability of each of these possibilities is 0.1, so all of these costs are equally likely a-priori.Then suppose we take just 2 stack samples, and we see instruction I on both samples, designated observation o=2/2. This gives us new estimates of the frequency f of I, according to this:The last column says that, for example, the probability that f >= 0.5 is 92%, up from the prior assumption of 60%.Suppose the prior assumptions are different. Suppose we assume P(f=0.1) is .991 (nearly certain), and all the other possibilities are almost impossible (0.001). In other words, our prior certainty is that I is cheap. Then we get:Now it says P(f >= 0.5) is 26%, up from the prior assumption of 0.6%. So Bayes allows us to update our estimate of the probable cost of I. If the amount of data is small, it doesn\'t tell us accurately what the cost is, only that it is big enough to be worth fixing.Yet another way to look at it is called the Rule Of Succession.\nIf you flip a coin 2 times, and it comes up heads both times, what does that tell you about the probable weighting of the coin?\nThe respected way to answer is to say that it\'s a Beta distribution, with average value (number of hits + 1) / (number of tries + 2) = (2+1)/(2+2) = 75%.(The key is that we see I more than once. If we only see it once, that doesn\'t tell us much except that f > 0.)So, even a very small number of samples can tell us a lot about the cost of instructions that it sees. (And it will see them with a frequency, on average, proportional to their cost. If n samples are taken, and f is the cost, then I will appear on nf+/-sqrt(nf(1-f)) samples. Example, n=10, f=0.3, that is 3+/-1.4 samples.)ADDED, to give an intuitive feel for the difference between measuring and random stack sampling:\nThere are profilers now that sample the stack, even on wall-clock time, but what comes out is measurements (or hot path, or hot spot, from which a "bottleneck" can easily hide). What they don\'t show you (and they easily could) is the actual samples themselves. And if your goal is to find the bottleneck, the number of them you need to see is, on average, 2 divided by the fraction of time it takes.\nSo if it takes 30% of time, 2/.3 = 6.7 samples, on average, will show it, and the chance that 20 samples will show it is 99.2%.Here is an off-the-cuff illustration of the difference between examining measurements and examining stack samples.\nThe bottleneck could be one big blob like this, or numerous small ones, it makes no difference.Measurement is horizontal; it tells you what fraction of time specific routines take.\nSampling is vertical.\nIf there is any way to avoid what the whole program is doing at that moment, and if you see it on a second sample, you\'ve found the bottleneck.\nThat\'s what makes the difference - seeing the whole reason for the time being spent, not just how much.You can use Valgrind with the following optionsIt will generate a file called callgrind.out.x. You can then use kcachegrind tool to read this file. It will give you a graphical analysis of things with results like which lines cost how much. I assume you\'re using GCC. The standard solution would be to profile with gprof.Be sure to add -pg to compilation before profiling:I haven\'t tried it yet but I\'ve heard good things about google-perftools. It is definitely worth a try.Related question here.A few other buzzwords if gprof does not do the job for you: Valgrind, Intel VTune, Sun DTrace.Newer kernels (e.g. the latest Ubuntu kernels) come with the new \'perf\' tools (apt-get install linux-tools) AKA perf_events.These come with classic sampling profilers (man-page) as well as the awesome timechart!The important thing is that these tools can be system profiling and not just process profiling - they can show the interaction between threads, processes and the kernel and let you understand the scheduling and I/O dependencies between processes.I would use Valgrind and Callgrind as a base for my profiling tool suite. What is important to know is that Valgrind is basically a Virtual Machine:(wikipedia) Valgrind is in essence a virtual\n  machine using just-in-time (JIT)\n  compilation techniques, including\n  dynamic recompilation. Nothing from\n  the original program ever gets run\n  directly on the host processor.\n  Instead, Valgrind first translates the\n  program into a temporary, simpler form\n  called Intermediate Representation\n  (IR), which is a processor-neutral,\n  SSA-based form. After the conversion,\n  a tool (see below) is free to do\n  whatever transformations it would like\n  on the IR, before Valgrind translates\n  the IR back into machine code and lets\n  the host processor run it. Callgrind is a profiler build upon that. Main benefit is that you don\'t have to run your aplication for hours to get reliable result. Even one second run is sufficient to get rock-solid, reliable results, because Callgrind is a non-probing profiler. Another tool build upon Valgrind is Massif. I use it to profile heap memory usage. It works great. What it does is that it gives you snapshots of memory usage -- detailed information WHAT holds WHAT percentage of memory, and WHO had put it there. Such information is available at different points of time of application run.This is a response to Nazgob\'s Gprof answer.I\'ve been using Gprof the last couple of days and have already found three significant limitations, one of which I\'ve not seen documented anywhere else (yet):It doesn\'t work properly on multi-threaded code, unless you use a workaroundThe call graph gets confused by function pointers. Example: I have a function called multithread() which enables me to multi-thread a specified function over a specified array (both passed as arguments). Gprof however, views all calls to multithread() as equivalent for the purposes of computing time spent in children. Since some functions I pass to multithread() take much longer than others my call graphs are mostly useless. (To those wondering if threading is the issue here: no, multithread() can optionally, and did in this case, run everything sequentially on the calling thread only).It says here that "... the number-of-calls figures are derived by counting, not sampling. They are completely accurate...". Yet I find my call graph giving me 5345859132+784984078 as call stats to my most-called function, where the first number is supposed to be direct calls, and the second recursive calls (which are all from itself). Since this implied I had a bug, I put in long (64-bit) counters into the code and did the same run again. My counts: 5345859132 direct, and 78094395406 self-recursive calls.  There are a lot of digits there, so I\'ll point out the recursive calls I measure are 78bn, versus 784m from Gprof: a factor of 100 different. Both runs were single threaded and unoptimised code, one compiled -g and the other -pg.This was GNU Gprof (GNU Binutils for Debian) 2.18.0.20080103 running under 64-bit Debian Lenny, if that helps anyone.The answer to run valgrind --tool=callgrind is not quite complete without some options. We usually do not want to profile 10 minutes of slow startup time under Valgrind and want to profile our program when it is doing some task.So this is what I recommend. Run program first:Now when it works and we want to start profiling we should run in another window:This turns profiling on. To turn it off and stop whole task we might use:Now we have some files named callgrind.out.* in current directory. To see profiling results use:I recommend in next window to click on "Self" column header, otherwise it shows that "main()" is most time consuming task. "Self" shows how much each function itself took time, not together with dependents. Use Valgrind, callgrind and kcachegrind: generates callgrind.out.x. Read it using kcachegrind.Use gprof (add -pg): (not so good for multi-threads, function pointers)Use google-perftools: Uses time sampling, reveals I/O and CPU bottlenecks are revealed.Intel VTune is the best (free for educational purposes).Others: AMD Codeanalyst, OProfile, \'perf\' tools (apt-get install linux-tools)These are the two methods I use for speeding up my code:For CPU bound applications:For I/O bound applications:N.B.If you don\'t have a profiler, use the poor man\'s profiler. Hit pause while debugging your application. Most developer suites will break into assembly with commented line numbers. You\'re statistically likely to land in a region that is eating most of your CPU cycles.For CPU, the reason for profiling in DEBUG mode is because if your tried profiling in RELEASE mode, the compiler is going to reduce math, vectorize loops, and inline functions which tends to glob your code into an un-mappable mess when it\'s assembled. An un-mappable mess means your profiler will not be able to clearly identify what is taking so long because the assembly may not correspond to the source code under optimization. If you need the performance (e.g. timing sensitive) of RELEASE mode, disable debugger features as needed to keep a usable performance.For I/O-bound, the profiler can still identify I/O operations in RELEASE mode because I/O operations are either externally linked to a shared library (most of the time) or in the worst case, will result in a sys-call interrupt vector (which is also easily identifiable by the profiler).