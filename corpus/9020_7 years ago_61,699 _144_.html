I\'ve always thought it\'s the general wisdom that std::vector is "implemented as an array," blah blah blah. Today I went down and tested it, and it seems to be not so:Here\'s some test results:That\'s about 3 - 4 times slower! Doesn\'t really justify for the "vector may be slower for a few nanosecs" comments.And the code I used:Am I doing it wrong or something? Or have I just busted this performance myth?I\'m using Release mode in Visual Studio 2005.In Visual C++, #define _SECURE_SCL 0 reduces UseVector by half (bringing it down to 4 seconds). This is really huge, IMO.Using the following:g++ -O3 Time.cpp -I <MyBoost>\n  ./a.out\n  UseArray completed in 2.196 seconds\n  UseVector completed in 4.412 seconds\n  UseVectorPushBack completed in 8.017 seconds\n  The whole thing completed in 14.626 seconds  So array is twice as quick as vector. But after looking at the code in more detail this is expected; as you run across the vector twice and the array only once. Note: when you resize() the vector you are not only allocating the memory but also running through the vector and calling the constructor on each member.Re-Arranging the code slightly so that the vector only initializes each object once:Now doing the same timing again:g++ -O3 Time.cpp -I <MyBoost>\n  ./a.out\n  UseVector completed in 2.216 seconds  The vector now performance only slightly worse than the array. IMO this difference is insignificant and could be caused by a whole bunch of things not associated with the test.I would also take into account that you are not correctly initializing/Destroying the Pixel object in the UseArrray() method as neither constructor/destructor is not called (this may not be an issue for this simple class but anything slightly more complex (ie with pointers or members with pointers) will cause problems.Great question. I came in here expecting to find some simple fix that would speed the vector tests right up. That didn\'t work out quite like I expected!Optimization helps, but it\'s not enough. With optimization on I\'m still seeing a 2X performance difference between UseArray and UseVector. Interestingly, UseVector was significantly slower than UseVectorPushBack without optimization.I tried changing malloc() to new[] in UseArray so the objects would get constructed. And changing from individual field assignment to assigning a Pixel instance. Oh, and renaming the inner loop variable to j.Surprisingly (to me), none of those changes made any difference whatsoever. Not even the change to new[] which will default construct all of the Pixels. It seems that gcc can optimize out the default constructor calls when using new[], but not when using vector.I also attempted to get rid of the triple operator[] lookup and cache the reference to pixels[j]. That actually slowed UseVector down! Oops.What about removing the constructors entirely? Then perhaps gcc can optimize out the construction of all of the objects when the vectors are created. What happens if we change Pixel to:Result: about 10% faster. Still slower than an array. Hm.How about using a vector<Pixel>::iterator instead of a loop index?Result:Nope, no different. At least it\'s not slower. I thought this would have performance similar to #2 where I used a Pixel& reference.Even if some smart cookie figures out how to make the vector loop as fast as the array one, this does not speak well of the default behavior of std::vector. So much for the compiler being smart enough to optimize out all the C++ness and make STL containers as fast as raw arrays.The bottom line is that the compiler is unable to optimize away the no-op default constructor calls when using std::vector. If you use plain new[] it optimizes them away just fine. But not with std::vector. Even if you can rewrite your code to eliminate the constructor calls that flies in face of the mantra around here: "The compiler is smarter than you. The STL is just as fast as plain C. Don\'t worry about it."To be fair, you cannot compare a C++ implementation to a C implementation, as I would call your malloc version. malloc does not create objects - it only allocates raw memory. That you then treat that memory as objects without calling the constructor is poor C++ (possibly invalid - I\'ll leave that to the language lawyers).That said, simply changing the malloc to new Pixel[dimensions*dimensions] and free to delete [] pixels does not make much difference with the simple implementation of Pixel that you have. Here\'s the results on my box (E6600, 64-bit):But with a slight change, the tables turn:Compiled this way:we get very different results:With a non-inlined constructor for Pixel, std::vector now beats a raw array.It would appear that the complexity of allocation through std::vector and std:allocator is too much to be optimised as effectively as a simple new Pixel[n]. However, we can see that the problem is simply with the allocation not the vector access by tweaking a couple of the test functions to create the vector/array once by moving it outside the loop:andWe get these results now:What we can learn from this is that std::vector is comparable to a raw array for access, but if you need to create and delete the vector/array many times, creating a complex object will be more time consuming that creating a simple array when the element\'s constructor is not inlined. I don\'t think that this is very surprising.This is an old but popular question.At this point, many programmers will be working in C++11.  And in C++11 the OP\'s code as written runs equally fast for UseArray or UseVector.The fundamental problem was that while your Pixel structure was uninitialized, std::vector<T>::resize( size_t, T const&=T() ) takes a default constructed Pixel and copies it.  The compiler did not notice it was being asked to copy uninitialized data, so it actually performed the copy.In C++11, std::vector<T>::resize has two overloads.  The first is std::vector<T>::resize(size_t), the other is std::vector<T>::resize(size_t, T const&).  This means when you invoke resize without a second argument, it simply default constructs, and the compiler is smart enough to realize that default construction does nothing, so it skips the pass over the buffer.(The two overloads where added to handle movable, constructable and non-copyable types -- the performance improvement when working on uninitialized data is a bonus).The push_back solution also does fencepost checking, which slows it down, so it remains slower than the malloc version.live example (I also replaced the timer with chrono::high_resolution_clock).Note that if you have a structure that usually requires initialization, but you want to handle it after growing your buffer, you can do this with a custom std::vector allocator.  If you want to then move it into a more normal std::vector, I believe careful use of allocator_traits and overriding of == might pull that off, but am unsure.Try with this:I get almost exactly the same performance as with array.The thing about vector is that it\'s a much more general tool than an array. And that means you have to consider how you use it. It can be used in a lot of different ways, providing functionality that an array doesn\'t even have. And if you use it "wrong" for your purpose, you incur a lot of overhead, but if you use it correctly, it is usually basically a zero-overhead data structure. In this case, the problem is that you separately initialized the vector (causing all elements to have their default ctor called), and then overwriting each element individually with the correct value. That is much harder for the compiler to optimize away than when you do the same thing with an array. Which is why the vector provides a constructor which lets you do exactly that: initialize N elements with value X.And when you use that, the vector is just as fast as an array.So no, you haven\'t busted the performance myth. But you have shown that it\'s only true if you use the vector optimally, which is a pretty good point too. :)On the bright side, it\'s really the simplest usage that turns out to be fastest. If you contrast my code snippet (a single line) with John Kugelman\'s answer, containing heaps and heaps of tweaks and optimizations, which still don\'t quite eliminate the performance difference, it\'s pretty clear that vector is pretty cleverly designed after all. You don\'t have to jump through hoops to get speed equal to an array. On the contrary, you have to use the simplest possible solution.It was hardly a fair comparison when I first looked at your code; I definitely thought you weren\'t comparing apples with apples. So I thought, let\'s get constructors and destructors being called on all tests; and then compare.My thoughts were, that with this setup, they should be exactly the same. It turns out, I was wrong.So why did this 30% performance loss even occur? The STL has everything in headers, so it should have been possible for the compiler to understand everything that was required.My thoughts were that it is in how the loop initialises all values to the default constructor. So I performed a test:The results were as I suspected:This is clearly the source of the slowdown, the fact that the vector uses the copy constructor to initialise the elements from a default constructed object.This means, that the following pseudo-operation order is happening during construction of the vector:Which, due to the implicit copy constructor made by the compiler, is expanded to the following:So the default Pixel remains un-initialised, while the rest are initialised with the default Pixel\'s un-initialised values.Compared to the alternative situation with New[]/Delete[]:They are all left to their un-initialised values, and without the double iteration over the sequence.Armed with this information, how can we test it? Let\'s try over-writing the implicit copy constructor.And the results?So in summary, if you\'re making hundreds of vectors very often: re-think your algorithm.In any case, the STL implementation isn\'t slower for some unknown reason, it just does exactly what you ask; hoping you know better.Try disabling checked iterators and building in release mode. You shouldn\'t see much of a performance difference.GNU\'s STL (and others), given vector<T>(n), default constructs a prototypal object T() - the compiler will optimise away the empty constructor - but then a copy of whatever garbage happened to be in the memory addresses now reserved for the object is taken by the STL\'s __uninitialized_fill_n_aux, which loops populating copies of that object as the default values in the vector.  So, "my" STL is not looping constructing, but constructing then loop/copying.  It\'s counter intuitive, but I should have remembered as I commented on a recent stackoverflow question about this very point: the construct/copy can be more efficient for reference counted objects etc..So:oris - on many STL implementations - something like:The issue being that the current generation of compiler optimisers don\'t seem to work from the insight that temp is uninitialised garbage, and fail to optimise out the loop and default copy constructor invocations.  You could credibly argue that compilers absolutely shouldn\'t optimise this away, as a programmer writing the above has a reasonable expectation that all the objects will be identical after the loop, even if garbage (usual caveats about \'identical\'/operator== vs memcmp/operator= etc apply).  The compiler can\'t be expected to have any extra insight into the larger context of std::vector<> or the later usage of the data that would suggest this optimisation safe.This can be contrasted with the more obvious, direct implementation:Which we can expect a compiler to optimise out.To be a bit more explicit about the justification for this aspect of vector\'s behaviour, consider:Clearly it\'s a major difference if we make 10000 independent objects versus 10000 referencing the same data.  There\'s a reasonable argument that the advantage of protecting casual C++ users from accidentally doing something so expensive outweights the very small real-world cost of hard-to-optimise copy construction.ORIGINAL ANSWER (for reference / making sense of the comments):\nNo chance.  vector is as fast as an array, at least if you reserve space sensibly.  ...Martin York\'s answer bothers me because it seems like an attempt to brush the initialisation problem under the carpet.  But he is right to identify redundant default construction as the source of performance problems.[EDIT: Martin\'s answer no longer suggests changing the default constructor.]For the immediate problem at hand, you could certainly call the 2-parameter version of the vector<Pixel> ctor instead:That works if you want to initialise with a constant value, which is a common case.  But the more general problem is: How can you efficiently initialise with something more complicated than a constant value?For this you can use a back_insert_iterator, which is an iterator adaptor.  Here\'s an example with a vector of ints, although the general idea works just as well for Pixels:Alternatively you could use copy() or transform() instead of generate_n().The downside is that the logic to construct the initial values needs to be moved into a separate class, which is less convenient than having it in-place (although lambdas in C++1x make this much nicer).  Also I expect this will still not be as fast as a malloc()-based non-STL version, but I expect it will be close, since it only does one construction for each element.The vector ones are additionally calling Pixel constructors.Each is causing almost a million ctor runs that you\'re timing.edit: then there\'s the outer 1...1000 loop, so make that a billion ctor calls!edit 2: it\'d be interesting to see the disassembly for the UseArray case. An optimizer could  optimize the whole thing away, since it has no effect other than burning CPU.Here\'s how the push_back method in vector works:After calling push_back X items:Repeat.  If you\'re not reserving space its definitely going to be slower.  More than that, if it\'s expensive to copy the item then \'push_back\' like that is going to eat you alive.As to the vector versus array thing, I\'m going to have to agree with the other people.  Run in release, turn optimizations on, and put in a few more flags so that the friendly people at Microsoft don\'t #@%$^ it up for ya.One more thing, if you don\'t need to resize, use Boost.Array.Some profiler data (pixel is aligned to 32 bits):BlahIn allocator:vector:arrayMost of the overhead is in the copy constructor. For example,It has the same performance as an array.My laptop is Lenova G770 (4 GB RAM).The OS is Windows 7 64-bit (the one with laptop)Compiler is MinGW 4.6.1.The IDE is Code::Blocks.I test the source codes of the first post.O2 optimizationUseArray completed in 2.841 secondsUseVector completed in 2.548 secondsUseVectorPushBack completed in 11.95 secondsThe whole thing completed in 17.342 secondssystem pauseO3 optimizationUseArray completed in 1.452 secondsUseVector completed in 2.514 secondsUseVectorPushBack completed in 12.967 secondsThe whole thing completed in 16.937 secondsIt looks like the performance of vector is worse under O3 optimization.If you change the loop toThe speed of array and vector under O2 and O3 are almost the same.A better benchmark (I think...), compiler due to optimizations can change code, becouse results of allocated vectors/arrays are not used anywhere.\nResults:Compiler:CPU:And the code:With the right options, vectors and arrays can generate identical asm.  In these cases, they are of course the same speed, because you get the same executable file either way.By the way the slow down your seeing in classes using vector also occurs with standard types like int. Heres a multithreaded code:The behavior from the code shows the instantiation of vector is the longest part of the code. Once you get through that bottle neck. The rest of the code runs extremely fast. This is true no matter how many threads you are running on. By the way ignore the absolutely insane number of includes. I have been using this code to test things for a project so the number of includes keep growing.I just want to mention that vector (and smart_ptr) is just a thin layer add on top of raw arrays (and raw pointers). \nAnd actually the access time of an vector in continuous memory is faster than array.\nThe following code shows the result of initialize and access vector and array.The output is:So the speed will be almost the same if you use it properly. \n(as others mentioned using reserve() or resize()).Well, because vector::resize() does much more processing than plain memory allocation (by malloc).Try to put a breakpoint in your copy constructor (define it so that you can breakpoint!) and there goes the additional processing time.I Have to say I\'m not an expert in C++. But to add some experiments results:compile: \n    gcc-6.2.0/bin/g++ -O3 -std=c++14 vector.cppmachine: OS: Output: Here the only thing I feel strange is that "UseFillConstructor" performance compared with "UseConstructor".The code:So the additional "value" provided slows down performance quite a lot, which I think is due to multiple call to copy constructor. But...Compile:Output:So in this case, gcc optimization is very important but it can\'t help you much when a value is provided as default. This, is against my tuition actually. Hopefully it helps new programmer when choose which vector initialization format.